Okay, so... old GPUs are dropping in price, and new GPUs are starting to go for MSRP.  This isn't the market collapse that people seem to be pretending is on the verge of happening, this is pricing returning to pre-pandemic norms, where the occasional bottom-end card selling for below MSRP is to be expected, not seen as a unicorn.
I hate the Dutch second hand market. I see 6700xt's going for the same price as they cost new now. And yes the new ones are in stock.
"Used market pricing is such a good deal, to the point that going new is pretty much undesirable anymore, yes even factoring the risk when going used.

Going with RTX 3060 Ti selling at $250 or RX 6700 XT at $270 vs RX 7600 new at $260 it is pretty much a no brainer for the first 2 choices even if they are used hands down."
I upgraded to 3080 as I was sitting at home during covid. Now I see no need to upgrade again nor to splash so much money when I can spend it on outdoor activities. Like buying handheld!
Checks price of 4090 ... still too expensive :(
"It's pretty telling that the new stuff (4060 Ti, 7600) is already being discounted. Graphics cards are not like SSDs and RAM where the price fluctuates like your usual commodity, which means Nvidia and AMD signed off on these price drops to some extent.

We're currently in a state of consumer chip oversupply, so patient gamers can probably expect some more price drops in the short term. However, there are already rumors that Nvidia (no word yet on AMD) is set to reduce production of consumer chips in order to better match the cold demand for graphics cards. That means the discount party is going to end at some point.

How exactly it ends is hard to predict. It could be that prices will raise back up to MSRP once the undersupply meets the under-demand. Personally, I think Nvidia and AMD will just let these discounts stay the way they are and just replace existing SKUs with ""Super"" refreshes. The ""Super"" SKUs will set a brand new MSRP that are more likely to hold due to no longer being oversupplied, and the existing discounted SKUs will eventually sell out."
Looks like Iâ€™ll wait another generation. Shits too expensive. I was able to snag a 1080ti for $550 CAD in Nov 2019 and a 4070 will cost more than $900 after taxes for an 80% uplift.
"For any active duty, vets, or dependents, armed forces exchanges are selling FE cards below MSRP now. 4090 was like $1519 I think. Also no sales tax, so probably ends up being like $150 savings. 

Of course they didn't start doing that until after I paid $1599+tax at Best Buy for mine."
[5:56](https://youtu.be/gb49lnHpKiE?t=356) The 4070 is selling pretty well this generation
I'd love to pick up a 7900 XTX and move my 6800 XT to one of my kids PCs but the price to performance is complete trash. $700 and I'm in.
"The mid level came definetely too late, ram too low, expensive... 

Amd is making a refresh that may be ok."
There ought to be vram slots on gpu's, so user can upgrade to their heart's content. Problem will be solved for otherwise obsolete cards.
4090 down $15. Finally time to buy one.
Still waiting for a decent price to upgrade my 2070S.
"The gpus are getting alot faster now, and the upgrade cycles will be much longer. 3-4 generation upgrade period will be the norm.

I guess it depends alot on how demanding future games are. VR gaming requires alot of gpu power as well.

price/performance stagnation can't be good for gpu sales. Let's hope next gen will deliver. Or we get some reasonable price cuts this gen. $400 4070, $600 4070ti, $800 4080."
I've said this 3 months ago!  When Christmas shopping spree will be in full swing and you'll not believe you eyes! The good thing is that it won't be just this year, big tech companies are facing financial crunch that will linger on for at least another 2-3 years!
Aussie retailers are artificially keeping pricing high...they simply don't care because nobody is calling them out directly!
Keep holding out fam! Show these pricks who's boss. Let them sit on those cards. It's going to be a great black Friday.
Dlss, Fsr & xess are godsend ... I feel like people might not upgrade so often anymore. Unless Jensen unleashes overdrive on more games and just use fomo.
"Lol drop?
Prices still expensive ðŸ«°

U know the gpu maktrd prices wherr totally fucked and scammed up when they say its dropping,
But its still expensive lo

I rather throw my pc out the window then supporting this crap gpu matked 
Fucking disgusting companies"
Folks, do not buy a card with less than 12gb of vram in the current year. lol
Where are the prices going down? The Asus 4070 I got for $649 is now $671 on Amazon.
"Prices remain firm in the UK. Most 4070 GPUs are still 550-600 for cheaper brands and go all the way up to 700. 

Absolute criminal that they just forget their own supply and demand rules when it doesn't go in their favour.

AMD aren't much better here either.

Buying power of our currency has dropped significantly and the cost of living has skyrocketed. 

Shit needs to change fast."
Where are the best places to purchase used GPUs?
ZzzZZzz.. wake me up when PC gaming makes sense again. All of the latest games are sub-par ports of Console titles, with resolution bumps and slightly higher quality lighting, but with all kinds of shader compilation issues and stutters. There are a few exceptions with some high-end AAA titles, but nothing that would justify the cost of a modern gaming PC over a PS5 / Series X. Prices are bat shit crazy, coming from a life-long PC gamer who hasn't upgraded from a 980 Ti that I bought secondhand in 2016. I'm rolling with the PS5 for newer games until hardware prices come down, or we get truly unique experiences that can only be achieved on much more powerful hardware.
Hopefully this is a sign of how the next gen's MSRPs are gonna look.
4090 must good price 1000-1200 usd ..
It's almost like someone turned saturn into a giant gpu and they became unnecessary
"Pour one out for the great crypto-crash price slump that never eventuated. 

Instead we're getting a painfully slow return to almost-somewhat-sensible prices (assuming this continues for another year or so)."
"The market crash only didn't happen because Nvidia artificially held new pricing at or above MSRP, delayed launches, and increased their new gen pricing.

There simply isn't a supply:demand reason for the 3070 to be 530-570 even *after* the 4070 released, but that's what happened. Only weeks after (when I assume Nvidia gave a rebate) did prices fall.

Not to mention the 3060 and 3050 being *awful* value for nearly an entire year straight. It was baffling to see 3060 TI's going for 430-450 while 3050's were 300 or more. Only like 3 weeks ago did those products dip below their already inflated MSRP's.

AMD delayed launches but continued to lower pricing over time, but it didn't crash the market since AMD's supply is a lot less than Nvidia.

If Nvidia launched Ada in mid Q3 like most expected and the 4080 was 800, it would've absolutely crashed the market."
Same here in Italy, I can get a 6750xt for 370 on amazon and 400e used on ebay. Almost the same with a 6800xt: the sellers are slow.
How long have they been in stock? There was a lull in the US between the time video cards were in stock and used market adjusted.
German second hand market is the same except itâ€™s everything. They think their X year old used, out of warranty crap is worth 80-90% of msrp.
[deleted]
Is really common to agree to a price even 30% lower of the listing on marktplaats
europe's second hand market is trash, people want high prices for their cards because they paid a lot 2 years ago.
"This is surprisingly common for AMD. The used market has a built-in time lag to it, so when AMD is aggressive at cutting New proves, Used prices take time to follow.

There were many points (I'm in the US) during the 2022 proce crash where brand new cards were going for the same as used ones.

Nvidia, meanwhile, artificially held pricing above MSRP for an exceptionally long time. So, used sellers had to slash prices below that because they could not hope to sell them. As a result, buying an Ampere card just before its next gen successor came out is like rolling a new car off the lot. They wanted 530-570 for the 3070 even *after* the 4070 was announced. Today, just 2 months later, there's 3070's on eBay for below 300.

If you take eBay fees and the sales tax you paid for your new card into account, you're getting back less than half the price you paid for it.

AMD by comparison was aggressive at lowering prices so this doesn't happen."
The current gen price points at the low end just don't make sense, outside of the A750 at $200ish dollars I guess.
"> RTX 3060 Ti selling at $250 

I wish Canada was like this, peopel still asking $400CDN for a 3060"
My friend bought a used 6700 XT for $207 in Malaysia. He said it smelled like rat poop initially but performs just fine.
"Nowadays everytime I'm in the market to buy a new gpu I usually wait for the new nvidia gen and then buy a used last gen xx80 gpu for cheap.

It worked pretty well with my last 2 cards (1080ti, 3080).

There's just no way for me to justify spending 1k+ on a gpu alone, and I'm someone who usually buys pricier products."
"I just saw a 3080 12GB for $420 on my local Craigslist, the times are good

(Relative to before)"
If youâ€™re going ultra budget, 980tiâ€™s/titans are going for maybe $60-80, and thereâ€™s a lot of them. Theyâ€™re old, for sure, but pretty capable, especially if you donâ€™t play the latest AAA thing.
I'm seriously considering the 4060 once it comes out, depending on the reviews of course. My usecase is a 2560x1080 200Hz monitor that I want to drive the latest AAA games on (high settings with at least 60 FPS, more if possible). Currently I have a 5700XT and it struggles in some new games quite a bit, and FSR2 is absolutely terrible at 1080p-class resolutions so I can't use it. I'm hoping a 4060 can drive at least 60 FPS on its own or with DLSS help (since it's far better at this res) and then framegen it up closer to the monitor's refresh rate.
"Due to AMD's aggressive New pricing, I actually don't find it that much worth it.

For example, the 6700 XT brand new is 310 and comes with a game.

The 6600 is 180 with a game, and last time I checked, they were like 160 at the cheapest on eBay."
">handheld

Genuinely asking what the use case is for handheld, why the explosion of steamdeck and that whole segment. Is it people who commute."
"It's a halo product, and is actually a decent deal looking at previous Nvidia halo products price/performance. It won't likely drop much. 

I'd like to see the 4080 drop a lot more. The pricing is criminal."
"And almost faster than a high power limit 7900xtx...  Just a little faster and you could call it a 7900xtx killer...

If that's not worth $2000+ I don't know what is..."
it's worth it
"My forecast is that even as last-gen inventory thins out, they'll let current prices ride out through the holidays, leveraging game promotions to help carry some value. 

The discount party will continue ad-hoc if sales aren't doing well enough through the holiday. The retail side will not be happy about carrying inventory that isn't selling and supply will be higher during the holiday. If sales aren't good enough exiting the holidays, it doesn't get better over time. Seasonality going through the first half of a year will only see weaker sales.

Nvidia recently said that they're observing upgrade cycles to be about 3-3.5 years. The RTX 3060 was available in late February 2021. So not only are we in a down-cycle due to the inventory glut, we're not yet in the upgrade cycle for last-gen users. 

I agree. I think a mid-gen refresh is likely next year. 

Also, if Nvidia is expecting to grow in the datacenter, it's not only H100/A100 that they'll be selling. The L4 will likely be their top selling inference card, like the T4 was. That means they'll be producing lots of AD104 to bin for the L4. Overtime, supply of defective GPUs will build up and a mid-gen refresh will be needed to sell the resulting cut-down, AD104-based GPUs."
"> ...set to reduce production of consumer chips in order to better match the cold demand for graphics cards.

Demand is high; it's just sales that are low due to the ridiculous pricing. $500 for a 4060 Ti 16GB?  Are they out of their f*#&ing mind?!

If these products weren't simply terrible value, and were priced reasonably, they *would* be selling."
Not only it's expensive, it's also not future proof with too little vRAM.
"Itâ€™s selling very well according to mindfactoryâ€™s numbers, theyâ€™re kinda downplaying that one a bit.

Itâ€™s the top selling card on mindfactory by like, a 50% margin over the next highest (6700XT), and has been there for weeks. Some weeks itâ€™s double, which is extremely good given that itâ€™s mindfactory and literally twice the price.

https://reddit.com/r/Amd/comments/14d0ffe/graphics_cards_retail_sales_week_24_2023/

https://twitter.com/TechEpiphany/status/1667537641288417283"
"4070 is the best positioned card for most people.

4090 is out of reach for most, 4080 is out of reach for most with relatively poor value. 4070 TI is overpriced with less VRAM/performance than the competition. 4060 TI is 10-15% faster than the 3060 TI at best and drops to -5% to 5% faster at the resolutions buyers will actually use it for (and that's if you have PCIe 4.0)

4070 is 30% faster than the 3070, has 50% more VRAM, and lower power consumption. Unlike the more expensive cards, the only competition from AMD are last-gen high-end cards, which means *drastically* higher power consumption. While RDNA3 isn't *that* much more inefficient than Ada, RDNA2 is a lot more inefficient. The difference in absolute watts is large enough that many 600 dollar GPU buyers will need to get a new PSU if they get a 6950 XT over a 4070.

The 4070 *really* should've been a 500 dollar card, but even now, it's the best positioned of the Ada line, even if it's value is kind of garbage by historical standards."
because for everybody that doesn't have an rtx 3000 card it is still a good upgrade. Sure you are paying 100 usd more than "it should cost" but does that make such a big difference when you are coming from a 1070 or something like that
Might be a while for the XTX to hit that price as anything other than an open box, but a new XT isn't too far off the mark right now.
"There are form factor (heat sink and slot) issues as well as signal integrity issues (speed) that have a tradeoff to do this.   
  
In this particular case, there are technical reasons this doesnâ€™t happen.     If we re-designed the ATX standard and PCIe interface, we could likely find a solution where this tradeoff is more manageable."
"the value increase for the 40 series is absolutely gross. They did improve value if you want to spend $1000+, but who wants to do that. Everything from $700 and under is basically the same value as the 30 series, so where you see 20% more perf, you see a 20% increase in price.

If nvidia wants to sell 50 series cards, they need to make them a major value improvement."
"Typically, it's either the AIB's or distributors in these cases. Retailers make infamously low margins on the flagship/doorbuster products.

Oftentimes, the retailer is afraid to slash prices and sell the product because now they have to chase down the AIB for a rebate after the fact.

Or they have a contract where they need to order a certain amount, but with a cap on how many they are forced to have on hand. In this case, by *not* selling the product for a loss, they can force the AIB to give them a rebate as they can not take anymore product.

We saw this gane a chicken a lot, where Nvidia GPU's sharply dropping in price out of nowhere anong all retailers at the exact same time, which means this was a decision by a larger party than the individual retailer."
This retailers retailers can just ask for like 100 (whatever currency) more, Nvidia can't prevent that from happening. And people would still blame nvidia
Pathtracing is legit just amazing though
A friend is offering his 3080 10gb for $400. I'm really struggling to decide between that and a new 6700XT, which does have 12gb. Looking for 1440p gaming but the productivity that comes with NVIDIA would be nice too.
[deleted]
"I can get behind this.  


2.5 years ago when the 30 series launched 8gb was good enough for max textures. That lasted for \~2 years, but the last 6 months have seen releases target current gen consoles with their unified 16gb of ram. That means in practice we are seeing 12gb of gpu usage on consoles, which means it's the minimum for PC.

&#x200B;

So you can probably get another 2 years out of an 8gb card at medium-high textures, which makes an 8gb card purchased in 2020 a reasonable investment. But in 2023, an 8gb is a liability."
I think you might be looking at the OC. I bought my Asus 4070 OC for $679 on Amazon.
">Absolute criminal that they just forget their own supply and demand rules when it doesn't go in their favour.

that doesn't work like dude. I think demand is higher or supply lower than you think. There is no point no selling anything at all. Because you make 0 profit, you actually make a loss even. People like you act like Nvidia is run by a bunch of idiots and that makes you the idiot."
Microcenter open box is killer if you live near one. I found a 7900 xtx for $750.
Facebook Marketplace
Thatâ€™s funny man, Iâ€™m still rocking a used 980ti too. I just havenâ€™t found the necessity to upgrade. I was going to do a new build for Starfield, but the ridiculous prices have me in limbo.
">Instead we're getting a painfully slow return to almost-somewhat-sensible prices (assuming this continues for another year or so).

Not even that.  No one is calling the RTX 4000 lineup ""somewhat sensibly priced"" even at MSRP.  An RTX 4080 is still going to cost you $1200 unless you get something like a Yeston card shipped from China, and then it'll still be $1100.  Those aren't ""sensible prices"", it's just closer to MSRP than the norm has been for the past few months (>$1500)."
"They are not slow, they simply look at other people prices, and match those with maybe a small discount. The people who dropped their prices a lot, have already sold theirs and this result in you seeing ""overpriced"" cards.

And to be honest, i also tend to put my hardware way higher than it needs to be because of the buyer mentality of massive undervaluing sales items. 

People buying on ebay tend to be very, ... cheap these days. If you sell something brand new or very new for 350 euro (remember, that is already like 20 a 30% under the retail price). 

Somebody will come with a 230-price offer and another with 270 euro but that tends to be it. Sell that same piece of hardware for 400 Euro, and you get maybe a 310 a 330-euro price offer that you can accept.

You need to play with the price to get good lowball offers. Put it like bargain low, yes, it will sell very fast (and people will still lowball you!). But you are taking a big cut on the already secondhand price (especially when you add ebay fees). 

In Germany they now removed ebay fees for private sellers because the platform is like half death. You can tell from the constant lowball offers that are just so ridiculous and the relative low number of buyers. A lot these days there is shops and big commercial sellers."
I politely pointed out to someone that they were selling a 5800x3d for _more_ than it cost new, thinking that he might be unaware of how the price changed since he bought it, but was immediately cursed out. I wish I could report people that sell stuff for more than new but really if there are people that buy this crap without checking what it is actually worth then I guess they have no reason to stop.
problem is a lot of them won't deal without non German countries. e.g. mindfactory.
30% off for products that are second hand that might be years old by now is still a bad deal.
The issue with arc is amd is already the budget option. They have better drivers and are priced reasonably... rx6600xt is $229
"Cc selling 3060 for $350.

Not the best deal but it is what it is..."
You can get that down to 300 no problem. 400 is for a 3070
Can I ask, when did you buy the 3080? And what price did you pay? Trying to decide whether to buy a used 3080 for $450 or a 6700XT card for $350 for 1440p gaming.
Yeah with these people here talking about xx60 cards for $300. Gotta be some kind of joke...
"They're getting tempting, my only hangup is the 350 W TDP.

It's funny that the main selling point for 4000 series cards for me at this point is that it's summer time and the older cards would kick out much more heat. I guess when price to performance gains are so small, all the efficiency gains go to benefit the TDP."
Yeah If I see a decent rx 6800xt 16GB for ~420 I'm gonna pull the trigger, I don't like the idea of a old gen Nvidia that does not support DLSS 3 and frame generation.
"I'm in the same boat as you - 5700XT and play at 1440P and it really struggles in a lot of games.  Mostly I've been noticing that it's down to the 8GB of VRAM.  Hell, I've been running Fallout 4 (from 2015) with high-res texture packs and I notice that I'm running out of VRAM at times in game.

Hoping to upgrade this year to a card that will more effectively run 1440p games and am looking for a 16GB card since the reports are these days that 12GB is *just* barely enough for 1440p."
What? The 4060Ti is on par with the 3060Ti and the 5700XT is on par with the 3060 soâ€¦ how much of an upgrade do you expect the 4060 to be?
Depends on the price of the 4060, but a 3060Ti will have bigger performance and will probably be cheaper/same price
"I am basing my next choice on Starfield. The recommend specs on that seem pretty damn high, and I think if I have a card that can play that pretty well, then that card should last me for several more years. 

I am not confident a 4060 will play it well."
"> FSR2 is absolutely terrible at 1080p-class resolutions so I can't use it.

A lot of people say this, but I finally experienced FSR2 with the Witcher 3 next gen update, and IMO FSR2 quality mode is the best of the available (on my RX 580) antialiasing options.  The others being native (extreme shimmering), FXAA (shimmering), native-TAA (looks about the same with worse performance), XESS (ghastly performance), and DLSS (Nvidia only)."
If performance is even equal to the 5700 XT at just over half the TDP, that is in fact progress. It will probably outsell the 70, 80 and 90 combined when you include mobile versions
Honestly some games make more sense on a couch or just not at your computer. Like do you really need all that power to run some of those older games or are you fine getting pretty decent performance while getting the comfort of your bed/couch?
"Speaking for myself:  


I have one (Steamdeck) for two reasons:

1) It allows me to take an absolutely \*massive\* library of software with me wherever I go  


2) If I want to play an extremely heavy game or one that has been heavily modded while the wife watches TV, I can stream to it from either one of my PCs without much issue without having to relocate to a different room while also avoiding the headache of taking a hatchet to image quality + performance or messing around with my mod load orders."
"Not being stuck at a desk to play games is fuckin sweet, and the steam deck has access to my entire catalog of games.

Handhelds have literally always been popular, all the way back with the Gameboy, the ds sold over 150 million consoles, 3ds sold ~75 million consoles, phone gaming is the most popular form of gaming in the entire world, the switch also has over 100 million consoles sold.

Pc gamers just only pay attention to PC shit (which is why they suddenly care now that the steam deck exists.) Handheld games have always and will always be super popular."
Kids, commuters, casual games where the visuals are secondary...
"I play a lot of slower paced games or JRPGs on the couch while watching TV. A handheld is ideal for activities where you don't need to dedicate your full undivided attention to the game.

I have trouble staying engaged for multiple hours in front of a gaming PC, but with a handheld I can dedicate a couple minutes here and there during the day. The Nintendo Switch is even better for that given how much lighter it is."
I bought the Ally and believe it or not, it cured me from my Overwatch addiction. Every Time i wanted to play a solo game on my pc, soon or later i fond myself getting black to that shitty game. With the Ally i'm enjoying solo game more than ever, i dont even know why. There is something about playing on your bed or your couch that d'ores something to my brain
I have a dog and want to cuddle with him while playing games.
In addition to what the others said, the advantage of the Deck is that since it can convert to a desktop, you can have all of your games, files, emulators, roms, and saves on one device.
It's only a decent deal because for some fucked up reason the price/performance of the halo product beats the entire 4000 range. Something unheard of until this generation.
I'll be observing the launch of Battlemage and the relative cost of the 4080 over the next 8-12 months as I save for a GPU upgrade. The sub $500 market is just dissapointing, and I'm content with my 980 Ti for the backlog and a Series X with Gamepass Ultimate for the new releases.
If I see someone say it's a decent deal one more time, think I'm going to die of cringe.
">and with dlss (even just 2.x) it pretty well slays the 6950XT at nearly half the power (especially since FSR2 sucks at 1440p and 1080p).

In terms of FPS, not really? 

You're right that DLSS > FSR, but the 6950xt doesn't get slay the 6950xt, and will also hit a Vram wall far sooner."
"Lately I've been seeing some amazon warehouse deals that are very close to the prices I find reasonable. 4070 were like $450, 4070ti were $650, and 4080 were $850. Amazon had a coupon deal for some new gpus as well.

I'm expecting these gpu prices to fall even more toward the end of the year, I don't think these gpus are ever going to move volume at these prices.

I don't know about next gen gpus, I don't think nvidia nor amd wants to give us the performance we'd expect for every generation increase in price/performance.

It's going to be very interesting to see if the bring out another software feature that's going to hold back the gpu performance again. Clearly they can't limit the vram next gen or it will be doa for sure."
Retailers here are simply greedy...it sucks!
It looks amazing, but there's one (?) game that supports it, and it absolutely murders the frame rate.  I know I'm not shelling out $1.6K just to play that.
Just buy a used 6700xt for like 220
Maybe if you're just playing ps4 games
Mine is the Asus Tuf Gaming OC.
What an absolute troll.
Prices slowly going back to MSRP is a lot less sensible when NVIDIA and AMD ridiculously inflated MSRP for this generation to essentially crypto/scalping prices themselves.
If you are willing to go to eBay you can get 4080s for $1000 sometimes but it will likely be open box or used.
[deleted]
"I don't understand the German market: most stores like Mindfactory don't sell / ship to other Europe countries.

May I ask what do you use to sell / buy used instead of Ebay?"
I also did the same on willhaben Austria and he was like "then? why dont you just buy it first hand on amazon if its cheaper", I replied, "Cus the MSRP is still pricey for me so I came looking on here for cheaper 2nd hand options" but he was still annoyed I told him about the prices outside.
Agreed. I think Intel could succeed in cannibalising AMDâ€™s GPU sales but not this generation.
Drivers are bad for old games but they seem pretty decent for modern games these days.
I think the drivers situation with Arc is very overplayed at this point, that was a problem at launch, at this point in time Arc's drivers are pretty matured and still improving, for most users its simply not an issue anymore for the vast majority of games.
It is what it is
"I bought it the around the time the 4080 was released so ~mid november. I paid 600â‚¬ for it, which is a shame cause now they go for like 500â‚¬ used here in germany. My 3080 is also used for 1440p gaming.

If u can get a 6800 xt for around the same price as a 3080 you can prolly get another 1/2 years more out of the card cause it has way more vram and is just faster in some titles."
Currently assembling some more hardware for stable diffusion. Im liking 3090s. They're hovering around $600
If you donâ€™t like the idea of an nvidia card that doesnâ€™t support frame gen, you arenâ€™t gonna believe the DLSS support an AMD card is going to getâ€¦
"People really underestimate how little 8GB actually is. 

I'm playing a lightly modded Skyrim at 1080p with my 6700XT and it's approaching 9GB of VRAM usage in the major cities. The current budget and mid-range 8GB GPUs will completely choke on a lot of 2025 games."
20% or so of raw performance + actually usable upscaler and framegen (no diss on FSR as a tech overall, but it's legit unusable at 1080p). Depending on how much I can sell the 5700XT for, the budget might work out.
"Price is officially $300 (thatâ€™s why 7600 dropped to 270). 
Performance is to be seen"
Nvidia claims 20% performance without DLSS3 over the 3060. 3060 prices have dropped in anticipation of the 4060. Imo if I'm gonna buy an Nvidia card today I'd rather have DLSS3 and Adas architectural improvements than buying Ampere.
Or a 6700xt.
Can't speak for others but when I game I want at least a monitor sized screen. Sure the bed might be comfy but I ain't playing on a handheld sized screen if I have the choice.
"If you're just on a couch/bed what's the appeal over a laptop though? 

I can use a laptop for actual work too on top of gaming."
Playing pc games on the couch is the only correct way. Playing on a desktop is so 2000s
Why not throw it on the TV and use a full-size controller then?
miyoo mini+
It compares favorably to the halo products from the last 3 generations (at least) as well.
"Idk why people say this, itâ€™s mathematically not true lol. The 4090 has the worst value of you just divide the price/performance. 

Now the price/performance of previous xx70 xx80 etc class cards is the worst this gen however that doesnâ€™t make the 4090 a value play lol."
It's literally double the speed of the 3080TI and cost a fraction at launch. Is it expensive and a ridiculous purchase for most people? Yes. But it's hard to argue with the value proposition if you're shopping in that price range. Most people aren't and never will, but that's not my point.
"[As Tim HUB saysâ€¦](https://youtu.be/ycXkvVfc2yw?t=2583) itâ€™s not to the point yet that you should *only* consider cards with dlss, and itâ€™s probably not worth buying an 8gb card instead of a 12gb (3060 ti / 4060 non ti vs 6700XT) butâ€¦ itâ€™s a pretty major advantage for nvidia in the 1080p and 1440p segments and itâ€™s supported in all the games that matter. In these cases of tiebreaker it is a pretty big lean in Nvidiaâ€™s favor.

[According to HUBâ€™s video on the topic,](https://m.youtube.com/watch?v=Iy3ikm8MxOM) the 6950XT is 15% faster at 1440p and thatâ€™s significantly less than the amount that DLSS Quality adds (while maintaining roughly native quality output), and FSR2 Quality mode has bad output at 1440p. If you are willing to take the quality loss from using FSR2 quality mode for a 20-30% bump, DLSS2 performance mode will bump the DLSS increase to more like 50% and it utterly blows the 6950XT away at equal visual quality again. Thatâ€™s the situation AMD finds themselves in competing against DLSS, their cards really need to be delivering around 30% more raw raster to keep up with the image quality loss from FSR2 in the 1080p and 1440p segments given how bad FSR2 is at those resolutions. Dlss2 quality is roughly on par with native (often better, actually), itâ€™s essentially free frames, and FSR2 is **not**, especially at 1080p and 1440p where itâ€™s rather poor.

If you think of upscaling like an op-ampâ€¦ there is a gain ratio and AMD cards are simply much noisier for a given gain ratio. So they need to be offering a stronger input signal to offset that, because to the end user it doesnâ€™t matter - stronger input vs cleaner gain is an implementation detail, all they care is that it sounds the same. And nvidia has got it to the point where their first setting of gain doesn't add any noise, and if you donâ€™t want the gain you can choose DLAA and get cleaner-than-native output for free instead. Thatâ€™s the marketing problem AMD faces in the 1080p and 1440p segments. Also, all of this applies to intel as well, their XeSS is much closer to DLSS2 than FSR2, AMD is significantly behind *both* competitors.

Let alone any kind of perf/w comparisonâ€¦. 6950XT consumes 200W more power to get that 15% native res win, and cost 5% more, at the time of their video (itâ€™s gone up a bit more since as stock depletes).  And DLSS doesn't really increase the card power, the TDP budget is what it is and the framerate is what it is.  It's a cheap way to render more frames per watt and more frames period.  Laptop dGPU is a dying segment but DLSS is absolutely a must-have if you are buying a gaming laptop imo.  Ada is pushing like 2x the native perf/w of RDNA2, and with DLSS piled in that rises to like, 250-300% of the perf/w of RDNA2 dGPUs.

Yes, the 6950XT has 4GB more but the 4070 isnâ€™t up against the wall in the same way the 8gb cards are. Series S has 8GB of VRAM and series X has 10GB, so, itâ€™s probably not going to massively massively increase such that 12gb is unplayable.

Itâ€™s arguable but at equal price the balance really does tip towards the 4070, even Steve is onboard with that one. If you can get a 6800XT for $450 or $500 then whatever but the era of paying $600-700 for Navi 21 is really drawing to a close. Giving up DLSS and/or suffering through FSR2 1440p at 200w more power and 5% higher cost for a 10% performance advantage and 4GB more vram really is not worth it in the big picture.

4070 is a legit decent card, itâ€™s just not a return to Moores law era progress. The 3080 and 3060 ti were the value peak of the lineup and struggled to reach msrp, 4070 being a more efficient 3080 12gb for $100 less really is not bad. With the $100 microcenter steam GC deal itâ€™s actually pretty great, 4070 12gb for de facto $500 is about as good as things are going to get this gen. Yes, not as good a deal as 1070 or 970, but things are different now."
It's actually surprisingly playable with cards weaker than a 4090 too (as in, stable FPS above 30). DLSS 3 really works wonders.
i would rather see nvidia and amd get the money than scalpers
"Actually the 4090 is actually pretty good value. The 4080 is excellent value compared to the 30 series launch msrps.

The problem is normally we should have a 4080 that improves value 20-30% over the 30 series and that has not happened at all. The 40 series is 30 series value extended to $1600 where traditionally after $600, value completely tanks.

And that is fine for going over the value knee and buying the last 10% of performance can be incredibly expensive. I like a world where a camary, a corvette, and a mclaren all exist. The problem is nvidia is acting like entry point is a bmw."
"> most stores like Mindfactory don't sell / ship to other Europe countries.

That is mostly a shipping cost and legal issue with scammers. Shipping inside of Germany is dirt cheap with DHL. The moment you cross the border even by one meter, that price really jumps up a lot. 

Now combine that with a very active community of thief's that use stolen PayPal, visa, and other payment methods to buy items. By the time it's get flagged as stolen, the seller is eating that cost because if you want to report the issue, well, good luck dealing with French of Romanian police. I also do not like it but as long as our justice system is so fragmented in Europe, well ...

The issue is also, lets say they ship to Belgium. Maybe they have less problem with scammers there, but if you ship to Belgium, the next guy in Romania can say ""hey, you are discriminating my EU country"" (what technically is correct, it is a form of discrimination and that is against some EU laws of goods sold in EU). So, it became ""We simply ship to nobody"".

My father yeaaaars ago dealt with a Polish seller for some wood stove. He wanted to buy directly from Poland (and pay shipping). They said: ""No, we have a distributor in your country, you need to use him"". 

Except that guy was asking like 50% more for the exact same thing, that this distributor needed to order from ... Poland. Dad fed up (and stubborn as a mule) contacted EU and said, ""hey, they are selling these products across border using a distributor, that means i can order directly from them"". Yep, EU agreed, and Polish shop needed to sell directly. And if you only sell inside your own country, well, EU cannot force you to sell outside your country, but if you have a network or selective sell outside, then you need to sell to ALL EU countries.

So i hope this explains a few things...

> May I ask what do you use to sell / buy used instead of Ebay?

For the bulk is still Ebay, they still have the best buy protection, sell protection is kind of iffy and i always advice people to document EVERYTHING. Your good working, take picture, put them in box, pictures, packaging, pictures, label, pictures... Really helps with issue buyers.

It's just a lot less people then in the past so it takes longer to sell items. 

Part of the issue with affordable hardware is, people simply do not bother selling on eBay and throw stuff in a corner. The high sales tax really hurt selling anything below a X value, as your time/effort vs $$$ became an issue (combine that with thiefs/scammers/karens). So, this is probably one of the reasons eBay removed the tax. Dwindling sales and less people. In other words, i do not think people move to different planforms so much as people simply stopped selling their old stuff and chucked it. I also did this a lot in the last few years. It was not worth the hassle vs the money stuff sold at (after ebay their tax).

A alternative is Kleinanzeigen.de (owned by ebay as they bought up almost every secondhand platform in Europe). It's very, VERY unregulated and even more open for abuse. Burned a few times there. With also a very small buyer/seller group.

Amazon is an alternative for secondhand sales these days but i have no experience with their policies.

So yea, not a lot of choice... Every country has some local 2th hand shops but most are owned by ebay these days."
Thing is I donâ€™t think Intel spent all that time and effort building a dGPU division just to fight with AMD over their 5% share of the market. I think eventually they will want to try and grab some of the high end market from Nvidia.
"dx12 mostly, dx11...  not so much, apparently, for instance, arc can't play any dx11 assassins creed...

That's...  ""suboptimal"""
"There are still lots of old DX11 games that don't play well on Arc. 
DF showed Assassins Creed Unity on an Arc A750 last week and ran horrible. 

If what you're going to play is mostly new games, then sure Arc will be a fine choice. 
But for anyone wanting to play older games, then AMD or Nvidia are better"
I kinda desagree: as someone with a budget GPU I tend to play old games, at least I get to be able to play those at 4k with 60fps now.
DX12 is usually fine. But older games, especially older than DX11, still have issues from what I saw with Digital Foundry's revisit to Arc.
"People may not remember, but there was a sizable community that had issues with their 5700XT's when they were first released.  Took them a solid 4-5 months before my card would work in my system without it rebooting during gaming.  It all came down to shit drivers.

Intel just launched their first graphics cards - it's gonna take time to iron out all the bugs."
[Arc driver situation still looks pretty bad.](https://www.phoronix.com/news/Intel-Xe-DG2-No-HuC)
The ARC drivers need a lot of work. It's definitely not overplayed.
Thanks for the reply. Good info for me to think about.
Whaa? Don't leave me hanging! What's going on man?
"20% uplift over 3060 would put it within 10% of the 4060Tiâ€¦ letâ€™s wait for benchmarks to see if that claim holds up. 

At $300 is also approaching 6700XT territory and that is definitely a card well suited for 1080p ultrawide."
Oh so the 3060Ti is cheaper. Yeah it doesnt make sense going with a 4060 then
"And I'm comparing to a 3060Ti. 
The 3060Ti and the 4060Ti are equivalent in performance, so is bigger than 4060 and cheaper too if you can get it in a deal"
For me, it depends on the game. FPS or action adventure I trend toward console. But simpler games or emulation is better on a handheld.
Laptops are hot and loud and have a clunky keyboard. Steamdecks hit a nice portable spot where it doesnâ€™t have the hear and the bulk.
"Find me a laptop that can run games as well as the steam deck for 400-500 dollars. 

The form factor for handheld is way better too"
Yeah dude. How else do you want me to say "unheard of"? This generation is nothing like previous gens. Now the halo product is also the value product. wtf?
Seems like you're smoking some good shit, mind sharing it with us?
because it was way worse for previous generations.
And it's still overpriced.
"I feel like I'm taking crazy pills. Nobody seems to like 4070 but it seems like the best x70 since 1070 to me. Probably 1070 was better value for its time, but 1070 was also among the best cards Nvidia ever made. 

People cheered wildly for the $500 3070, but even a MSRP 3070 bought on launch day isn't as good a value as a 4070 today."
"Nice write up, but all it shows is the 4070 doesn't 

#slay 

The 6950xt.

If you'd said there's a strong argument for a 4070 over a 6950xt, I wouldn't have bothered commenting. But you saying it ""slays"" it made it sound like it demolishes it, which it certainly doesn't. 

Also many people don't even use upscaling, which your entire argument is based on, and some games (One I play for example is grounded) only have FSR, not DLSS. So it wouldn't matter if DLSS was 1000x better than native, since it doesn't exist in that game.

But even the above point is moot if you don't bother with any upscalers. 

Lastly many of us are not in the US, I'm in the UK, so US prices and micro center deals mean nothing, they're equally priced most places over here. Nvidia should have released the 4070ti as the 4070, most people would be happy with it, and it would have meant nobody could possibly recommend the 6950xt over it."
"In a situation where a person only needs raster performance, it's generally better to hit a FPS target natively than to rely on upscaling to hit that same FPS target.

However, the situation changes when RT is involved. While AMD cards are faster at raster than Nvidia cards at the same price, enabling RT puts them at a level playing field or a slight Nvidia advantage. Using upscale for both puts Nvidia at an even bigger advantage."
is that microcenter / steam deal still going on? im not able to find it
I'm not sure I would classify 30 FPS as playable, even if it were achieved without DLSS 3.  The added latency to a low input frame rate really hurts DLSS 3 as well.
"Except that if it's purely a scalping issue, there is at least a chance or prices returning to ""normal"" levels because those inflated prices are recognized as inflated.

When NVIDIA and AMD decide to just adopt scalper prices as the new ""normal"" or represents a huge jump in pricing between generations and future price increases will springboard off this new normal and likely never return to pre scalper prices."
[deleted]
"Hi.

I work in a mgmt retail position for a large company. Some (most of actually) of what you said isn't quite right.

>Now combine that with a very active community of thief's that use stolen PayPal, visa, and other payment methods to buy items. By the time it's get flagged as stolen, the seller is eating that cost because if you want to report the issue, well, good luck dealing with French of Romanian police. I also do not like it but as long as our justice system is so fragmented in Europe, well ...

For paypal this can be true. Chargebacks are a thing, however with many other payment methods including credit card, this isn't so. 
In EU you should explicitly require 3DS as mandated by PSD2 ratification. As such, the risk shifts to the cardholder/bank. You can easily limit payment methods, for certain customers. It's not particularly hard, unheard of or illegal.

>The issue is also, lets say they ship to Belgium. Maybe they have less problem with scammers there, but if you ship to Belgium, the next guy in Romania can say ""hey, you are discriminating my EU country"" (what technically is correct, it is a form of discrimination and that is against some EU laws of goods sold in EU). So, it became ""We simply ship to nobody"".

This is also not true. You can discriminate countries and indeed you do so. Go to amazon.es and try to order a battery operated item to your native country. Chances are you will not be able to due to safety regulations that make sending stuff around more difficult than in-country.

Giving Spain or Portugal as examples. These two countries have islands, some of which do not have airports and the commodities are shipped via boat. Even within these countries it was and is possible to have shipments denied - especially for large appliances. I believe, recently Portugal had a law change that makes it so national delivery must be possible but my memory is fuzzy and cba to google.

In reality, it's very much a business choice. You have to factor in logistical cost, mis-shipments, damages during transport and all the customer support headaches that come from that. Further, now you'd have to probably support different support languages, have your website calculate shipping costs to different regions (more ship vias, perhaps different carriers, different cargo calculations at warehouse/hub/stores/whatever, etc) and then incorporate all of this into your accountancy and forecasting.
Perhaps you'd need to cross-dock somewhere (big appliances often do, due to size), etc. Dropshipping now becomes an issue as it must cross-dock in your facilities if the supplier can't deliver there, effectively transforming these items into long tail items - once again adding risk/cost to you since now you're responsible for everything and not the supplier as well as making the lead time longer...

In sum, all of this is added complexity and at the end of the day it's just a decision of cost vs net margin vs growth and I'd say for this company the numbers just aren't there to justify all of this hassle. I could go on for hours on this. There's a lot more stuff to do when selling EU-wide than just worrying about payments and frauds..."
"> So i hope this explains a few things...

I mean, we european citizens buy and ship stuff 'round Europe all the time, it's just German shops that can't manage to have an english site and ship stuff. Also your example of ""Romania"" or ""Belgium"" is ofc racist as fuck, like there was no VISA or Paypal business there...

Cost of shipment is not important when it's 10e on a 500e GPU, also we european citizen are used to pay those 10e anyway so it's not like German shops woin't be competitive.

BTW: I often buy used Thinkpads and most of those come from Germany (and UK) Ebay.

Amazon Renewed is interesting as well as they offer some extra garantees like batteries at 80% and 1 year ""amazon"" client assistence. Amazon Warehouse is weird as it looks like that half of the stuff you buy from Amazon is ""open boxed"" anyway, so I use it quire a lot.

The thing I really hate is FB Marketplace, there's all kind of crazy there.

Thanks for Kleinanzeigen.de, I'll give a look at that as I'm buying one more Thinkpad 480s, yet I have to say that the German Keyboard layout bothers me."
"Why would you think they want to make more money when they could make less money?

If only AMD had the same lofty goals!"
Also you need a recent system with ReBAR. A lot of the appeal of cheaper cards is being able to throw them in older PCs.
very true, DX11 in particular seems to have issues on Arc.
Yes old games can be a problem but if you only play modern games then Arc is surprisingly decent.
I like to think they just need to work on their backwards compatibility.
"> It all came down to shit drivers.

Assumes facts not in evidence. Rumor is that it was a hardware defect that was eventually worked around in software and/or fixed in later steppings."
Lol not sure if serious, but, yeah... a 3080/3090 is gonna give you a lot more options than a 6800xt/6900xt. It's got tensor cores. And more efficient RT cores.
It perfectly makes sense because is -100w which is HUGE
[deleted]
According to techpowerup the 3060ti is 20% faster than the 3060 12GB. I expect the 4060 to slot in below that maybe 5-10% slower. Even at 10% slower you do get DLSS3 which is usable with 8GB of VRAM, as long as you keep your VRAM in check that is a big advantage over the 3060ti which has the same amount of VRAM. If the pricing is similar I'll take Ada with warranty over Ampere without.
">This generation is nothing like previous gens

Skip it. People skipped the nvidia 4xx generation and lost no sleep."
"he's correct. 

https://www.techpowerup.com/review/nvidia-geforce-rtx-4060-ti-founders-edition/33.html

The 4090 has the worst performance/$ at all resolutions (ok technically the 6500 xt is worse at 4k, but that's not a usable card at that resolution) of any card you can buy.

The real point is that previously the '90 card was a big price hike over a slightly slower card, and now it's a big price hike over a significantly slower card.

This is not an improvement lol, they just won't sell you a great product for less than $1600 now, whereas previously they would.

E.g. 3080 for $699 vs the 3090 for $999. The 3080 saved you 30% and was 12% slower. The 4080 is is 21% slower for 25% less. That's not a very compelling price difference. As a halo product, the 4090 is arguably 'better value', it's more about the absolutely awful overall selling price , and the lack of similar cheaper products."
"No it isn't. People keep saying this but by whose metric is it overpriced? There aren't market guidelines stipulating that it HAS to cost under a certain amount. It's selling quite fine. 

Just because a ferrari is $250K does not mean it's overpriced. Companies don't owe anyone affordability."
I like RTX 4070 too. Problem is MSRP too high. I live in SEA and after taxes, the card prices the same as RX 6900 XT.
"The 3070 equalled the 2080ti for much less money.

The 4070 equals.. a 3080 for only a bit less money.

This is why people aren't cheering

Edit: 2080ti was v close to titan RTX iirc - so 3070 equalled the best of the previous gen.

3080 wasn't as close to 3090, which means the 4070 isn't close to equalling the best of the previous gen."
The 4070 is the only mid range card that is even worth buying this generation. Great 1440p performance with RT at less than 200w power draw. Very efficient and quiet. DLSS FG is just a bonus.
"what are you even talking about? The 3070 was equal to the 2080 TI (and beat it at ray tracing) while the 4070 can barely match a 3080 (and performs significantly worse at 4k).  
The 3070 was a MUCH better value thats why people cheered for it."
"4070 got slammed by the (a) x70 is supposed to be $329 forever! bandwagon (because of the one singular time they dropped it that low), and (b) general hate bandwagon around Ada.

4080 and 4060 Ti are unsalvageable at their current pricing, and 4070 Ti and 4060 16GB will be mediocre, but 4070 is actually a solid card at a solid price and has *enough* VRAM, although 16GB would have been nicer at that price (I understand the hardware reasons ofc).  4060 8GB and 4070 really are both great cards given the way node costs/etc have risen.

But x70 is such a popular segment and everybody loves the 970, and remembers the fake non-FE MSRP that nobody followed for the 1070.  And everyone is just so primed for ""new cards are bad, don't buy"" that even actual good deals just don't penetrate the hivemind.  It drove me nuts that people didn't think the microcenter deal (effectively $500 for a 4070) was even very good, like that was just ""ok"" to people.  That was actually an amazing deal and the alternative is going to be shit like the 4060 Ti 16GB.  People aren't going to like whatever Navi 32 product AMD farts out at that price segment either.

People also love to do comparisons against the 3060 Ti MSRP which was literally never at MSRP until like a month ago, or mix and match the 3080 10GB (cheaper, smaller VRAM) against the 3080 12GB (no official MSRP), and the 4070 really is the best of both worlds and also $100 cheaper.  Same for 4060/Ti, people start compare against the 3070 performance and mixing that with the 3060 Ti price and then whining that it doesn't stack a bunch of additional improvement on top of that.  4060 Ti is still mediocre but 4060 being 3060 Ti perf for $300 is really pretty great actually, you're 25% cheaper than the very best card in the 30-series lineup that literally never hit MSRP until like two months ago because it was too damn cheap for partners to want to make it.

FWIW I do feel that 7900XT has really gotten the pitchfork brigade unfairly too.  Yes, bad card at $900.  At $700 it's getting down into reasonableness - although it still has the same struggles around FSR/DLSS except at 4K.  But I just don't think AMD cares, they can't reasonably fix it without some kind of ML core, so they're just gonna cater to the ""native res is the only res"" crowd.

On the other hand, I also am not expecting much from the Navi 32 cards.  I think like the 7600 they're basically going to slot in over the prices of their RDNA2 performance equivalents, while being weaker in some ways.  Again, like, whatever N32 AMD slots at $500 is not going to be better than a 6800XT, it's gonna be similar-ish but generally a cheaper weaker card that leans on the RDNA3 gimmicks to hold performance up, just like NVIDIA leans on DLSS to hold performance up.  Just like 7600 didn't blow away a 6600XT either.

That's always the tradeoff when you go from a high-end previous gen model to a midrange current-gen model... like 1080 Ti vs 2070/2070S.  The 2070 has a lot of numbers that are lower than the 1080 Ti, because it's using the architectural improvements to push the cost downwards instead of push the performance upwards.  It's a weaker card that tries to get more out of the performance it's got, and that makes it cheaper.  But people want number to go up, forever, and honestly that's not *quite* how it worked even in the glory days.  970 has less raw memory bandwidth than 780 Ti, and so on.  Architectural improvements have always been used to mitigate those cost-reductions while keeping the actual performance going up."
The best 70 class was the 3070 period. (At msrp)
"I said over 30 FPS. You can easy hit stable 30 with just upscaling alone.

And no, it doesn't hurt latency ... you won't have higher latency playing at 30 FPS than 60 FPS with frame generation. If anything latency is lower because of Reflex."
"Yeah, I agree with that part.

The demand/availability on 30 series and the scalped pricing royally fucked the market. Weâ€™ve had 3 years of ass pricing, yet the 4090 is a shockingly good deal all things considered."
"Yeah this is also a reason I didn't get an Arc. 
My PC has a 2700X so no ReBAR support"
Yeah, I strapped a 3060Ti to a 3930K (which is technically a 2nd gen Intel).
Even some new games have problems. Those that do run sometimes have random graphical bugs.
"They do, but it's unclear how much they will or even can. There are hundreds of DX11 games, catching up with that back catalog is a colossal undertaking. Nvidia and AMD could both do it piecemeal at the time as the games came out, but Intel has to both go through that list to fix issues *and* support the new releases coming out now as well.

It's likely that popular DX11 games will eventually run fine on Arc, but anything niche will be a crapshoot."
That's fair.  I don't know what the actual issue was but drivers fixed the issue after 4-5 months.  Actually updated my drivers again a few days ago from ones that I had running from mid-2021 and noticed better playability in some games.
-300w, you say?
"Every generation it's easier to skip.

720p to 1080p was an upgrade.

1440p to 4k? It's kinda noticeable if you're looking for the differences.

30FPS to 60FPS was an upgrade. 

144FPS to 240FPS? I guess some people are at least *technically* capable of detecting the difference..."
Sure, but after multiple cycles of this you still get stagnation or weak price:performance movement.
Been sitting on a 1660s after my 1080ti died. I ain't buying a high value GPU again. These prices are eye gauging. and you know what? I can play 90% of game just fine. There's no need, unless you want blistering fps on the top tier of games that are designed to promote gpu purchases.
That's it, I died of cringe.
"> The 3070 equalled the 2080ti for much less money.
> The 4070 equals.. a 3080 for only a bit less money.

Mostly, this is because Nvidia lied more about 3080's price. Comparing with a nearly fictional $699 3080 doesn't feel like an appropriate baseline to me. Street price for 3080 was comparable to 2080 ti street price, if not higher, for most of the product run.

Also, 3080 and 2080 ti are the same tier of product, both being cutdown 102 dies, and provide similar uplift over the previous gen's top card."
">And everyone is just so primed for ""new cards are bad, don't buy"" that even actual good deals just don't penetrate the hivemind. It drove me nuts that people didn't think the microcenter deal (effectively $500 for a 4070) was even very good, like that was just ""ok"" to people.

It feels like people have completely lost the plot. Nvidia releases some bad value cards, like they do every generation, and people have lost the ability to see good value cards.

>FWIW I do feel that 7900XT has really gotten the pitchfork brigade unfairly too. Yes, bad card at $900. At $700 it's getting down into reasonableness - although it still has the same struggles around FSR/DLSS except at 4K.

AMD just can't stop itself from overpricing their cards, getting lousy reviews, and then dropping the prices down to good value when nobody is paying attention. I really don't get this strategy.

>On the other hand, I also am not expecting much from the Navi 32 cards. I think like the 7600 they're basically going to slot in over the prices of their RDNA2 performance equivalents, while being weaker in some ways.

Full N32 would make a pretty attractive 7700XT. The problem is that AMD misnamed the N31 products, so it's gotta pretend to be a 7800XT. More bad reviews and post-launch price drops incoming."
"4080 could be a good card if they drop the price but the 4060ti is a garbage card at whatever price. The specs just don't match. 

Like the pure calculation power of the card is good but it is held back by the bus an 8 lanes. Should have made it slightly slower but give it 160 or 192 bit bus with 10 or 12 gb of vram and 16 lanes."
"The 1070 was by far the best in the somewhat recent history:

* 670 and 770 aged poorly due to Kepler architecture/low vRAM;
* 970 was very good, but the 3.5GB made it a bit worse than amazing;
* **1070 had everything - power of the old flagship, amazing efficiency and tons of vRAM to spare;**
* 2070 was a joke from a pure perf/$ perspective;
* 3070 would have been the best but the 8GB vRAM are barely enough and not an improvement over 1070 at all in this regard.

4070 looks good until you compare it to the card it replaces under the hood - the 3060 Ti.

It's definitely the best card overall in the current market, but there's no way nVIDIA could not sell it as 4060 Ti at 499$, making it actually very good when you account for inflation.

But yes, I know, they already have the best thing on the market, AI, yada-yada. 

That doesn't mean it's even comparable to how good the 1070 was back in the day, though."
"I consider ""playable"" to be 60+ FPS average, with 0.1% lows above 50.

And yes, DLSS 3.0 *always* [adds latency](https://youtu.be/92ZqYaPXxas?t=1707)."
Yo im with b450 and 1600af with ancient rx 460 and after updated to latest BIOS i enabled rebar. Wanted to prepare for new GPU, gonna get 4060ti i think. So dunno ur motherboard but u can have rebar with this CPU
"Rtx 3060ti is close to 220w with 6x vram. Regular which is pretty rare, maybe almost non existent idk if they still produce is around 200w. 

The 4060 is 115w?"
"> 1440p to 4k? It's kinda noticeable if you're looking for the differences.

It can be anything from undetectable to painfully obvious depending on:

- The size of the screen
- Your distance to the screen
- Your eyesight

I have a pair of 4K 27"" monitors at work and it's kind of unbearable for me to go back to my 1440p 27"" monitor at home, but I am sitting less than a meter away in both cases."
">144FPS to 240FPS?

No I think path tracing is next. Lighting in video games has been smoke and mirrors my entire life, just a series of shortcuts and approximations. To my eyes, Cyberpunk 2077 at 720p@30fps DLSS upscaled to 4K on an OLED fully path traced was a Holy Grail moment. It looks so good."
ray tracing is the next thing.
">weak price:performance

I waited very patiently with a 1060 unable to reach 60fps anymore to buy a 3070 at launch. If 5xxx is bad too I can wait for the one after but it will be increasingly painful. I sympathise with anyone who skipped 2xxx and 3xxx."
I bought a 3070 at launch and count myself very lucky. I skip over all these doom and gloom GPU  posts. When I saw the phrase 'price drop' I clicked to see what it's all about. I'll skip 5xxx as well if I have to. It would be like still using a 1060 in 2021 but it's uncommon two generations back to back are awful.
">Mostly, this is because Nvidia lied more about 3080's price. Comparing with a nearly fictional $699 3080 doesn't feel like an appropriate baseline to me. Street price for 3080 was comparable to 2080 ti street price, if not higher, for most of the product run.

Not always, COVID and mining affected it shortly into the cycle, but if they hadn't happened, it would have been MSRP.

I got my 3070 at msrp mid COVID 

>Also, 3080 and 2080 ti are the same tier of product, both being cutdown 102 dies, and provide similar uplift over the previous gen's top card.

They differ significantly in how close they are to the titan/90ti series cards though"
">	AMD just can't stop itself from overpricing their cards, getting lousy reviews, and then dropping the prices down to good value when nobody is paying attention. I really don't get this strategy.

I think it's to avoid the 2070 Super/4070 Ti response.  If they actually *announce* a way lower MSRP then NVIDIA will cut the prices immediately.  If they overprice it and let it float down organically, I think the sense is that NVIDIA generally doesn't care about that.  I donâ€™t understand why nvidia treats it differently but it seems like they do.

You can see like the 7900XT vs the 4080/4070 Ti... when AMD launches the 7900XT/XTX then NVIDIA immediately launches the 4070 Ti in response, but if they just keep letting the XT float downwards *hundreds of dollars*, nothing.  Same for 3060 vs 6600/6600XT/6700XT, or 3070 vs 6800/6800XT/6950XT... AMD just floats the prices right on down and NVIDIA studiously ignores it.  And of course the classic 5700XT leading to Super, 290X leading to 780 Ti, etc.

So they take it on the chin in reviews, but they get the actual sales out of it.  Yeah, it'd be better to get both, but, they kinda can't get both.  The only thing they get from that is lower margins.

And I do think margins are getting to be more of an issue in the low end than people think.  PHYs don't shrink and are starting to dominate the area of low-end cards, that's why they kept N33 on 6nm too.  Launch at $279, sell at $200, OK, but they can't afford to launch at $200, have NVIDIA respond, and be forced down to $149 or whatever on an 8GB card."
"On paper, 3060 Ti was phenomenal. In practice, the ""$399"" 3060 Ti was usually more expensive than the ""$479"" 6700 XT. This was a very difficult card to get your hands on even at $600.

Nvidia would certainly make money on a $500 4070, but they don't need to improve their offer. It's not like AMD or Intel is going to push them on this point, and they have plenty of demand in the server market."
"8GB for 70 class is good, I agree that 12GB VRAM would have been good, but even then, at msrp 3070 was great. 3070 slightly edges out 1070 imo.

4070 is not good at all in that regard."
And 99% of people don't. Okay.
"Oh really? Damn, I'm gonna update the BIOS to check it then.

Thanks for letting me know"
"The practice was due to COVID/ETH mining alone, those are now gone, the demand isn't there anymore, it's only about AI who might indeed cannibalize the chips allocation on desktop - which I guess is one of, if not the main reason of releases such as 4060 Ti.

I know nVIDIA doesn't need to improve their offering, but that means that their card offers worse value than the good old days where they released some killer GPUs, that's all."
"No, 1070 was better than 3070 because 1070 had more **than enough vRAM after 3 years**, the 3070 already doesn't or is very close at best, and it's only going to get even worse.

In terms of longevity, the 1070 already beat the 3070, it's not even close, making it arguably a better card.

If the 3070 had at least 10GB, let alone 12GB, I would have agreed."
Be careful. Its risky updating BIOS. For example for my ASRock Gaming K4 ppl were saying that they dropped support for old CPU coz limited size ROM. That was not true otherwise i would be needed new CPU. But im keep using 1600af for now.
"The VRAM is unfortunate, and 6800 is better imo. But, 3070 was the first 70 class to play everything at 60 fps at 1440p, and even 4k. I think that alone slightly edges it over 1070 (not 1070ti). And for most games, 1440p/1080p 60 will not cause VRAM issues for 3070.

I remember I was struggling to hit 1080p 60 fps with 1070 in RDR2. When even after 3 years, we can hit 1440p 60fps with 3070 on new games.

I dont even think 3070 was the best card on msrp on 3000/6000 series. It was 3060ti. However, it slightly edges 1070 imo."
[deleted]
3 years later, double the performance from my 5600XT (which would land around a 6800 non-XT) is still almost double the price so yes, hardware is still GROSSLY overpriced.
Short answer yes, long answer yesssss
For me Plan A was wait until prices normalised, this will not happen (ie 1080ti for Â£720), so I have moved on to Plan B which is wait until mid-tier (7900XT equivalent) can crush 4k like they can 1080p.
There has been virtually no improvement in price-performance for the past 3 years now.
">GPU Pricing Update: Hardware Still Overpriced?

Yep."
When 4080s and 7900xtxs are 600-800 ill think about it
"Watching the mid-grade cards now and it's kind of hilarious and sad at the same time. Cards will ""go on sell"" for the prices they were for months last year. This is while other 6700xts (or whatever) are still listed for 2x as much or more like this is peak crypto. 

Speaking of, you can find no end to used cards listed for much more than what a new card costs. 

Oh, and many cases of cards listed higher than the next step up at the same retailer because fuck why not."
Iâ€™ve got a chance to buy a 3080ti founders edition for $650. Is that a decent price?
GPU's are about 2-3X the price that they really should be. I'm just going to wait until I can get a GPU that will be good enough to warrent an upgrade from a 1070.
I am just gonna sit there with my 580 for next few years. If it will die I will just get something "cheap" like 6600 (maybe even used). I so much wanted a ray tracing gpu but it seems like I have to wait either for RTX 6xxx series or Intel. I am still not sure how Intel can have better rt performance than AMD on their first try but well. That's just sad.
"Yep lol, their thinking sand's not bad but still overpriced to heck :-P C'mon i'm sure we can 1/3 prices of consumer electronics and goods.

Inflation's still a problem all accross occident."
"It is for me, it's not even that I can't afford it - I just won't pay Â£800+ for a 4070ti. It's getting stupid now.

On the plus side, thanks to the pandemic I bought a 6600xt a while back as a temporary upgrade, and discovered that AMD have actually started making acceptable drivers. It's a great card for 1080p.

I hope Intel's foray into the GPU market is successful, it definitely needs more competition."
The real problem is AMD/Intel. No real competition for years.
https://www.youtube.com/watch?v=9kiOLC2Ca_I
Anyone considering we are using less than 5nm nodes to produce these chips and costs increasing is just a natural product of production advancements? I don't like higher prices either but aren't they necessary when FABs producing these high end chips are incredibly expensive to manufacture?
Nvidia definitely is. AMD is slightly overpriced (see november pricing on that chart for GPUs like the 6600), but not in a BAD place.
Meanwhile lemmings go buy a $1000+ annual upgrade iPhone without batting an eye for a 5% upgrade. 3090 performance for $800 NEW is a better and cheaper than what majority people paid for NEW 3k series cards during covid but everyone forgets that. *gasp*
Hereâ€™s a question, why canâ€™t the manufacturers use the same die to make it cheaper, and just optimize it better.
A friend of mine actually planned on switching to console because he deems his gpu upgrade to expensive. And the card he has now can still play esport titles. I'm pretty sure these prices are going to heavily impact the pc gaming community in the long run.
"I also just *love* how well it works out for Nvidia calling the card a 4070ti. So many techies end up comparing it to the badly priced 3070ti instead of the 3070. The same goes for 3090ti and 3080ti comparisons.

How does something this simply totally clown on numerous reviewers?!? Does anyone here recall a single ""3070ti is an amazing value card"" review? No? Oh right, they (probably) don't exist because the card is more expensive and still worse than the 3070 in some ways."
Just out of curiosity, what was the inflation rate over the last 3 years?
Don't expect 50%+ improvements at the same price point from gen to gen anymore. Each gen is getting exponentially harder to fab/design.
"> hardware is still GROSSLY overpriced.

why do you say hardware? many things stayed the same, some even got cheaper. things like psu, fans, cpus, ram either stayed the same or got cheaper. motherboards got more expensive because pcie4/5 simply is very expensive.

gpu is the exception"
"A scalper recently snatched a 400â‚¬ 6900XT from me by paying 700â‚¬ for it and then reselling it for 800â‚¬. 

Market's fucked"
In the same boat with my 970, waiting for 4k to be a "yes" and not a "maybe"
Meanwhile they're crying because GPU sales have sucked. I'm still waiting to upgrade from a 580 and I bet many others are too, holding out until something of value comes to the market at the right price. I'm not willing to spend more on a GPU than the rest of my system combined, not when Â£500 can get you a whole system bar GPU that has serious processing power. CPU value is insane nowadays compared to GPU.
These companies are doing everything they can to choke 4k and 1440p on everything but their flagship cards.
"I'm still running on a 1070 and it would be a problem if there were any new games worth playing. 

But looking at that chart it's not that terrible if you're willing to buy used. A 6700 XT or 6800 do 65-80 FPS at 4k are around $300-400 even in my EU market. Not amazing, considering their original MSRP, but not too bad; it's within the amount I'd be willing to spend on a GPU and they do manage 4k well, while I only have 1440p."
"That what I'll do, but I don't need 4k.

I'll keep games in backlog (not like there's many interesting ones to play anyway), and after a period of 4-5 years, I'm gonna buy an older gen GPU, even if it's second hand, and play those games at 1080-1440k resolution, and then repeat. 

Fuck giving money to these greedy companies for keeping their share value high. I'd rather buy second hand, do the job with the gpu, and move on. 

The prices are a fucking joke."
"> 1080ti for Â£720

Nobody is buying a 1080ti for that price. Just because someone is asking for that, does not mean it is the market value."
You can have a 3090 for Â£720 if you're lucky enough.
If GPU patterns continue isn't this almost assuredly happening next gen?
"I have been waiting for a new flagship card since I got my new job with the 2080Ti. I missed out there because the model I was after never came to NA (the Aorus Turbo). Then for the 30 series I had enough money from savings that I was going to get a Titan, I was going to get a 3090 but Nvidia decided to not allow blower coolers which work better in my use case. I then decided I would get *any* top end card that would actually fit in 2 slots and still nothing besides the one Gigabyte 3090 Turbo that still goes for crazy prices.

This year I have just decided to get a A4000 of ebay since they can be found for $400~$500 and are still an upgrade for my 2060/980Ti."
Plan b might happen within 2 more generations, id suggest getting a console in the meantime if your gpu is really really far behind but cant afford 1k+ gpus
I wanted something that can get me to 4K now that I got an OLED TV, but I'm coming to the realization (esp as I play mostly switch games w/ my kids) that... I really don't care enough.  If I run into something that makes me want to upgrade I'll change my mind, but until then I'm totally cool w/ my i5-4570 and 1070 for another few years.  A couple years will probably be enough to show us whether there's any future for DLSS3 as well.
High-end "value" cards like the 3080 FE were the bait part of a bait & switch. I saw this firsthand at a physical Best Buy drop where they got dozens upon dozens of overpriced AIB cards, but only six 3080 FE's.
Yup same. Wait a few years like I did with my 6900xt and get em for cheap. I refuse to pay over 850$ for a top end model gpu
Sounds ok to me
Very good deal. I paid MSRP for mine and don't really regret it, tbh.
I had one and I sold it, awful noise. Don't buy it unless it has been re-pasted / re-padded.
Good price
I think that's a great price
"> GPU's are about 2-3X the price that they really should be

nonsense. financials are public. they are being sold for 2x to 3x the cost. no company sells for the cost."
"> GPU's are about 2-3X the price that they really should be.

nah, prices weren't that cheap even back in let's say the maxwell days, let alone the kepler days.  GK110 cutdowns launched for $999, which is now $1274, full GK104 launched at $499 which is now $637.  And people often ran two of those cards!  And also CPI doesn't describe computer hardware and the inflation has been higher there than elsewhere due to spiraling costs and lackluster shrinks from newer nodes.

people are just making up fantasy prices.  actually GPUs are 27x more expensive than they really should be!!!!!

the ""good scenario"" is GPU prices dropping like 20%.  Prices aren't going to drop 50-70%, get fucking real, that's just wishful thinking.

Everyone including NVIDIA and AMD wish they could do that, nobody is *excited* about mediocre generations, everyone wishes they could introduce a product that would make your old shit obsolete every year so they could sell you a new one.  But it's just not going to be possible without some fundamental breakthrough in silicon that brings back the glory days of moore's law.

Doesn't mean it's worth upgrading every year, we are now in the CPU-style model where a good GPU can easily carry you for 5 years and that's a good thing for the planet overall.  But crying doesn't change the physics involved - there is some gouging for sure but it's like 10-20%, not the 70%+ reductions that some people are demanding.  That's not going to happen."
"Sitting on my 480 until it dies, too.

I've got a nice monitor in need of a GPU upgrade but the prices and games are not really there for me."
"> I so much wanted a ray tracing gpu

It doesn't make much of a difference. I tried Portal RTX and it wasn't apparent unless you stopped to look at the lights and how the shadows moved."
how is Intel at fault lol they didn't even make a dedicated GPU until last year
"Sort of; not exactly. The 3070 was a 392 square MM die and the 4070 Ti is a 295 square MM die.

While TSMC 4nm process isn't as cheap as Samsung's 8nm process, sure. TSMC's 4nm process is a specially-tuned run for Nvidia AFAIK and would be using 5nm wafers, which cost around $18,000. I'm unsure of Samsung's cost post-2021.

So, the die is probably something like 15.5 by 19mm. They would be getting something like 180~190 dies per wafer.

As you can imagine, that's not terribly much per GPU die. Attaching the dies onto a hard silicone substrate, probing the dies and testing, packaging them onto the board that gets soldered to the card itself, the VRAM, the power delivery circuitry, the connectors, heatsink, *all the other chips*, are all extra cost and are relatively fixed. They're not going to vary terribly much from generation to generation.

Nvidia is just trying to say that their R&D is worth an extra hundred or two dollars while delivering somewhat lackluster gains."
"> cheaper than what majority people paid for NEW 3k series cards during covid 

So cheaper than inflated prices? Doesn't make it a good deal lmao"
"If they're doing an annual upgrade, they're probably trading in last years phone. If buying from Apple you can get $470 credit on a 13 Pro. That makes a base 14 Pro $530. 

If you aren't buying outright you're paying $40 a month to rent a brand new phone with warranty for accidental damage, which is $10 more than the trade-in value if you had bought it outright. 

If you're like most people and keeping your phone for 4 years and buying outright, it comes to $20 a month total to buy one of these things, for a device most people use more than 4 hours a day, sometimes more, for a lot of actually useful shit. 

It's very different economics than a GPU people use almost solely for entertainment (not many 4090 customers are buying it for AI work). 

I know you might have built up a bit of a superior identity around shitting on people for buying a phone you wouldn't, but they're hardly lemmings."
This is EU pricing, where Android dominates and Apple is actively made fun of.
"People fall for this shit too, I've seen this repeated here with the 3090 Ti as well. 

Like ""why are you upset at the 4080, it's as fast as the previous $2000 card""!

Yeah no shit, and it was a horrible card back then."
"Pricing has been terrible on the nvidia side since the 2000 series. And it's gotten so bad recently people are starting to look like the 2000 series was ""how the market should be"". No. That was the 1000 series, which stiill largely followed the same pricing that existed since the 400 series. 

2000 was a price gauge, 3000 had COVID and crypto related issues, and now the 4000 series is another price gauge."
Because the moment companies start getting called out, this sites start getting dmca not getting review samples etc.
"> [3070ti] is is more expensive and still worse than the 3070 in some ways.

Could you please elaborate how / what area the 3070ti is worse than a 3070? Looking at the spec sheets, I don't see the shortcoming."
">How does something this simply totally clown on numerous reviewers?!?

They want to get the next nVidia GPU for free, that's why."
[deleted]
[deleted]
"Yeah, people need to realize that Moore law while not dead, is undead.

It's technically alive but you don't get extra performance for free like before. Each node gets more experimental, and even more difficult to manufacture, while shareholders want more profits."
I would love to live in your reality... But all I see is price hikes in the last couple years in all of the categories you mentioned. Even cases are 2x what they used to be.
Dude everything got more expensive, directly or through increased inflation, check out prices year over year in Europe compared to salaries for last 3 years. Even cases were more expensive due to shipment costs from China.
"I think it's because we're in the awkward inbetween where products have been discontinued and are selling out but their successors haven't launched yet. So third parties try to prey on the ignorant who aren't thinking about the new product launching.

But even then, why would someone pay 800 for a 6950 XT when the 4070 TI is like 850 and the 7900 XT is 900?"
"Something similar happened to a friend of mine. 

Got his pc with a ryzen 9 5900x and a 6800xt for 4000â‚¬ in 2021 (yeah I told him to wait a little but he didnâ€™t want to). Sold it this year because money got a bit tighter. Got 1900â‚¬ for it and the dude instantly put it back on eBay for 2500â‚¬ 

Ngl I was kinda impressed"
If youâ€™re still running a 970 any card you buy would be an absolutely massive upgrade. Can you even play new AAA releases?
Similar boat, I'm using an rx580 and just now in the market for a gpu. Seems to be a bad time to shop but I've heard that for nearly 3 years now.
Resolution is a moving target.  980Ti came out almost a decade ago and initial techpowerup review it avged 52fps at 4k.  When will midtier crush 4k like they can 1080p?  Probably when 8k is as common as 4k is today?
You misunderstand. That is the last card I bought years ago for that price.
Dunno where they got that price from. I can see a few on ebay unsold for Â£250, Â£260, Â£210. Don't see any going for Â£720. Highest I can see are Â£350 to Â£400.
No. This is temporary. NVidia and AMD know that if they ever want to sell existing 3000-series owners new GPUs, they must improve price-performance as theyâ€™ve historically done. While theyâ€™d love to keep prices crazy high, thatâ€™s not sustainable outside a black swan event like we had, and poor sales of 4000-series (excepting 4090) and RDNA3 cards reflect that.
Yes but not for 720 quid. The 5080 should be even better than the 4090Ti and Titan but it won't be cheap.
"And for those wanting to buy a GPU that cost $250 or less? Either buy a previous gen card that is used or on discount, or some dodgy s*** such as the RX 6400 and the GTX 1630.

The RX 570 back in 2017 had a launch price starting at $169 (not including the scapler prices from the crypto boom back then): https://www.tomshardware.com/news/amd-rx-580-rx-570-gpu-polaris,34168.html

That's not close to what the RX 6600 currently cost right now, and below that is where you're staring at the RX 6500, GTX 1650 (if you can find a low cost one) and GTX 1630 cards."
IDK about the 30 series, but the 40 series use the meme PTM 7950 which in theory should never need to be changed. Repasting with normal paste could make temps even worse since that 7950 stuff is magic.
"Eh if we compare 1000 series pricing (my own baseline) to existing pricing, 3000 series GPUs are a good 40% above where they should be. 4000 series ones are double. 

So 2-3x might be a SLIGHT exaggeration, but only slight. We're really talking 1.5-2x."
"Yup, the insane meltdown over prices is hilarious.

Wafers are much more expensive on bleeding edge nodes, inflation is a very real factor, and the market has demonstrated that it will bear higher costs.

Redditors are just one step removed from demanding that Nvidia and AMD sell them vidya' toys for a loss.

Every single thread about the current prices being filled with ever increasing hyperbole about prices is just tiring.

No discussion about the actual hardware (what this sub is ostensibly about) just unhinged crying about prices."
I got my 680 for Â£500 I think but it was the top of the line EVGA Classified with a wopping 4GB of VRAM, I replaced it with my 1070 for Â£340. GPU prices these days are silly I thought my 680 was silly money and it was at the time but I wanted to treat myself because I got a new job, it wasn't worth the money at all but considering the new 4080 is more then double that the current market isn't where I want it to be at all. If I wanted to upgrade now for around the same money I paid for my 1070 I would need to get a 3060 which while is more powerful then my 1070 isn't all that more powerful to warrent upgrading.
Yeah in the Jensen quote thread the other day there were people arguing all forms of 'tech is supposed to get cheaper' like it's some kind of constitutional right. Now a couple days later we have another thread of /r/hardware users literally calling for antitrust investigations of Nvidia...
"> we are now in the CPU-style model where a good GPU can easily carry you for 5 years

The trick is to buy in the middle of the console generation. My cheap and cheerful RX 580 is basically an Xbox One X that slots into a motherboard (same arch and TFLOPS, but less bandwidth). Ended up being perfect for playing last gen games at 1080p.

Assuming nice uplift, mid range Ada and RDNA3 will probably be a good time to jump in on this gen if your goal is just console replacement."
Intel waited on sidelines for way too long to make a dedicated GPU.  I would surprised if the next Intel generation of cards come out with a high end product that competes with AMDs high end card. And Nvidia's 4900 is beyond that performance.
Lackluster gains? 4080/4090 is lackluster?
What? you can get way higher credit than that if you trade in when the phones come out.
"I'd love to see the % of people who actually keep their iPhone past 1-2 models, it's probably epic low. Can you also explain the % performance boost the iPhone 13 vs iPhone 14 received to justify a nearly $500+ trade in price tag? Anything with an apple logo is overpriced and from China. 

I'd also argue an average pc gamer definitely uses their pc system for more than 4 hours per day with likely less on screen cell activity to match."
Who cares if it's apple or Samsung? It's the point of a $1000+ flagship that's less than 15% more efficient or quicker than last years $1000+ model. Millions gobble it up annually, statics prove that sadly. Also, msrp is a joke, nobody pays msrp for any significant purchases in 2023, time to face reality of inflation and increase cost of living year over year.
Gauge (pronounced gay-j) is a measurement tool. The word youâ€™re looking for is gouge (pronounced gow-j)
"Nvida a master plan to infinitly raise prices and still sell whatever they want starting with the 2000 series. With the 2000 series they increased prices and everyone called it bad value, but when they kept those same prices (at least for msrp) for the 3000 series every review was talking about how great valiue the 3000 series is.
I think the are planing to do the same thing with 4000 and 5000 series, massivly raise prices for the 4000 series then keep the prices for the 5000 series the same, suddenly every review will talk about how great value the $1200 5080 and it will sell like crazy even at that higher price, becouse it's so much better value then the 4080. 
Then they just raise prices again for the 6000 series and keep them the same for the 7000 series and bam ""great value"" at even higher prices.
This way they could keep raising prices forever and still sell a ton of GPUs since the generation where they don't increase prices will always be ""great value"" comparted to the one where they did."
"3000 series cards (with the exception of the 3090, which was never meant to be a value card) were greatly priced before the pandemic mining era; that's up until the 3060 Ti. Everything after that was just a shameless cash-grab to try and make as much money as they possibly could from mining inflation.

What's scary is how Nvidia kept building up on overpriced 3000 series products for their 4000 series, despite mining being dead. It's even scarier to see consumers defending Nvidia because ""a 4070 Ti is better value than a 3080 Ti"" when the 3080 Ti was never good value to begin with. When you mention the 3080, they'll usually resort to the ""but no one could buy one"" excuse."
True, there are barely if any reviewers large enough to be able to absorb a hit like that.
"30% more power for 6% more performance. The increased power consumption was almost completely due to the GDDR6X RAM, which ran hot. Iâ€™m not sure how many cards have died (or will die) because of this, but people worried that high VRAM temperatures would make the card less reliable. This was an especially big issue when mining was common, as mining heavily used VRAM and caused it to get extremely hot. Lots of 3070Tis were sold during the mining craze so there is some chance that if you buy one now, it has potentially had its RAM overheated. Is this especially likely to be a problem on any given card? Maybe not, but itâ€™s definitely not an *advantage* for the 3070Tiâ€¦ 

Either way the performance increase was so small, it was basically a normal 3070 for $100 more. Plus all the extra youâ€™ll pay in power costs over the lifespan of the card."
36% higher power consumption, still 8gb of vram. It's not a good card. https://tpucdn.com/review/gigabyte-geforce-rtx-3070-ti-gaming-oc/images/power-gaming.png
Power efficiency is one
"For anyone wondering, this is the correct answer. The US Consumer Price Index (CPI) for the past 3 years (Dec 2019 to Dec 2022) grew by 15.5%. Which works out to an average annual inflation rate of just under 5%.

(Most of that inflation was in 2021/2022, so it's not a _consistent_ 5%. But I digress)"
It's way more than 5%
[deleted]
Fair enough, doesn't seem like the market will adjust though. There's sufficient demand to support these prices.
Software too. AAA's are hitting the $70 mark and games like Factorio get inflation price increases.
"Cpus, ram and motherboards are cheaper now. The 5600x used to be $300, a month ago the 5600 was $130. I saw 32gb of 3200mhz ram for $50 in December during a sale. 

No idea where you live but in the US things are way cheaper in than in previous years, besides gpus."
"i bought a 5800x for 459â‚¬ when it was released. this is a 219â‚¬ CPU now. my last ram upgrade was 2019, upgraded to 2x16gb 3000mhz cl15 for 144â‚¬. that same kit now costs 75â‚¬. i bought a 650w be quiet psu in 2019 as well, straight power for 101â‚¬. this psu now costs 108â‚¬, thats not even a 10% increase and a rather natural price increase. 

in regards to motherboards, bullzoid has a video on why they got more expensive, i already explained the reasoning.

but sure, if it doesnt fit your narrative, then go ahead and ignore stuff like significantly cheaper cpus and ram."
"Keep in mind these are European prices. Cheapest 7900XT is 960â‚¬ and cheapest 6950XT is 800â‚¬ (even used). That means you can get the ~20% better performance of the 7900XT plus the myriad of issues for 20% more price. 

The 4070Ti seems to be going for 900â‚¬ so that would be a somewhat compelling offer if you're willing to put up with Nvidia."
Where I live that's 1k with taxes. A $680 6900XT was only $68 in taxes so I save $250 on the GPU. 6950XTs are regularly going on sale for $700 now. The 7900XT is only \~17% faster on average than the 6900XT for 25% more money. Not an attractive proposition.
Yeah, I'm way too risk averse but when you find a cheap offer on eBay buying it and reselling it for more seems like a great way to make some money nowadays, unfortunately.
"Runs somewhat for AAA at lowest. I'm looking at something for emulating, VR, and NVENC, which AMD isn't as friendly with.

* 10 series comes out ,""eh, my card is still pretty new""
* 20 series comes out, ""not much performance upgrade and $100 price hike""
* 20 Super comes out, ""more price hike, barely any price/performance change""
* 30 series comes out, \*doesn't exist for forever, then still at MSRP up to today\*
* 40 series, name of the game is gouge gouge gouge, huge huge huge

I was looking at lower end cards that are better price/performance, but I'm thinking now ""I've waited THIS FUCKING LONG for only a 20% performance bump at $400?!?"" If I'm shooting for the moon, I want the eye candy at 4k. No way I'd give some bum $10 under MSRP for a used card without a warranty."
Get a Rx 6800 500 bucks and is a great 4k card.
[deleted]
it can around 1080p mid ti high
If it's playable, making it more playable is a waste of money.
Most AAA games aren't even worth playing these days
8K is stupid on a desktop pc and future thread necromancers can quote me on that.
Ah, I see.
"Do they, though? When they're bragging at investor conferences about being able to jack the price up hundreds of dollars and people will still buy it, I don't see that as a ""they know this isn't sustainable"" mindset.

The only thing that's gonna make any difference is if these cards sit on shelves and it starts affecting their quarterly reports and eventually their yearly revenue. When the stock holders come for their heads is when we will see price movement. Probably won't be Q1, maybe not even Q2.... But Q3 and Q4 they're gonna start shitting themselves if they're way behind on their yearly projections."
Â£720 when the 1080 Ti was released is Â£880 now though
The next halo gpu won't be $1600 but $2000. I'm sure of it.
"I can find a few RX 6600s on Newegg for around the $250 mark, and idk whether it's a good deal honestly.

I'm honestly more concerned about the sub-$200 market. The only options right now are like you said the 6500XT, 1650, 1630 and the 6400.

It honestly feels to me that these companies no longer give a fuck about the budget PC gamers. AMD hasn't made any announcements for a Ryzen 3 Zen4 CPU, which allowed Intel to slack off (again) and basically repeat their 10100/10105 stuff with the 12100/13100.

Even for the $250-300 Nvidia basically put out a slightly better 1660 Super in the 3050 while AMD did fuck all for the longest time until they cut prices for the 6600, which performs similar to a 5600XT which launched for $279. So right now if one were to buy a 6600 they'd likely pay the same price as a 3 year-old card for almost the same level of non-RT performance, and this like I said is after the price cut for the 6600."
I know 30 series FE have a problem with GDDR6X and bad thermal pads. Mine would ramp up the fans even when the core was running cool (undervolted) just because the VRAM was reaching 106Â°C.
"Costs have soared. A 16nm wafer is less than half of what a 5nm wafer costs.
Memory prices have barely declined aswell.

Nvidia's gross margins mean their cost of goods sold is ~1/3 for gaming GPUs. AMD's is more like 2/5.

Either way, Nvidia's margins have not risen since pascal for gaming. They are a bit higher due to datacenter growing massively of course."
"970 were sold for 350â‚¬ on average. I'm not even sure about the MSRP, it was a time where MSRP was actually higher than the real price.

Right no a 4070 ti is 900â‚¬ at least.

So yeah, 2/3x is a good estimation"
"And also ignoring that corps gonna fucking corp.

Whatever people will pay - that's what they're going to price it at.

And so far, these GPUs do seem to be selling. Maybe not the 4080, but the 4090 and the 4070ti seem to be doing just fine so far."
Oh hey Jensen!
"Agreed, I feel like people really don't understand how good this tech is. When I played HL2 for the first time it was probably at less than 30fps on a 1024x768 monitor and it was *amazing*. Now people on this forum act like it's an insult to play modern games at anything less than 1080p/240Hz or 1440p/165Hz with ultra graphics settings.

Then there's the people acting like AMD is truly incompetent for having difficulties making a product with parts that are *5 fucking nanometers in size*. The fact that we've got this far is practically a miracle and shouldn't be taken for granted."
"> I replaced it with my 1070 for Â£340

That'd be about $400 in 2016 money, subtracting VAT from the 340 GBP

$400 2016 buys you roughly $500 worth today; that'd buy you an RTX 3070 at the $500 MSRP, not the 3060, which is roughly 50% faster

There's a few listings around the usual sellers for a new 3070 at $500, so it's available."
Funny enough, the 3050 performs just like your 1070 and costs the same too.
"Yes? They've more than doubled the transistor budget, doubled the theoretical performance, added mountains of cache, and increased the boost clock by 700MHz... and the performance gains amount to +50% over the 3090 Ti for games that aren't bottlenecked on memory bandwidth, with *all of that*.

Architecturally, it's a bit of a dud. The performance per CUDA core didn't really change, which is alarming. Everything but memory bandwidth did. There's **so** much more silicon under the hood and we're not really seeing the gains from that."
[Prove me wrong then](https://i.imgur.com/Y9TxBuE.png). Apple will give you a maximum of $470 for a 13 Pro when checking out a 14 Pro.
Any evidence that itâ€™s buyers of last yearâ€™s model driving sales of this yearâ€™s model? I imagine mean time of ownership for phones is increasing.
It is still a price gauge
Wouldnt be surprised.
"They followed 2000 series pricing which was a huge cash grab. If we had the same pricing structure as pascal, we'd have the 3050 for around $150, 3060 for $230, 3060 Ti for $300, 3070 for $380, 3080 for $500-600, etc.

4000 series is just pure greed."
"Thank you for that.

I managed to get a retail 3070ti at a relatively good price during the mining boom. I've undervolted it already as I run an sffpc but knowing how to focus on vram will hopefully help me bring temps down a little more.  Can't believe I've overlooked the 'x' vram for so long."
Please show a table of "everything" and their prices then vs now. Then ask yourself, do I consume "everything" in equal quantities?
"Motherboards are definitely, absolutely, not cheaper now than they were back 2-3 years ago.  Back then you could get a decent budget B450 board like an AsRock B450 Pro4 for $80 or under.  A new board with equivalent ports and features now costs over $150.

Also, it's kind of a disingenuous argument to say that the 5600X was $300 when it was agreed upon en-masse when it launched that is was very overpriced and that AMD was taking advantage of their newfound gaming performance.  It sold poorly and prices were slashed to bring it more in line with what it should've been.  Even in 2020 and 2021 $300 for just 6 cores was highway robbery.  Previously they'd always launched both the regular and X SKU at the same time and pretty much everyone went for the regular SKU so instead they decided to change it up by forcing people to have to get the overpriced X SKU for many months after launch.  Before the very overpriced 5600X launched all previous SKUs like the 3600, 2600 and 1600 launched with an MSRP of $200-220.  Of course with inflation prices are gonna go up but if it were matching that you would've expected a product launching at $230, not $300.  They knew exactly what they were doing.


Out of those 3 it's only RAM that has gotten cheaper."
Ah, I didn't take VAT into account, if you can get the 6950 XT for 800 since no VAT vs 1000 with VAT for a 4070 TI then it mak s sense.
"I mean, willing to put up is maybe not the best way to put it. Generally it's the more feature rich product, so it's not a downgrade from a 7900 XT.

It's more about a willingness to support Nvidia's increasing monopoly, in which case, what choice does one have? AMD has equally overpriced products and shitty practices, and Intel is a few generations away from being realistically able to compete directly"
"You'd be a bit surprised, my 5700XT is ~double the performance of my old 980Ti and my new 6950XT is ~double the performance of my 5700XT. 

That means a 6700XT is roughly triple the performance for ~400â‚¬. 6750XT for 400â‚¬ used. 

The 3060Ti goes for a similar price if you don't like AMD."
I went from 970 to 2060 and thought it was an ok upgrade in the same price class. 2060 6G was $20 more than the 970 and was as fast or faster than a 1080. Obviously things got dumb when the 3060 12g was over $600 street price for most of it's production life and the 4000 series starts at $800
"For what it's worth, meaningful performance bumps do exist now. I was very fortunate and got a 3080 via a friend, and it's more than twice as performant in VR than my 1080 was. I went from 100% render scale 15fps to 250% render scale 30fps in large VRC instances,  and even better in games like ITR.

Around 2x performance in horribly optimized AAA fare like Cyberpunk and Darktide.

Iirc, a 4070ti is still around 3080 performance or better, so a 4060 might finally hit your requirements in the long run. A shame AMD isn't stable enough to be reasonable as a primarily VR device; the vram on their cards is extremely enticing. Same for Intel."
"There's some good deals out there if you have the funds and some patience along with some reasonable expectations.  If you're made do with a GTX 970 so far it's not really reasonable to be expecting and to be complaining about upgrading to a GPU that's not able to do 4K High Settings at 60 FPS for under $400.  At that point you are well past the point of diminishing returns.  The image quality upgrade going from 1440p to 4K isn't really noticeable unless you're pixel peeping and/or have a very large monitor.  1440p High settings is gonna look a hell of a lot better than 4K Low settings.

What you can get for half that price is still an insane upgrade over your GTX 970.  [Thanks to driver improvements RX 5700XTs perform the same as the GTX 1080 Ti and RTX 3060](https://youtu.be/1w9ZTmj_zX4?t=534) and come up for $150-170 from time to time.   If you only buy NVIDIA you can snatch up an RTX 2070 for $200, though it is slightly slower than the 5700XT nowadays."
It's a 2 year old card though isn't it? If I'm upgrading every 5 or so years I'd rather get a new gpu so it's not 7+ years before I upgrade.
A 970 is lower than some new games minimum requirements. Itâ€™s a mid range card from 8 years ago. Itâ€™s not a weird question.
Itâ€™s not relevant if you personally donâ€™t enjoy AAA games. If you only like older or indie games then you can use whatever card you want it wonâ€™t matter.
"seems like as the pixel count goes up the more you can lean on upscaling to get you there.  so why not 8k?  might be nice for working with vector graphics on your >42"" monitor, and upscaling a 1440 or 4k or whatever image can look nominally better than 4k.

but without upscaling i agree 100%."
Running the game at 8k is the best anti aliasing you can imagine, even if your monitor is 1080p. Still worth it, if you can afford it.
"Of course we wonâ€™t know for sure what theyâ€™ll do until they do it, so we all are guessing, even NVidia havenâ€™t decided what theyâ€™ll do in 2024 yet. Weâ€™ll have to wait and see. 

Ah, but those cards (except for the 4090) *are* sitting on shelves and selling poorly, thatâ€™s the thing. Sure, some of the other cards sell, thereâ€™s always some percentage of the public thatâ€™s desperate or foolish, but when sales are way below where they should be, and NVidia sits on piles of stock that they need to sell since TSMC has already gotten their money from NVidia, and ofc NVidia needs to sell cards and chips to make a profitâ€¦ yeah, Iâ€™m sure they see all this, and ofc they know then unfed price tiers are lost sales, and again, they have to know that existing 3000-series owners are not going to upgrade if the performance isnâ€™t thereâ€¦ no, they have to improve price-performance, they MUST. This has to be a temporary state of affairs, and Jensen isnâ€™t known for being stupid. Again, barring a black swan event, late 2024 is going to be a way better time to buy GPUs than today."
Yes, he could pick up a 7900 XT card for it, but I don't think the 7900 XT quite "crushes" 4k. He should wait another generation.
"I just noticed that the RX 570 8GB still pulls ahead of the GTX 1650, which just drives home of how terrible the sub $200 market is: https://www.rockpapershotgun.com/nvidia-gtx-1650-vs-amd-rx-570

And for the RX 570 vs the RX 6500, I'd imagine the RX 6500's performance is quite crippled on PCIe 3.0 motherboards due to its 4x lanes."
They were pretty freaking high when they released the 2000 series IIRC.
"970 was like $330 (same as 3060) and was abnormally cheap for a 70 card, most of them cost $350-380 normally. 

But yeah. The fact that 2000/3000 series 60 pricing is clearly in 70 range shows how screwed the market is. And people treat that as normal and fair after all of the supply shortages. 

Meanwhile I'm like ""we're my $250 value champion?"" Then I give the nearly $300 3050 a stink eye, evaluate AMD's options, and go for a 6650 XT."
They were $330 MSRP, but you could get it for much cheaper after Q4 '14. I paid $250 for one in mid 2015.
">970 were sold for 350â‚¬ on average. I'm not even sure about the MSRP, it was a time where MSRP was actually higher than the real price.

GeForce 600â€“900, that's Little Kepler, Big Kepler. Maxwell 1.0, and Maxwell 2.0, were all on ***TSMC 28 nm***, so the 900 series using ""mature"" silicon was especially cheap to manufacture. Every Nvidia generation since then has used better and better silicon."
Inflation between 2014 and 2022 is about 25%. So comparison would be 437.5 to 900. So 2x.
"""Back in my day we used to compute the whole god damn game in our heads and draw every frame with pebbles on sand. The kids these days and their high resolution displays want to have everything without working for it.""

Computational power has always risen over the years and the average consumer has always expected to buy something better every few years at the same price. Doesn't matter if it's 5 nanometers, 1 millimeter or 1 planck because they are not building those using sticks and stones. The engineers who made some of the first CPUs had it no easier than the ones designing them today."
"was it gamers who wanted raytracing or was it the industry who wanted to push the tech to keep their profit machine spinning? 

I still play games with static baked lighting and I think they're fine. I didn't ask for ray-tracing, AI upscaling, bla bla bla. These are expectations that the industry claim that they could meet, so they set the expectation, and so failure to be able to deliver is on them."
Yea there isn't any point in upgrading from a 1070.
lol. the 3k and 4k series consumer gpus are failed cards from the server/hpc side. they where OG design for 800 watt running.
I hadn't thought about it that way.  The improvement in gaming performance vs what it should do on paper tell the story of it being a bit of a dud but the story looks a lot better when talking about compute performance.  Then again, only a small minority of the people buying these cards care about that.
I mean i see the 999 offer right there?
I'd like to see some concrete data myself but all you need to do is stand outside an apple store on pre-order or launch date of the next phone. It's been this way for well over 10 years now too sadly. Gotta have that new iPhone that's 1% or at all physically different than last years. Didn't the 13 and 14 change a few mm dimensions aka enough to force all new phone cases bwhahahahah.
There is a funny "coincidence" with the 4000 series pricing and it is the fact that they are priced as their 3000 series counterparts during the mining craze.
"They didn't. The 3080 featured the big GA102 die while the 2080 and 1080 had the smaller 04 dies. So you can't compare the 3080 directly to the 2080 because the 3080 was a higher tier chip. It was bigger, far more expensive and had more memory channels; yet, Nvidia kept the same $699 price tag. Pascal did have the $699 1080 Ti which was also based off the big die, but, when we consider three years had gone by between the 1080 Ti and the 3080, once we consider inflation, the 3080 was actually cheaper than the 1080 Ti. When inflation-corrected, the 3080 might be the cheapest big-chip GPU Nvidia has ever launched.

The Turing architectural equivalent of the 3080 would be the 2080 Ti (they even featured the exact same SM count) which was priced at $999/$1199. Likewise, the Turing equivalent to the 3070 was the 2080 series, which shared the 256-bit construction and exactly the same SM count (just updated to Ampere's standards), and the 3079 took prices down to $499. Byall intents and purposes, 1st wave Ampere products were a clear price cut from Turing. Even the 3090, built to replace the $2500 TITAN RTX, was also a price cut. It was only the second wave of Ampere products that brought pricing back up to Turing standards. And now, with 40 series, they're taking it one step beyond. I can't help but think of 40 series like 20 series happening all over again."
And those prices would be reasonable today, even with a +10% inflation hike.
I saw mobos for $80 in December and cpus are still cheaper and faster than 3 years ago. Maybe the prices aren't coming down enough for some people but to say they are higher is nonsense.
You need to realize that the majority of users are using the 60's or it's equivalent by next generation Intel should be ready to compete in that level and AMD already has substantial discounts in that area the 6600xt is going for 150 less than 3060.
"It's not just the price or monopoly, it's also the privacy invasion through GeForce Experience, which you have to install if you wanna use Broadcast or Reflex, as well as the worse support for Linux. 

For me, personally, both the privacy and the price/monopoly are the two reasons I wouldn't buy Nvidia new. The behaviour with EVGA/Third Parties adds on top."
"I went from a GTX 980 to a RX 6700XT a few months ago, it's been glorious going from struggling to maintain 1080p/60fps to easily handling 1440p/120+fps.

I basically had to accept that raytracing ain't in my immediate future, but I don't like the RT performance for anything beneath at least a 3080, and really more like the 3090, so that wasn't a big deal for me. It might be more significant for other people."
Yeah looking at the 4080 FE right now to fit my Sliger S620.
Pixels per inch, not pixel count. 8k at 32" is very dense, at 64" you can probably see the difference.
Yeah but that's at the end of Ada's lifespan. Of course the prices are going to drop by then. I think it'll come at the latest Q1/Q2 2024 whenever the Arc Battlemage is released. Supposed to be a 4080 competitor at 1/3 the price, and with more Vram. If Nvidia doesnt drop prices before thats released then theyre going to be losing a LOT of business, possibly permanently because people are so soured on their greed. I will be one of them if they dont get their head out of the clouds quickly. Nvidia's days of milking the populace IS coming to an end, I just think it'll be sooner than later. Especially when those orders start rolling in from TSMC. They can only delay the deliveries for 3 months.
2000 series price increases were commensurate with cost increases. Die sizes and memory grew massively, so Nvidia had to raise prices, but performance didn't improve as much because all the compute, RT, and tensor capabilities they added. So perf/$ looked bad, but Nvidia's margins were basically stable.
So? Cpu didn't doubled their prices even if they use "better silicon".
You're ignoring some pretty real physical limitations when it comes to producing increasingly smaller components. Silicon atoms only sit about 0.2 nanometers from each other which was not as relevant in the past as it is now. They're literally running out of space to make things faster the old fashioned way.
No they aren't. None of the Nvidia server GPUs run at 800W, and Hopper (the latest generation of compute GPU) is entirely different to Ada Lovelace (gaming/workstation). Bigger die, HBM rather than GDDR6(X), less cache, less FP32, more FP16, more tensor cores. The A100 is similarly different to the other GA10x Ampere GPUs. It's not even on the same process.
Architecturally, I wouldn't say failed, but gaming wasn't the #1 priority from go.
"> If buying from Apple you can get $470 credit on a 13 Pro.

Yeah, *from a carrier*. I am and always have been talking entirely about Apple offers and Apple programs for iPhone credits."
"I dont care about bus width and die sizes and other random BS specs people on this sub seem to obsess over. 

60 card = $250

70 card = $380

80 card = $500

That's how it used to work. That's how it ALWAYS used to work. And every generation, we would see healthy gains in price/performance. Sometimes 20-30% during a refresh generation, but sometimes up to double. 

In 2016 you could buy a 480 or 1060 for $250ish. In 2019, we only got a 1660 Ti to replace that roughly, which was a measly 35% improvement. Sure, the 2060 came out, but they wanted $350 for the thing, which was ridiculous. But after 3 years, you'd think we'd get something like that for $250. We didn't. 

Then nvidia did the same thing with the 3060, charged $330 for it and people acted like they were doing us a favor. I dont care what happens in $800 land, because no one should pay that much for GPUs in the first place unless you're some insane enthusiast. 

Either way, yeah, the 1080 ti was $700. And anything above that is a scam. 

But yeah. Going back to my 1060 comparison, in 2021or 2022, i forget when it came out, but we got the 3050 for $250. It was just as 1660 ti with ray tracing. More stagnation. 4000 series, more price hikes, more stagnation. Performance per dollar only improved 50% on the nvidia side. IN SIX YEARS. That's insane. Historically before than we saw a doubling in performance every 3. For reference, the 2500k to the 7600k was also 50%, so we're at intel stagnation levels of price/performance. 

It gets even worse down the stack. 2016 the 1050 ti was $140 MSRP and when i bought sold for around $180. Nowadays, you get a 1650 at that price range, a card that came out in 2018 and was only around 25% better than a 1050 ti. 1660 ti is around $230. In 2018 it was the 1660 and in 2016 it was the 1060 3 GB. Another paltry performance gain. 1070 in 2016. We got the 2060 in 2018. Then the 3060 and 3060 Ti in 2020/2021. More measly 50% gains roughly.

You get the idea. I couldnt care less about die sizes and OMG this is GA102, this is 104, blah blah blah. DOnt care. I care about ""what performance can i get for the money"", and honestly this level of stagnation is so bad it should be triggering anti trust investigations into the GPU market. Because something is fundamentally unhealthy here.

In order to double my last card, i needed to buy AMD, because AMD for some reason is selling their GPUs a whole tier of price lower than nvidia, which is what everyone should've been doing in the first place.

if you want to know what i think GPU pricing would be fair, I'd say, take the cheapest RDNA2 cards over the past 3 months and about that.

6600s for around $200 is fair. 6650 XTs for $250-300 is fair. 6700 XTs for $350-400 is fair. 6800s for around $500 is fair. 6950 XTs for $700 is fair. That's what the market should've looked like all along. Heck i just wish we had some serious budget options too. 6500 XT is a massive gap in performance down for not a lot less money. And the 6400 is just pathetic. 6500 XT should be like $120ish and the 6400 should be sub $100 IMO. And there should be some decent $160 option somewhere around 1660 ti level in there somewhere."
Everything youâ€™re saying holds up. Itâ€™s plain and simple, nvidia cards ARE priced higher. But what is everyone gonna do, not buy them and pray for a price drop? Buy used? Buy older cards at MSRP? Buy AMD? None of those options are really that good. It ends up being lesser of evils option that works for you. But then you throw in the shit 12 gb of ram on the 4070ti and lower, and the options look real bad.
[deleted]
That's fair, but the comment I was replying to was specifically talking about the mid/upper tier (900ish range), and my comment was in reference to that.
"""it's also the privacy invasion through GeForce Experience, which you have to install if you wanna use Broadcast or Reflex""

No you don't, I use both and don't have Geforce Experience installed"
"Yeah, the only game I actually play that demands anything is Cyberpunk really. I bought the 6950XT mostly as a final farewell because I don't think imma update this or the next gen anyways. And the 16 gigs are nice. 

Otherwise I play CSGO, Dwarf Fortress, RimWorld, AC BlackFlag...like none of them really need that much horsepower. Save for Cyberpunk I don't think I've bought an AAA game in 4 years or so."
Get a rx6800 you will have 4k great performance and for less than half the price.
"I am very skeptical Intel will be a competitor in the GPU space at all, anytime soon. Perhaps later this decade, but I just donâ€™t see them being relevant in 2024. Intel keep making claims there, but fail to execute, Arc was years delayed, and when it finally came out, to the very limited extent it did, it had horrible drivers. I just donâ€™t see Intel being relevant in this space if/until they prove themselves. At such time as they bring out a performant card, at a good price, Iâ€™ll give them serious consideration. Until then, Intel, talk is cheap, but doing things is what matters.

People will keep buying NVidia as long as those cards are performant and plentiful at a given price tier, and I donâ€™t see that changing anytime soon. Yeah, they tried to shift the price tiers, thatâ€™s halfway failed, and theyâ€™ll compensate. late 2024 should be much better in terms of price-performance. Of course, no guarantees, but thatâ€™s how it looks to me in Jan 2023."
That bit about â€œthe orders rolling in from TSMCâ€ and the delayed deliveries by 3 months, that already happened in 2022. Had NVidia gotten what it wanted, itâ€™d be Spring 2023 before we saw 4000-series launched. Instead, well, we got what we got.
">	Supposed to be a 4080 competitor at 1/3 the price

Intel is currently using a die bigger than the 3070ti to at-best match a 3060ti. Thereâ€™s no way they would sell a 4080 competitor at $400"
"Uh saw a chart a while ago indicating they were getting massive profits until the pandemic with that strategy.

Either way, I'm of the opinion all of that new crap is overrated and I would've preferred they kept making the same kinds of GPUs from 2016 on. Who actually uses RT outside of the premium segment anyway? I sure don't. Not with the costs."
"No, they just stayed on 4 cores for a decade. Even now, we're only up to 8 gaming cores, and they try to upsell gamers on productivity crap that's useless for gaming. You're not actually getting much high quality silicon out of consumer-grade CPUs.

The i9-13900k is 257mm^(2) Intel 10nm, monolithic

The 7950X only has 265mm^(2) of silicon, and its made from 3 chiplets

* 2x 70 = 140 mm^(2) TSMC 5nm
* 125 mm^(2) TSMC 6nm (mature 7nm)

Compare that to GPUs:

The 7900 XTX has 520mm^(2) of silicon, and is made from 7 chiplets

* 1x 300 mm^(2) TSMC 5nm
* 6x 36.6 mm^(2) = 120 mm^(2) TSMC 6nm

The 4090 is a monolithic 608mm^(2) TSMC 4N

Monolithic is also more expensive than chiplets. The bigger the chip, the lower the yields.

For the Graphics Cards, you also have to factor in the cost of the board, power delivery, 24GB of VRAM, cooler, thermal materials etc. All that stuff is offloaded to other components sold separately from CPU's (Mobo, RAM, Cooler)."
5nm is a marketing name. They are not actually making transistors 5nm in any physical dimension.
this is why ARM is the future.
correct. but tolancy on the gpus are very tight for what there being used for. Stability is key!
best buy has had the same deal before
"Thank you. I found someone in here with some sense. This place gets so hung on die sizes and counts, frames per $, and a whole host of other bs. either to shit on current gpus or excuse it. It's all about raw fps performance gains.

I've been waiting 4-5 years for a reason to upgrade my 970. Everything in the $300 price range has been nothing but a side grade. It's taken 2 years for a 6700xt to come down to around $350 that would make an upgrade worth it. And honestly its just MSI that has dropped prices to that level. All other AIBs are still jacked up in price for 6700xts"
"> That's how it used to work. That's how it ALWAYS used to work. 

It actually only ever worked that way for like 2-3 generations. The prices were usually all over the place."
">80 card = $500. That's how it used to work. That's how it ALWAYS used to work.

In reality, the first 80 tier card as we now know them, the GTX 280, was launched at $649, and that was back in 2008. Corrected for inflation, that would be almost $900 today. Extremely high competition from AMD's 4000 series forced Nvidia to drop the price down to $499 (and, even at $499, it was still $200 more than the highly competitive HD4870).

&#x200B;

>And every generation, we would see healthy gains in price/performance. Sometimes 20-30% during a refresh generation, but sometimes up to double.

And 1st wave Ampere also brought that, which was my original point. 3080/70 had double the performance-per-dollar than 2080/Ti cards.

&#x200B;

>But after 3 years, you'd think we'd get something like that for $250. We didn't.

Turing had the 16-series GPUs to cover this price range, as you've already mentioned. While Ampere has the RTX 3050 at this price range. If we factor inflation, the RTX 3050 is actually cheaper than the GTX 1060 was.

&#x200B;

>Then nvidia did the same thing with the 3060, charged $330 for it and people acted like they were doing us a favor.

I never saw anyone say this. The RTX 3060 came after the 3060 Ti, and, if anything, it always regarded as terrible value, given the 3060 Ti offered over 33% more performance for 20% more money. If anything, the 3060 kickstarted Ampere's second-wave of cards - those that offered progressively worse performance-per-dollar. 1st-wave Ampere (3070, 3080 and 3060 Ti - not 3090 for obvious reasons) was the last time the market see Nvidia launch truly compelling products.

&#x200B;

>Either way, yeah, the 1080 ti was $700. And anything above that is a scam.

As was the 3080.

&#x200B;

>Performance per dollar only improved 50% on the nvidia side. IN SIX YEARS. That's insane. Historically before than we saw a doubling in performance every 3.

That's because performance per dollar has been stagnant since the 3080 was launched, back in 2020.

&#x200B;

>For reference, the 2500k to the 7600k was also 50%, so we're at intel stagnation levels of price/performance.

CPUs can't scale like GPUs because CPU workloads are highly sequential, as opposed to graphics which are tremendously parallel workloads. It's easier to scale compute units than it is to make those compute units faster, this is why GPUs improve faster than CPUs.

&#x200B;

>We got the 2060 in 2018. Then the 3060 and 3060 Ti in 2020/2021. More measly 50% gains roughly.

The 3060 Ti was a **great** GPU. It was brutally faster than **anything** priced below it, and it delivered performance over the $699 2080 Super... for $399. If you want to compare it with Pascal, it offered nearly double the performance of the 1070 which shared the same MSRP. Plus, you had DLSS and RT. So, in just two generations, the 3060 Ti was offering twice the performance per dollar (even more if you adjust for inflation), plus DLSS and RT support.

The 3060 Ti still has no genuine substitute. And, if the 40 series leaks are anything to be taken seriously, it might seem that 40 series will never spawn a genuine replacement for the 3060 Ti."
You get an upvote.  Couldn't agree more, and can't add anything.  GPU prices suck, no doubt.
"I will hold my GPU for as long as it takes until we come back to sanity. Sure I have a 3070 Ti and it might be easy for me to say, but I would do the same regardless. I have so many games pending to be played:

* Uncharted 4
* Horizon
* Days Gone
* Spiderman
* Death Stranding
* Batman Arkham City, Arkham Knight
* Elden Ring
* Dark Souls II and III
* Kingdom Come
* Halo MCC
* Jedi: Fallen Order
* A wishlist of more than 100 already released games

Why should I upgrade? Just so I can play at 240 hz? Not worth it. My 3070 Ti can run these games on ultra at 100 fps and anything above 60 fps is fine for me, even if in the future I have to decrease to medium.

Maybe I should upgrade to play those amazing AAA titles that are coming out? Let's spend 2.000 â‚¬ to play Forspoken. Game is trash but at least I could brag about it.

Guys, let's stop for a minute and think about it. You most likely do not need so much GPU power."
"Or wait until the Arc Battlemage is released Q1 next year. With the neutered memory bus of everything below the 4080 I can't in good conscious spend $800+ on a 1440p GPU. 

Gonna make a prediction now, when the Battlemage is released we will see a huuuge price drop on 40 series cards. They're going to have to, otherwise Intel is gonna take over."
I mean, obviously buy AMD, but the 7000 series right now is worse value so no...
"And how is this any different from 20 series? Was there any competition to 20 series? Was there an AMD alternative for Turing? What did people do? GPU's aren't something essential for living, they aren't even essential for entertainment. If no one can offer you a decent GPU for a good price, what will most people do? They simply won't buy one.

And that's already considering Turing, for all its disgrace and failure, was far cheaper (even when we adjust for inflation) than 40 series."
[deleted]
"Well, just don't buy. What is the reason to buy new highend hardware now, unless you're an enthusiast with money to burn?

Just wait and use what you have.  If you were one of the people who built systems with no GPU and so on... well, you played yourself."
">This isn't true. The 3080 is cut down 19% from the full die, the 1080 and 2080 are cut down 36% and 33% respectively. The 2080 Ti is cut down 5%.

You got the numbers right for the 3080 and 2080 Ti. Not for the 1080 and 2080. The 2080 had 46/48 SMs and the 1080 had a fully enabled GP104 (20/20 SMs).

>So the 3080 is roughly halfway between a 2080 and 2080 Ti in terms of SM count compared to the full die.

That's misleading, to say the least. The 3080 performs within 90% of a 3090, which features a nearly full GA102 and operates at comparable power levels. I'll take the 3090 Ti out of the equation because the 3090 Ti relies most of its performance uplift on the massive 450W TDP (which, in real-world testing, is closer to 480W for the reference FE model) rather than any actual architectural upgrades (it's only got 2 more SMs than the 3090). If you feed 3080s and 3090s with the same 480W (which isn't a far-fetched idea, given all those cards are based off the exact same die; if a 3090 Ti chip can take 480W, so can any other GA102 chip), most of the performance advantage of the 3090 Ti will be gone.

Unlike the 3080, the 2080 and 1080 performed nowhere near the bigger models. Even with partially disabled chips, the 1080 Ti and 2080 Ti brutally outperformed their 104 siblings. The 1080 Ti was, in average, 35% faster than the 1080 at 4K. Meanwhile, the 2080 Ti still outperformed the 2080 by nearly 30%, and that's considering that, at the stock 250W TDP, it operated nearly 300Mhz below the 2080. As a reference, the 2080 had a 215W TDP. In other words, it had 67% the SMs but 86% the TDP. This is why so many AIB 2080 Tis would operate over 300W, which allowed them to operate at similar clock speeds to 2080 cards; even overclocked 2080s. The TU102 had no problems operating as fast as the TU104, as long as it had comparable power per SM (and adequate cooling to boot). Once you adjusted the cards to operate at similar clockspeeds, the 2080 Ti opened an even bigger performance advantage over the 2080.

The bottom line here is that, in the end of the day, what really matters is what chip your GPU has. Obviously, a full version of a chip will perform better than a partially disabled chip. However, the truth of the fact is that all cards that feature a same chip will perform relatively close to each other, just look at the 3080 and 3090 and how close they are to each other. Similarly, the 3060 Ti offers near-to-3070 levels of performance, and this comes ate no surprise as it features the same chip. Now, step down to the 3060 and there's a massive performance drop, they're nowhere near in performance. And that's easily explained by the 3060 using a lower tier chip. Of course, there are some exceptions, like the first GA106-powered RTX 3050s. In essence, because the GA107s weren't ready available at the time, the first 3050s to hit the market featured the GA106 instead of the GA107 which should have equipped it since the beginning. Reducing the GA106 to GA107 levels means the chip is massively capped at 2/3 its capacity. Obviously, this type of decision only serves as a temporary solution, given it makes no financial sense to commercialize chips at such reduced capacities. As a reference, the smallest GA102 - the 3080, still features a full fat 20 SMs more than a fully enabled GA104."
> I'm looking at something for emulating, VR, and NVENC, which AMD isn't as friendly with.
"Intel has made huge progress on the drivers in a very little amount of time. I agree it had a very bad release, they probably should have delayed it, but it is what it is. Today it's performing at the 3060ti level with a lot of the kinks already ironed out. It still has a few issues but honestly if I were willing to buy a card in the 3060ti/3070 range then I'd jump all over the A770 LE. Everywhere I look, the recent reviews all say its an amazing card vs the current market. I just happen to want at least 3080 level performance.

Q1 next year and we'll see if Intel is ready for the big time."
Really? I didn't know that. I thought we were in the current 3 month delay right now. That's interesting and irritating at the same time. Supply is greatly outpacing demand for all but the 4090 as far as we can tell, and they're still not budging on prices. Greedy assholes. Can't wait for the Arc Battlemage.
You're confusing the Arc Alchemist, which is already out, with the Arc Battlemage, which is still in development...
"Their financials are public.... Gross margin is what we are talking about. The mark up over cost to manufacture. 

Nvidia's margins have not risen since pascal for gaming. They are a bit higher due to datacenter growing massively of course."
"> They are not actually making transistors 5nm in any physical dimension.

I mean this just backs up my point. Making things on this scale is so difficult, they literally aren't able to make things as small as they want. I don't see why this is so controversial."
"OK. Best Buy still isn't Apple. 

Take a walk."
"Frame per $ matters, but that's kind of the point. We've seen a mass stagnation since 2016 when each generation should be a regular gain. Even then, Id prefer at least to keep the price points for various performance classes the same. 

And yeah im in the same boat as you. I sprung for the 6650 XT though. MSI, like you said."
"Uh, more like 6?

400 series through 1000 series. All similar price points with little variation. 

200 series was a bit weird, akin to the 2000 series where nvidia just tried charging insane prices for new hardware, but that time AMD massively undercut them and it corrected the market. 

And before that, we had the 8000 and 9000 series which were relatively affordable. 

And before that is ancient history and i didnt follow the market. Still, I think there was something sacred about the 6 series even then. 6600 GT, 7600 GT seemed to be the best bang for your buck cards. FX 5700 before that. 

You get the point. 

The only other generation where nvidia tried the crap it's doing now was the 200 series, where it had a relative monopoly over the success of the 8000 and 9000 series and then it offered a massive performance boost for the money. Except that time, AMD countered with cheap 4000 and 5000 series cards and kinda forced the situation back to reality. 

AMD arguably could and should do it again, and kind of are with their 6000 pricing right now. 

But still, they originally released those cards with much higher MSRPs and they're trying to do the tit for tat strategy with the 7000 series again. 

AMD could play hero, give us all cheap GPUs but they seem to be trying to abandon the ""budget brand"" label in recent years by charging more for their products. They could act as the same benevolent force to correct the market but they're not really doing it as much as they could and arguably should."
"> In reality, the first 80 tier card as we now know them, the GTX 280, was launched at $649, and that was back in 2008. Corrected for inflation, that would be almost $900 today. Extremely high competition from AMD's 4000 series forced Nvidia to drop the price down to $499 (and, even at $499, it was still $200 more than the highly competitive HD4870).

Yes I know that and they were ####ing overpriced too. 

Hence why everyone lauds AMD for releasing the 4000 and 5000 series cards at sane prices and driving the prices down. 

This is just ""well ackshully"" counter factuals ignoring the fact that the launch prices of the 200 series were insane and nvidia got punished for doing it effectively. 

>And 1st wave Ampere also brought that, which was my original point. 3080/70 had double the performance-per-dollar than 2080/Ti cards.

Cool for rich people, does nothing for the sub $400 segment.

>Turing had the 16-series GPUs to cover this price range, as you've already mentioned. While Ampere has the RTX 3050 at this price range. If we factor inflation, the RTX 3050 is actually cheaper than the GTX 1060 was.

And the 16 series was weak and provided only a 35% improvement in price/performance. The 3050 was garbage. And ERMAHGERD INFLATION, THAT JUSTIFIES EVERYTHING.

>I never saw anyone say this. The RTX 3060 came after the 3060 Ti, and, if anything, it always regarded as terrible value, given the 3060 Ti offered over 33% more performance for 20% more money. If anything, the 3060 kickstarted Ampere's second-wave of cards - those that offered progressively worse performance-per-dollar. 1st-wave Ampere (3070, 3080 and 3060 Ti - not 3090 for obvious reasons) was the last time the market see Nvidia launch truly compelling products.

Cool, except it was exactly $400, which was way too expensive for a 60 series card, and keep in mind, does nothing for any of us in the sub $400 market.

I dont care how great these cards were for rich people. Im a normal person who buys $200-300 GPUs. Your arguments do not connect with me at all. Because I see it as ""1060 -> 1660 ti/super -> 3050"". That's a HORRID improvement in 6 years. 

>As was the 3080.

yeah and 80 cards normally go for $500-600.

Besides youre missing the point that now there are literally three tiers of card above that and we're a whole tier lower than 2017.

>That's because performance per dollar has been stagnant since the 3080 was launched, back in 2020.

it's been stagnant for far longer if you're NOT in the $800 segment. JESUS.

>CPUs can't scale like GPUs because CPU workloads are highly sequential, as opposed to graphics which are tremendously parallel workloads. It's easier to scale compute units than it is to make those compute units faster, this is why GPUs improve faster than CPUs.

Jesus christ you're dense.

Way to miss the point.

>The 3060 Ti was a great GPU. It was brutally faster than anything priced below it, and it delivered performance over the $699 2080 Super... for $399. If you want to compare it with Pascal, it offered nearly double the performance of the 1070 which shared the same MSRP. Plus, you had DLSS and RT. So, in just two generations, the 3060 Ti was offering twice the performance per dollar (even more if you adjust for inflation), plus DLSS and RT support.

Cool, except it LITERALLY DOES NOTHING FOR THOSE OF US WITH 1060S.

Jesus christ you're dense. It's like you're literally the ""well ackshully"" guy. Like holy ####, touch grass. Seriously. 

I ain't doing this any more. You are clueless.

If you wanna look at what i consider good pricing, look at the AMD side of things. I literally bought a 6650 XT, which is almost on par with your lauded 3060 Ti for **$230**. That's a deal. Nvidia is price gauging. The only decently priced cards for what they offer right now are the AMD 6000 series.

https://imgur.com/MnZF6q1

Seriously, I got my deal at $1.78 according to that chart. THAT'S a deal. Even your 3060 Ti is still a worse value than almost the entirety of AMD's offerings right now. Only flagship cards are a worse value."
"Iâ€™m on a gtx 1080 with 4K as my primary monitor. Battlefield 2042 looks like trash and so does warhammer dark tide. Now both those games suck, theyâ€™re not my style. But we have destiny light fall, witchfire, and atomic heart set to release soon. Those games will likely eat my pc alive. Thereâ€™s a few other less demanding games like Diablo 4 that will probably be fine on my pc but, my son also is due for a GPU upgrade so he can play his VR set. 

Bottom line is that I held out long enough, if I had a 3070ti Iâ€™d probably stick with it for a little while play 4K on medium/high settings. But when current games are chugging on my system then I likely canâ€™t play future games."
How much did you pay for your 3070 ti?
Wondering if buying a ranking Intel stock at 30 bucks a pieces is actually a good idea? That card looks like a good answer to the budget GPU segment. I wouldnâ€™t buy one, but maybe others will and that will put the pressure on nvidia and amd to drop prices. Iâ€™m in the market now, and I canâ€™t wait another year.
"Yes - the current 7k series is not good value compared to the 4070 ti for instance. 

They get hotter and use more power and the whole thing is such a price  inflated mess. Even if all these cards are absolute overkill for everyone but people running ray tracing at native 4k. Which is after all a tiny segment of the market. The vast majority of the market is not served by the cards available at this time from the new gen."
Technically there was - but people just can't seem to do anything but think about how they just want to pay that sweet nvidia tax.
"Maybe itâ€™s me but this whole die size metric seems like people are pulling it out of its ass. I donâ€™t remember people ever arguing over die size, similarly this price/performance metric is fairly new and being used to justify these overpriced cards. 

But thatâ€™s the point the cards are over priced."
">the 3080 is not a 2080 Ti equivalent by any measure except for perhaps total die area

That's wrong, the total die area is not equivalent, not even close. The TU102 outsized the GA102 by over 100mmÂ². They are, however, equivalent in other metrics.

3080: 68 SMs  
2080 Ti: 68 SMs

3080: 6 GPCs  
2080 Ti: 6 GPCs

3080: 34 TPCs  
2080 Ti: 34 TPCs

3080: 68 RT units  
2080 Ti: 68 RT units

3080: 272 TMUs  
2080 Ti: 272 TMUs

3080: the only die to feature disabled memory channels  
2080 Ti: the only die to feature disabled memory channels

3080: the cheapest version of its die  
2080 Ti: the cheapest version of its die

If you want to compare how well the Ampere architecture scaled over Turing (that is, how much better a Ampere SM is compared to a Turing SM), no other GA102 GPU was a better reference than the 3080, given it's nearly symmetrical build to the 2080 Ti. A similar relation went on between the 3070 & 2080 and 3070 Ti & 2080 Super."
Youâ€™re right, I donâ€™t need to spend 1k on a GPU. Itâ€™s almost irresponsible of me to do so just to play a couple games at 4K. But itâ€™s also something Iâ€™ll keep 5 years or so. So less than 20 bucks a month for a hobby? Thatâ€™s not terrible. I donâ€™t need top of the line 4K 120, but 4K 60 isnâ€™t attainable on ultra without at least a 4070ti/7900xt right now. And if Iâ€™m going to spend money, why wouldnâ€™t I at least buy the minimum now?
"They will budge on prices, they have to.. but it can take time for this all to play out.

Sure, they're greedy, but that's what happens when corporations are beholden to shareholders, they're sometimes forced to pursue short-term choices at the expense of the long-term. Greed is fine and expected to a point, it's human nature.. but when it's shareholders calling the shots you get stupidity like what we have with NVidia. But, I digress. They're going to charge what they'll charge, and all we can do as consumers (besides complaining about it on Reddit) is to buy or NOT buy. 

I'm highly doubtful about Battlemage. My next card will probably be a 5090."
And whatâ€™s the difference between the two?
"https://www.macrotrends.net/stocks/charts/NVDA/nvidia/profit-margins

Uh....

Yeah outside of recently where the market is crashing because the prices are way out of sync, nvidia has had much higher margins from 2018 on."
Are you ok? You do realize no one is being hostile or anything right?
"Well, the thing is that 4K is just too much for that 1080.

Personally I wouldn't go above 1440p because above that you need some serious power for your games and currently power is very expensive. I am really happy with my 1440p because it does not put me under pressure for upgrading.

Everything works great and it is just enough for me."
"600 â‚¬, managed to get one at MSRPin a drop. Then I sold the 2080 I had for the same 600 â‚¬ to a miner.

It is only a 30% performance improvement but it was free otherwise I would still be rocking my 2080."
The Arc Alchemist is supposed to have silicon on par with a 3070, so that's the target once they get the drivers optimized. I agree, its a little underpowered for my taste. But honestly, for what it costs, you get a LOT more than the 3060ti. I can see the A770 LE becoming the next big thing once people realize the drivers are significantly better now and you get 16gb of vram and twice the bus size as the soon to be released 4060ti for about 1/3 less
I think unless youâ€™re at 4k and or on an older Gen card you can probably skip this series. But, the 4070ti is just barely a 4k60 card and the 7900xt also just barely gets there but does pass the 4070ti. Now for ray tracing and frame generation, nvidia takes the cake but as you said itâ€™s a tiny segment, even a 4080 canâ€™t play cyberpunk at 4K ultra with ray tracing.
Looks to me like most titles will run 60 FPS on a 3070 at 4k. There's going to be outliers, sure there always is. Can't account for all of them.
I'm not developing the GPU, am I? All I know is what I've read. They're aiming at a 4080 competitor with less cost and more vram. It's still in development, but it's supposed to be released Q1 next year.
"GROSS MARGINS.

No Nvidia's gross margin is not higher

We are talking about cost to manufacture vs sales.

That gross margin has been stable for Pascal since launch, but datacenter growing so much has pulled up corporate by ~5%"
1440p looks great, my son just got a 1440p 165 hz monitor and it looks really nice. But Iâ€™ve been on 1440p since 2012 and moved to 1440p ultrawide not long after and now on to 4K. I know my 1080 canâ€™t handle 4K well, even at launch it was only capable of maybe 25-30fps. But even to get a 4K experience it doesnâ€™t make sense to have to pay 800 bucks when the tech was almost possible 6 years ago.
If it ends up being like a 300-400 dollar card, with some games included, then Intel might snag a small percent of the market. Iâ€™m almost wondering if it will be Intel and AMD competing in the budget space, while nvidia focuses on the enthusiast level.
Looking at reviews, most of the cards from around a 3070 and up will do 4k 60 FPS in most titles that do not have raytracing. Or if you just turn it off - since that is what most people do according to "the interwebs".
Yes, but 3070 is a couple years old technology now, and is also vram limited. Iâ€™d buy the 4070ti and be done with it, if it was 16gb of vram. Iâ€™d buy a 6800xt for 500 but similarly, itâ€™s a couple years old. Best strategy for me is to buy the best I can afford and hold on for 5-6 years like I did with my 1080.
If anyone was going to undercut the 4080, it was going to be AMD with their chiplets, and we all know how that ended up. You still havenâ€™t justified your claim that Intel was going to sell a 4080 competitor at $400
Honestly, this seems really freaking cherrypicky. Anyway Im not really interested in engaging further.
"Yeah man, these prices are crazy. I really hope that soon prices come back to sanity although even in that case, I will not upgrade until my 3070 Ti can handle games at 1440p 60 FPS medium settings.

Maybe you can get a 3000 series, with DLSS running 4K should be fairly easy."
"The A770 limited edition is already available for $349. Considering the best selling cards in the GPU market are ~300, Intel is positioning themselves to chop Nvidia off at the knees and I'm loving it. Screw the duopoly and their ridiculous greed.


When Directstorage becomes mainstream the Intel Arc GPU's will be unbeatable IMO. It already does decoding as good as a 4090 and it's $350.... With 16gb Vram. Nvidia's only 2 cards from this gen that has at least 16gb vram are $1200-1600., the 4080 and 4090. And this is just the beginning. The Arc Battlemage is going to be a LOT more powerful than the A770 LE. Nvidia and AMD is in trouble, whether they are willing to admit it or not."
My 5700 XT has no issues with vram or performance. It's not powerful as a 3070.
Wtf? I didn't say they were going to sell it for $400. Read it again, my dude. When I said 1/3 the price, I was talking about today's 4080 prices, which starts at $1200. Ipso facto we are probably in the ballpark of about $800 for a 4080 equivalent.
No it's not. Gross margin is cost to produce vs sales price. That is absolutely not up.
Iâ€™ve heard a lot of people go 3xxx. Almost doesnâ€™t make senses either cause now youâ€™re either getting a used card or at MSRP itâ€™s almost the price of 4070ti at 800. And then youâ€™re not getting DLSS3. So itâ€™s like my choice is be cheap and get the 4070ti at minimum and be worried about 12GB of ram, settle for the 7900xt, or splurge for the XTX which maybe could trip my 750watt PSU.
Only at the low end though, and I think thereâ€™s enough of a low end market to be profitable. And those enthusiast cards will likely continue to raise in price and be sold at lower volume. Unless these companies make serious leaps in technology, they are likely pricing themselves out of their own markets.
And youâ€™re on 4K?
1/3 * 1200 = 400
Sometimes I plug in to my TV yes. But 4k is silly for a PC monitor.
Bro.... Go back and look at my original comment. I very clearly said 1/3 ***LESS***. As in 2/3, aka 66% of the current cost of a 4080....
Well as I said, Iâ€™m not on a 5700xt or 3070, both those cards beat my 1080. So, I can buy an only used card, or a brand new card and expect it to last, but if Iâ€™m going to gamble on one over the other, Iâ€™d take the card with more vram, and potential driver performance boosts for roughly the same price, unless I can definitively say that 12GB will be enough for 4K gaming for the next 4/5/6 years.
You said â€œ1/3 the costâ€, meaning one-third the cost of a 4080
Good lord, are you dense? Go back and look at my original comment that started this stupid ass sidethread. It very clearly says LESS.
">Supposed to be a 4080 competitor at 1/3 the price

I donâ€™t see a â€œlessâ€"
oh boy.. damn I was glad to see this u/ popup.  still an insecure moron who can't take an L
"comically high end rich boy / actual data science card - 1k

strong 4k card (80/800 series)- 500-600

strong 1440p card -(70/700) 300-400

strong 1080p card - (60/600) 200-300

decent budget 1080p card(50/500) - 100

technically provides video (30/300) - 30-40

And that's probably the price list for what they should be in times when pc hardware sales haven't completely collapsed."
I am poor so everything is expensive.
[deleted]
"A high-end GPU today, I would in theory go as high as $699, assuming that would be the lower tier take on the high end. Games today are ridiculously complex and running them at 4k requires a lot of engineering, so I can understand paying a little more but not $1600.

I say in theory because I game at 1080p, I like a smaller screen, less eye fatigue and I don't need more resolution. So I'd pay up to $400 for a mid-range 1080p/1440p GPU."
Just look at benchmarks for the best card that fits your budget.
"900xt/90ti - $1500
900/90 - $1000
800xt/80ti - $600
800/80 - $550
700xt/70ti - $450
700/70 - $400
600xt/60ti - $300
600/60 - $350
500xt/50ti -$200
500/50 - $150

This assumes amd / nvidia have equivalent products / names. Best of the best can cost however much, as long as it brings down the price of lower end cards. A new console level system should cost 500 total, that's what 50 is for. 60 is the mass appeal level (better than consoles, but cheaper end), 70 higher end (proportionate upgrade from 60), 80 premium (best reasonable option), 90 true top of the line (crazy power draw, crazy price, best performance, bad value)"
"200-300$ for a lowend

400-500$ for a midrange

600-800$ for a highend"
"Call me old fashioned, but I feel a graphics card shouldn't be more expensive than a console that can sorta match it in terms of visuals / performance. So I'm going to go with:

* â‚¬50-100 low end (720/30/low detail)
* â‚¬100-200 entry level (1080/30 medium detail)
* â‚¬200-300 mid-range (1080/60 high detail)
* â‚¬300-400 high-end (2k/60 high detail -edit- 1440)
* â‚¬400+ very high end (4k/60 ultra detail)"
Depends on what resolution and refresh rate youâ€™re targeting, as well as the CPU youâ€™re pairing it with because bottlenecks are a thing
"$50 / $600 / $1400

Low / Mid / High"
Buy a game console, problem solved
 I got an RX 6700 XT for like $369 and I think that's a cool price for one
That sounds reasonable.
"Yeah this is a reasonable answer,

50 series should be $200

60 $300

70 $400

70ti $500

80 $650

80ti $800

90 $900

90ti $1000

I think would be like an ideal breakdown on the Nvidia side."
"Just for your information 2K resolution is 1080.

> 2K resolution is a generic term for display devices or content having a horizontal resolution of approximately 2,000 pixels. In the movie projection industry, Digital Cinema Initiatives is the dominant standard for 2K output and defines a 2K format with a resolution of 2048â€‰Ã—â€‰1080. For television and consumer media, 1920â€‰Ã—â€‰1080 is the most common 2K resolution, but this is normally referred to as 1080p.


https://en.wikipedia.org/wiki/2K_resolution

16:9 1440 is WQHD"
"The PS5 costs 550 â‚¬, so perhaps that would be a more appropriate price for a high-end card.
The prices you listed are pretty low. I even paid 450 â‚¬ for a 1070 ti in 2018."
How many games are running 4k 60fps on ultra on consoles?
There has to be good synergy with the GPU and CPU, right? So if I buy a high end GPU I need to make sure that I have a good CPU, how can I see which ones fit together and won't bottleneck?
You asked what I would find reasonable, not what is reality. I got my GTX1080 second hand for â‚¬300 in 2020, and it does exactly what I'd want a â‚¬300 card to do - play games at 60fps at 1080p or (preferably) 2k.
"Yeah there are masses of benchmarks to pour over to get at the best answers to this problem, and rules of thumb derived from such to avoid doing that to excess.

As it is the most powerful GPUs on the market are bottlenecked by every CPU there is for some of the lower resolutions, but show something like their full potential at high resolutions and framerates. This can make selecting a resolution/framerate target a good first step for looking at hardware, and given that graphics cards are the single most important and expensive part in reasonable builds they should be the next step.

It's a multi variable problem."
If you are doing a lot of CPU intensive tasks all at once, then it will almost always bottleneck the GPU, regardless of what CPU you have because it won't allow the GPU to perform at its highest potential. A modern CPU with high cores, threads, and base/boost clock, will be fine in most scenarios. The higher end you buy the more tasks you can throw at it before the bottleneck occurs.
"Apparently Nvidia and AMD have switched to a ""margin"" focus instead of a ""volume"" focus. Why manufacture and sell 10 million units at $10 margin when you could sell 1 million units at $100 margin with less effort. This won't revert until the customer base drops low enough that they can't sell 1 million anymore and profit drops.

(the 10 million and 1 million units are just made up numbers for example)

The same thing has happened in the automotive market, and the housing market. Why make volume of $15k small cars when you can make the same profit selling less $30k SUVs. Why build a ton of starter houses for $250k when you can make the same profit selling a small amount of $1 million homes."
Because 2 companies aren't enough and people keep buying them.
Cause you keep buying them
Basically greed and we enabled them.  People showed these manufacturers that they would spend thousands for a graphics card. . Do you really think they are going to stop liking money?
"NVIDIA is increasingly making most of its revenue and profit selling server GPUs for AI acceleration.  Why waste precious die space that can be used for a $5,000-10,000 AI accelerator? 

AMD is similarly making more and more revenue selling high-margin enterprise CPUs.  Why use their TSMC allocation for low-margin GPUs?  As a result, theyâ€™re not interested in competing for market share. Even consumer CPUs are much more profitable with their tiny chiplets. 

So, NVIDIA has no real competition, and neither company is interested in selling low-margin parts. Intel wants to build midrange GPUs but the performance is highly inconsistent, so theyâ€™re not viable at the moment."
"A quick perusal of places like r/pcmasterrace demonstrates that there are plenty of people who are perfectly ok with paying the ludicrous prices that NVIDIA are charging for their newest cards.

If people are willing to buy them at that price, then why bother changing anything? Until their pricing practices actually begin to hurt them financially, nothing will change.

Same reason Apple was able to have proprietary charging ports and charge out the ass for replacements. Or Nintendo. People buy them, they make money off it, so why bother being fair with pricing?"
"Intel ARC is currently going through its RDNA moment, except I'm more confident they will deliver better drivers in the future years compare to what AMD did. 

So by the time my 3080 10GB retire in like 2027, I will be buying Intel ARC instead of Nvidia."
Greed
Duopoly
Jensen is pretty transparent that AI is their main focus now. Only Intel Arc can revive the gaming GPU market now. Not everyone can buy a 4090. On the other hand consoles are better than ever so that is always an option.
I just fucking hate cryptocurrency. Never gave me anything beneficial and doomed GPU markets.
because AMD and NVIDIA are hoarding their cards. Just look at their earning reports, inventory is going through the roof
People are still buying them
Theyâ€™re trying to condition us to believe these prices acceptable.
Greedy...  Fucks.
"""Greed"" is part of it but it's not really the only explanation. No other PC hardware has the same generational leaps in performance as GPUs. Whatever you want to say about Nvidia, they aren't sitting on their laurels and giving us 5% performance increments for 8 years like Intel did. Every 2 years they put out something 30-50% faster than the previous generation, something they've done with remarkable consistency for *12+ years*. And they've done so while adding features like G-Sync, DLSS, DLDSR, RTX Voice, Frame Generation, etc, which AMD is always catching up trying to implement a more open, but typically inferior versions of.

The incremental cost of designing and producing a 4090 die vs. a 3090 die is much, much higher than a 12700K vs. 13700K or Ryzen 5800x vs. 7700X. Is it greedy to inflate price tiers? Undoubtedly, but there's also no doubt that GPUs are far and away the most innovative components in PCs."
Profits and leather jackets.
"Because as I see in everything these days is...why sell for less when idiots buy for more anyway?   
If you double your prices and sell half as much, is still good for them. 

I'm surprised that the other PC parts aren't as expensive. Probably because you can still make a good PC with a budget motherboard and rams and so on, but you will still need GPU to run games.  
But imo, this will affect gaming sales at some point. If less people will afford to play newer games because of these prices, the less sales they will have. I for one won't buy games if I can't run them on High settings. I'd rather wait a few years till I upgrade my PC and play them then."
ITT /r/pcgaming finds out about capitalism and that they don't actually have the spending power they thought they did
N V I D I A
Because they are greedy cunts.
"counter article:  
""No, ChatGPT isnâ€™t going to cause another GPU shortage""  
https://www.digitaltrends.com/computing/chatgpt-wont-cause-gpu-shortage/  

tldr: 
Datacenter Cards for AI needs a ton more VRAM, an A100 has 40 and 80GB Ram options and for ChatGPT to ""run"" (not train, just run) you need 8 of them with NVLink = 800GB VRAM.  
For training 1000 A100 cards were used.  
It is also ECC ram (error correction!).  
The GPU die is in most cases not the same as on gaming GPUs although it is noted that RTX4080 and 4090 DIEs are on certain Professional Cards.  

quotes:  

> Iâ€™ve seen a few reporters build that exact connection, but itâ€™s misguided. The days of crypto-driven-type GPU shortages are behind us. Although weâ€™ll likely see a surge in demand for graphics cards as AI continues to boom, that demand isnâ€™t directed toward the best graphics cards installed in gaming rigs.  

...  

> Nvidiaâ€™s gaming GPUs generally arenâ€™t suitable for AI due to how little video memory they have compared to enterprise-grade hardware, but thereâ€™s a separate issue here as well. Nvidiaâ€™s workstation GPUs donâ€™t usually share a GPU die with its gaming cards.  

> For instance, the A100 that Heaton referenced uses the GA100 GPU, which is a die from Nvidiaâ€™s Ampere range that was never used on gaming-focused cards (including the high-end RTX 3090 Ti). Similarly, Nvidiaâ€™s latest H100 uses a completely different architecture than the RTX 40-series, meaning it uses a different die as well.  

> There are exceptions. Nvidiaâ€™s AD102 GPU, which is inside the RTX 4090 and RTX 4080, is also used in a small range of Ada Lovelace enterprise GPUs (the L40 and RTX 6000). In most cases, though, Nvidia canâ€™t just repurpose a gaming GPU die for a data center card. Theyâ€™re separate worlds.  

> There are some fundamental differences between the GPU shortage we saw due to crypto-mining and the rise in popularity of AI models. According to Heaton, the GPT-3 model required over 1,000 A100 Nvidia GPUs to train~~s~~ and about eight to run. These GPUs have access to the high-bandwidth NVLink interconnect as well, while Nvidiaâ€™s RTX 40-series GPUs donâ€™t. Itâ€™s comparing a maximum of 24GB of memory on Nvidiaâ€™s gaming cards to multiple hundreds on GPUs like the A100 with NVLink."
Motherboards at least for AM5 are still fucking expensive
Because Nvidia and AMD are in a price-fixing duopoly.
[removed]
"At these prices, GPU manufacturers are destroying long term demand. At a certain point, they aren't competing with each other anymore, they're competing with XBOX and Playstation.
  

  
I'm a lifelong PC gamer, but if I were looking to get into the hobby today, I would get a PS5 and a dozen games instead of a PC. The crazy thing is you're not even getting a worse gaming experience. Devs seem to put a ton of effort into getting everything to run smooth on console, and then push out a shit, unoptimized mess for the PC. Bugs, shader compilation, and stuttering somehow make my PC which is triple the horsepower of a PS5 the objectively worse way to play a game."
Because Nvidia owns both intel and AMD in this department. They have the more powerful GPU's, better ray tracing performance, better rasterization performance, and better upscaling technology in the form of DLSS 2 and 3. Nvidia are the king of GPU's and no one can top them. They may be greedy but they're sure as hell innovators of new technology that helps devs, gamers, and content creators.
greed i'd imagine.
Because most people buying gpus have proven to be the dumbest motherfuckers alive who will pay any amount, it's that simple.
*Monopoly!*
Because they believe people will eventually give into this new normal price as they keep saying.
Because Nvidia, AMD gpu mafia does not want to.
So many people showed that the price of scalped GPU's  are what people are willing to pay, so the industry adjusted. That's how it seems to me.
That's fine. As my 2070 ages and newer games target overpriced hardware, I'll just buy more games for my Playstation. Already halfway doing it because of the number of busted ports that have been released in the last few years. Just finished RE4 on my PS4, and it was a blast. It was perfectly fine in terms of graphics and performance. 60fps and 4k are nice, but definitely not necessary.
This generation of midrange cards is going to be the equivalent of previous budget stuff. AMD and Nvidia both seem to be prepping 128-bit 8GB cards as their x60/x700 offerings, and both will almost definitely cost north of $400. They've realized making good value products isn't worth it.
Personally I will not give Nvidia money this generation even though I was supposed to upgrade, but as long as people "shut up and take my money" Nvidia will most likely make GPU's even more expensive next gen.
inflation is usually caused by raising profit margins, reducing profit margin is difficult ask any CEO you know
Monopoly
$500+ for a good MB and that is disgusting as well
"People keep buying the shit. When we stop buying the cards pricing will come down. 
Iâ€™m running my 3080 into the ground before I upgrade, I sold my 4k monitor and went back to 1440p to extend the life of this gpu. How do you justify a $500 price hike for an 80 series card?"
Ngreedia.
"If anyone wants an actual explanation and not just ""greed lol"", the 2nd picture in this link helps explains why:

https://www.fabricatedknowledge.com/p/the-rising-tide-of-semiconductor

So how does this relate to GPUs?

Transistor count is increasing much faster than performance.

Lets take the RTX 4090 vs RTX 3090:

https://www.techpowerup.com/gpu-specs/geforce-rtx-4090.c3889

https://www.techpowerup.com/gpu-specs/geforce-rtx-3090.c3622

The RTX 4090 has **76 billion** transistors. The 3090? Only 28 billion.

So you're left with a situation where the RTX 4090 is much, much more expensive to make than an RTX 3090.

TLDR; Price per transistor is only increasing or remaining stagnant, while the number of transistors in GPUs is greatly increasing."
I've just about had it up to here with this stuff that I've decided that if I can't get a new pc/play new games I might as well just start working on my own games for entertainment and try to optimize it for yesteryears potatos.
It's cartel. US government should step in but won't because they themselves own shares of these companies like for example Pelosi (massive amounts of Nvidia shares). The only hope is in competition from China now.
nvida and amd are stuck trying to milk money from something they realistically cannot. the fallacy and lies of capitalism is infinite growth, but you cannot sustain that. the only thing that will keep NVIDA growing at this point is its AI investments among other things, but the price of GPUs will fall because people will not need to buy their newer cards. they just won't. same thing for AMD. overpriced paperweights.
Well my 3090 literally doubled my fps in every game, from a 1080ti. We'd never seen a performance leap that great.
"Sanctions on China, Nvidia and AMD can't sell their fancy data center AI chips to the Chinese market anymore which tanks their sales.

Don't get me wrong it's a good thing for good reasons, but the rest of us have to shoulder the cost for a bit.  Hopefully new plants and new markets will bring the cost down.

https://techcrunch.com/2022/09/02/china-us-nvidia-export-ban/amp/?guccounter=1&guce_referrer=YW5kcm9pZC1hcHA6Ly9jb20uZ29vZ2xlLmFuZHJvaWQuZ29vZ2xlcXVpY2tzZWFyY2hib3gv&guce_referrer_sig=AQAAAJk0_B1y1KgOJr8P22m2i6p3bK-b5M-BjUc90fxrQsdviBMeZBS9_kmDXCcjZqVqUzcb9v-MOWRnk3aHk-OJPTeJLJN8KYAk8r7obXzEMK0x2x7DvyjvoeBnRviZFsxO4erzFuk-lGScyRXfLNlc3qihATOzXEtRbUKtmCgMEV12"
Because weâ€™re still buying them
Supply and Demand. People are still buying them up. They got away with the ridiculous cryptocurrency prices and will continue to get away with it as long as it meets their bottom line.
Monopoly.
Greed is Good.
Because nvidia is greedy. Prices wonâ€™t change until people stop buying them.
Because they are greedy cunts , also do you know how much a leather jacket costs these days?
Unfortunately the consumer always loses against corporate greed. If ppl keep buying they just keep upping the prices
cuz money
"Certainly! The reason why GPUs are not as widely produced as other PC hardware, which is why they often have higher prices, is due to the complexity of their design and the high cost associated with manufacturing them. The production of GPUs requires advanced technology, specialized equipment, and skilled labor, which limits the number of companies capable of producing them.

As a result, the few companies that do manufacture GPUs hold a significant amount of power in the market, giving them the ability to set prices as high as they want. This is because consumers have no other choice but to purchase their products, as there are no alternative brands or products that can match the performance and quality of their GPUs.

Furthermore, the demand for GPUs has increased significantly in recent years due to the rise of gaming, cryptocurrency mining, and artificial intelligence applications. This has further strengthened the position of GPU manufacturers, allowing them to increase prices even more.

TL;DR -  the lack of competition and high demand for GPUs have given manufacturers the ability to maintain a monopoly and charge whatever prices they want, leaving customers with little choice but to accept the increased prices."
Because oligopoly.
I like how nobody here mentions AI
"GPU pricing is back to normal

Moore's law is dead, so the performance/price ratio for CPUs and GPUs will no longer increase"
Intel and AMD are fine. Nivida is just trash.
Then go buy Intel if amd and nvidia cost too much
I just want to upgrade to the 4070 so bad! my 3070 is still awesome but you know how it is wanting the new best thing all the time haha
"If we go back 20 years, top end GPUs were running about $500, which is about $720 today. The 4090 being the most expensive high-end part today with an MSRP of $1600 about 220% more. There's no way a 4090 could be sold for $500 today. However, you get a LOT more than 220% the performance. So performance per dollar is much higher today than it was 20 years ago.

The same principle applies to lower tier cards as well. A lot more expensive in constant dollars but a much higher performance per dollar than 2 decades ago. 

Is plain greed part of the issue? Of course, but it's not just nVidia and AMD but extends all through the supply chain with the unprecedented demand for chips driving up fabrication costs to unprecedented for cards with crypto to scalpers buying up a huge chunk of retail product. 

Bottom line, GPUs are hot item and greed always enters into the market with hot products."
If u canâ€™t afford GPU then this hobby is not for you.
Because NVIDIA.
Nvidia hopes that most ppl have a long term memory that caps at around 2-3 years, and that crypto wasnt a thing.
Tell that to mobo manufacturers lol
Nvidia be like...  https://www.youtube.com/watch?v=DFKdU6AIseI
There is no good games coming out so people keep playing old games with their old GPUs.
Is it? Are mobos back to a few hundred bucks? Feels like my son built a decent computer in 2020 for $700.
Because NVIDIA got away with overcharging us for (checks notes) 3 generations now, why in the literal fuck would they stop when it's been proven that the genius brigade will literally shell out the cash regardless of price? Of course AMD would follow suit to compete/cash in
Because I bought a series S that can run anything flawlessly for $299
"AI field is exploding right now with public attention, funding, research interest and companies jumping on board.

The best chips (e.g. nvidias A100 or H100) use very similar architecture to GPUs, and you can still use powerful GPUs well for large language models.

But the prices for AI accelerators are much higher per silicone, so of course companies prioritize producing them.

The 2/3 of Nvidia revenue in 2022 came from Data Centers Division!:

https://nvidianews.nvidia.com/news/nvidia-announces-financial-results-for-fourth-quarter-and-fiscal-2023"
VRAM
"Put an 8800GTX on a scale and then follow it up with a 4090. Price roughly follows the increase in weight.

But also what everyone else is saying about lack of competition and targetting ultra high end market for margins instead of mass market with volume."
How are $600+ board prices normal, they have bloated in price so much yet barely provide anything worth more than their predecessors
They control this market sadly. I hate how I purchased the 3g 1060 in like 2018 I think? And now xx60s cost triple the price (INR 18k at the time which was alot compared to it's actual price in the US, versus INR 36k now. I admit ray tracing IS a thing but nothing justifies THAT humongous of a price increase lol)
"They overproduced during the crypto boom. They're currently trying to close out the previous generation of cards. This is why they priced the new generation of cards so high: it gave big spenders the ability to upgrade, gave them big margin, and avoided driving the prices on old cards to the floor. That's what happened at the tail end of the last crypto boom and they lost their asses.

AMD has at least let the prices on their old cards drop. Nvidia has kept theirs at or above launch MSRP. It's not the most consumer-friendly strategy, but they answer to their shareholders first. They're only doing it because it's working. People keep buying 30 series cards with no regard for what they should cost over two years after launch.

If you don't like it, all you have to do is wait. Prices will come down."
Nvidia H100 AI "GPUS" are $42,000" on eBay right now.  It just doesn't make sense to put your best minds on gaming, or clog up fabs with gaming chips.  Probably a while before we get back to normal.
GPU miners are coming back online.  If daily profit jumps, say bye-bye to GPU inventory.
AI
I found good value in the used market. I got the 3090 for less than what I paid for my 1080Ti new in 2016, so I consider that a win. Might go for a used 5080/90 next, so I'm set for 4-5 years.
I understand the rationale behind a tactic of prefering higher margin vs higher volume, as we are seeing in the automotive market, but imho  it won't work for the discrete gpu market. Vastly better performance/visual quality at a reasonable premium was the reason  pc started to entice more and more console only gamers. That value proposition is now busted, and news of recurring technical problems in pc games are also damaging  the performance advantage aspect of the deal. For me this situation  will slowly but steadily erode the big growth that pc had in last years as more and more people will simply return to console gaming.
Because they passed on their failed relationship with crypto to the average consumer both on the upswing and the downswing. Crazy the government isn't investigating them at this point.
we're they not keeping up with how this has worked for the last 3 years? what have they been doing?
So a cratering of demand would do it? Looks like itâ€™s time to play chicken.
"â€œ automotive marketâ€

Were lucky that used cars last forever these days and thereâ€™s much more competition in the car market than the GPU. 2.5ish gpu makers vs dozens of auto makers.

So itâ€™s slightly bad instead of terrible."
And here I thought if crypto miners didn't exist we would have cheap GPU.
"No comparison to the housing market, we've been building houses almost the same way for decades. GPU tech is based on the latest cutting-edge semiconductor manufacturing process, and constantly has new improved tech.

In the GPU market the reason is lack of competition (although prices in the low-mid end have been improving if you're willing to buy AMD or Intel). 

The barrier for entry to being a housing constructor is far smaller than producing GPUs. The supply of land in high-demand areas is what increases the price, in addition to the rise in cost of materials in the last few years."
Don't forget that NVIDIA's main source of revenue is data centers, and its revenue will continue to grow every year. Gamers are not important anymore so who cares about making cheap GPUs and profiting a few millions. It's just not worth the time.
"I feel itâ€™s possibly also due to not wanting to face reality that their GPUs are no longer purchased by people wanting to profit off of them and thus are not worth what they used to be.

Maybe they think playing the long game people will just be forced to pay their ludicrous price eventually."
"That's literally controlled, monopoly type, equilibrium.

In theory the price on what they are not doing increases since they aren't supplying it, which results in competition which prevents this.

In the real world owners, who are irrational and emotional human beings, choose what to price their goods and services at."
"> Why build a ton of starter houses for $250k when you can make the same profit selling a small amount of $1 million homes.

I will push back here. Much of the cost of building is dictated by building codes, permitting and the planning process. 

250K starter homes are generally illegal."
I would call this the brazilian economy effect. Brazil's economy is like that.
"Nobody changed focus

Simply, the volume is not here anymore : The desktop PC market is plummeting"
This will only get fixed with government intervention.
Greed
"Interesting if that's true, because some glaring issues are...

If you're tying features to cards, devs will push features that the majority of people have. If every single game went hard into RTX, and it performed well, people would feel more inclined to upgrade. 

Devs can't develop around dlss 3.0, because the majority of people don't have dlss 3.0 (thank God honestly). From Nvidia's perspective, you'd think you'd want that kind of market dominance, because then the game features will follow. 

Most people that upgrade, do it without *needing* to upgrade. The second you make them question it, they'll simply stop doing it until it's necessary. So someone that might have upgraded yearly, might not do it until the games they play actually need it, which might be 5+ years. So you're not only pushing people towards consoles or something like a steam deck, which probably won't be upgraded, but you're making people second guess their dependence on your product. Seems like the opposite you'd want in terms of marketing. You see big Mac and coke ads all the time. They know you know what it is, they just want to remind you that you need/want it. Nvidia is forcing you to ask ""do I actually need a new card?"" Every time you run a new game. If performance is even remotely acceptable, the answer will be no now. Before it would be ""60 fps is good... But 100 is better. Time to upgrade""."
"that demand has to be cratering right now i imagine.

Why else would there be rumours at the new 4070 being priced around $599 USD?"
such a simplistic view. not surprised it catches on here
The middle class (what's left of it) is not the target audience anymore :(
Bruh capitalism in a nutshell
"mfs spending 900$ on a 70 class card with only 12GB of VRAM in 2023. The stupidity is beyond me.  

But yea Nvidia will keep winning"
"Not even that. Nvidia revenue was down >50% y/y. AMD made <1bn in the last quarter from desktop (and maybe laptop too) GPUs.

I personally think last gen AMD cards are still valuable such as 6950XT for 650/700 USD and people still buy 30 series at MSRP. The new cards just kinda don't make any sense, they probably want to capture the high margins before releasing lower tier cards and when the market is saturated they may re-release the same cards with different VRAM for less money or try bundling with games or just price cuts. The same thing happened with 3080 12GB which was sold around the price of 3080 TI but once demand cratered they brought it down to about the price of 3080 10GB. They're playing the long game plus as the article mentions, they make money off data centres (Nvidia) or semi-custom (AMD) so they're in no hurry. They have inventory worth several bn, if it doesn't sell, they'll have no option but to clear stock at reduced prices but that may take a while. It's just a cat and mouse game and consumers will win it in the end."
Everything's worth what its purchaser will pay for it
Yep, I could pay more for a new gpu today than I spent on my current card 6 years ago and get worse performance...
Gaming devs are at fault too. The less they optimize their games, the more we will need stronger cards for new techs.....
"Data center just took over gaming as their biggest money maker. I'm sure ChatGPT/OpenAI and AI race in general will boost them even more. But untill recently gaming was their biggest market. Although how much it was affected by crypto is unknown.


Source:

https://www.nextplatform.com/2022/05/26/datacenter-becomes-nvidias-largest-business/"
dont forget about the consoles (latest xbox and ps5). huge revenue of AMD came from the consoles. teslas and valve's steamdeck are also part of it.
This is a great point. They only have so much fab space, so they want to schedule all that allocation for the top dollar items.
"I think people are finally understanding what income inequality looks like. 4090s are not overpriced for some people because $2K is not a meaningful figure. It's not even a consideration. If you got a 20K bonus twice a year you don't care about it. 

It's just noticeable here because its a pc gaming subreddit and you need a GPU far more than slightly faster RAM."
I just bought a used RX470 today. I used to keep up with mid-hi range gaming. Now, my pc is slowly creeping into the r/lowspecgamer category. One day, 1080p will be too expensive for me and I will be back to 720p.
So CPU, memory, SDD, etc. makers are not greedy? Only the producers of GPUs?
Nvidia means greediness in Romanian
The short and right answer
Stereopoly
"Consoles are always an option, but it is a bit overboard to think everyone needs a 4090 or bust.

So many games play with a much much lower gpu and hey aren't most in agreement that most games that do push gpu's now are unoptimized shit anyway? It's a mixed message, ""I hate AAA games"", ""I have to have the strongest gpu to play new AAA games"", imo screw these poor AAA efforts, to me they don't drive my gaming pleasure. We are seemingly all over the map here.

Intel saving the day? They are only cheap now because they can't compete currently, which is understandable. The moment they become equals, they will (wallstreet will demand) they get proper market value."
"> On the other hand consoles are better than ever so that is always an option.

Yup, this is where it's headed, I think. Damn shame, console gaming just isn't the same with the closed-down ecosystems and basically zero modding capabilities, as well as being forced to re-buy games every other generation or so. But it seems like the big GPU producers have moved or are preparing to move to the ""squeeze the market for all it's worth before dropping it"" stage. It's the most logical explanation for the massive pivot towards other revenue fields and the batshit prices that ensure more than 2/3rds of current PC players will be priced out of the market within the next 2-3 generations."
We also have the Steam Deck now, which actually is affordable PC gaming. It's great because devs will be targeting lower specs as well
">  consoles are better than ever so that is always an option.  

can't play my Steam games on Console, so no that is not an option.  
Steam Deck came in clutch though."
"
>On the other hand consoles are better than ever so that is always an option.

With paid online and higher prices on console games you're just moving the money around rather than saving it, I can buy games for cheaper on Steam than I could ever on current gen consoles (even if you buy second hand), you could say that consoles still have the advantage of having ""physical"" releases but nowadays that is not relevant since discs are nothing more than a validation method for DRM that still requires you to download something in order to play the game."
"> Not everyone can buy a 4090.

Why should they? How many people do you think are actually playing the latest AAA games at 4K/144hz AND stressing about the price tag?

Since when did all of your GPUs become obsolete so you needed a 4xxx series?"
"Its a great option so long as you stick to 60fps games (a massive library so far). A lot of games are getting patches still too (FC5 just got it).

I have a 7900xt PC but still use my PS5 often. Love me some console action."
Here comes a new Challenger! artifilic  intelligence using gpu's.
At this point the prices being so high have nothing to do with crypto or even scalpers.
Yup! Fuck Crypto and NFTs, those fucking things along with "Virtual Realestate" nearly killed VR, luckily that shit is dwindling.
"cryptocurrency is not a problem anymore

Moore's law end IS a problem, at least for people that do not accept reality"
Hey, man; crypto has ushered in a golden age for internet drug dealers.
Imagine if Jensen hwang is really Sotoshi Nakamura
And now Nvidia wants you to build chatbots using their GPUs...
Yup, your fellow gamers are just as much at fault as the guys pricing them.
Thank fuck Intel is trying to present an actual alternative and not just "similar but worse features, but they're open source!"
According to the scalping hell of yesteryear, they very much are. That's probably where they got the idea from
BMW is greedy because I can't afford a new M3
"The difference is the 4090 is actually nearly double the performance of the 3090 for the same inflation-adjusted MSRP.

The rest of the 40 series stack so far isn't nearly as compelling, especially when compared against the used market."
[deleted]
Yeah because amd didn't price their shit at 1k too.
I mean, should they be expected to sell them at lower prices for the betterment of the community? It's a high demand luxury product. It's a shame how it is now but we were spoiled in the past with mid tier gpus that were priced as low end
waah $2000 is a lot to pay to play video games on my 4K monitor, greedy nvidia is to blame! I refuse play below 60fps and ultra settings! thats for poor people!
"Well obviously it's different people. There are people who have the money and don't care about pricing as long as they get the best stuff and GPU makers are fine with selling less at higher margins because they still make the same profit so rest of us are kind of screwed.

Then again you are right that I have enough games to play for years and since I don't care much about AAA games anyway there is really no pressure to upgrade so I can wait."
">""better rasterization performance""

To be fair they have exactly 1 GPU that has better raster performance than what AMD has to offer.

But yes on the other stuff they are quite a bit ahead which sadly gives them the chance to just overprice their products"
I think AMD's more delusional than nvidia. They're not competitive enough to be only a little cheaper than nvidia's counterparts and it's clear they're not putting in the same level of R&D.
Meh. once Windows 10 stops being supported Im moving on to Arch Linux/SteamOS, where AMD is king. So as long as Nvidia only cares about its proprietary bullshit, I'll stick to more easily supported and usable open source stuff if the only thing Im losing is gimmick AI generated reflections.
Intel A770 matched the 3060s in RT. So next generation should be on par with 40 series or better.
How them boots taste?
Hmm? Kind of reminds me of intel in its glory days - guess what happened to them shortly after.
Why is there a Henry Cavill flair
Lmao what do you think "mono" means?
">I sold my 4k monitor and went back to 1440p to extend the life of this gpu

Bad move. With a 4K monitor, you can just play at 1080p without image degradation"
and my 2070 more than doubled my fps  from my 970 (the one with the gimped memory that could only use 3gb vram out of 4)
Go ahead, elaborate
Tell me you didn't read the article without telling me you didn't read the article
all  3 of them a multi billion mega corporations
20 years ago we were playing Warcraft III though. I agree that in terms of raw performance, we've gained an enormous amount. But games' requirements have gone up in much the same way.
Yeah, we should just all convert to console gaming
[deleted]
The grip of hyper consumerism is too strong.  The minute a card comes out our YouTube feeds are slammed full of reviewers showing off the shiny new cards for us to just buy them then play Vampire Survivors, Minecraft, and CSGO on them anyway...
Sad but, (seems like) true.
The problem is that the developers are starting to take advantage of the more powerful pcs to underedevelop their shit, Last of us port is a good example of that.
"Thatâ€™s what the corporatists at the fed want. The labor class is getting too upity for their taste so they want to make sure that only the bourgeois are able to participate in the economy.

Theyâ€™re happy only making 1% of the widgets for the 1%. Yacht companies are not going out of business."
Weâ€™re crashing this market, with no survivors
"Maybe. Or maybe AI has such high computing demand that they are happy to focus on that and high end GPUs.

Besides, if you buy a console instead the GPU manufacturers still make money off that."
time to downgrade to a 1080p 24â€ but still 144hz monitor
"> Looks like itâ€™s time to play chicken.

Get on board. I'm here rocking hardware from 2012"
Dude they sell so many cards just with the whole angle of "horsepower" that the price isn't coming down. You've got one brand that is considered consistently behind the mark that it gives people a stigma about buying, and another brand that comes across as really not giving a shit, but steadily issuing updates for their drivers because they care about keeping tight enough margins to get people invested so they have more than just CPUs to push. That's AMD and Intel, respectively.
Yeah, but 90% of people aren't playing along.
"There are around 6-7 big car groups, being corporate groups that own several brands.

Competition is artificially inflated this way, however most times groups dont own brands that compete between eachother (think Toyota owns Lexus, but they dont cater to the same audience)"
What about when we shift to those fully electric cars with batteries. How long do theu last when used? And how locked down are they to fux? You'll need to go to a authorized company with aurhorized battery etc to repair. Cars are going to ve the new smartphones lol
They still exist. Just less profitable so itâ€™s not as mainstream anymore
really poor zoning in the US and Canada don't help either. the main issue with housing worldwide is that its rife with speculators and as supply isn't meeting demand speculation won't stop. public housing must be built at a rate that makes it nonsensical treat housing like an easy investment.
"> No comparison to the housing market, we've been building houses almost the same way for decades

https://fred.stlouisfed.org/graph/?g=124ya"
"Underrated comment. I would add how big ML is becoming as well, otherwise I can't explain why CPUs have much better prices.

We are not their priority anymore and we are doomed to either buy consoles, or be fine with previous generation cards unless some new players comes into the market. Intel could be it, but I guess that as soon as they get some traction they will just act like nVidia and AMD do.

Another thing I can see happening, is that another smaller player can start producing GPUs using cheaper and older nodes. No need to be in the latest node for a GPU to be good.

Of course it will not compete with the latest nVidia and AMD offerings, but they should be cheaper. The problem is that as long as developers keep releasing unoptimised games hoping that we can power through their issues, many people will keep arguing that you need a 4090 or you are not entitled to play.

Anyway, I have enough backlog for centuries so I can keep using my 3070 Ti forever. I do not feel any FOMO with new releases so power to me."
And so they are keeping prices high to maintain revenue rather than dropping prices to increase sales.
I needed to get a whole new PC for gaming / rendering and had to bite the bullet this year. I ended up with a 4070ti/13700/z690 build for like $1800 and after how savage prices have been the past few years it almost felt like I hit the lottery. On one hand this thing is a beast and DLSS 3 is crazy, on the other I just spent $1800 on a PC with 12GB of vRAM in 2023 like a clown.
"I needed to upgrade from a 1070 after 6 years (I skipped on 30 series because it was unavailable for the longest time, and the prices only stabilized months before 40 series was announced) for 1440p gaming, and despite the meh improvements over 30 series, I still more than tripled my performance in most titles by upgrading to the 4070 Ti. It was only slightly more expensive than a 3080 that has 2 gigs less VRAM, no HW AV1 encode support, about 10% less rasterization performance (which was about equal to the price increase over 3080) and no DLSS3 support. When deciding between the two, I chose the option that made more sense.

Of course you will always have customers with more money than sense, upgrading every single generation for a couple of frames because they have to have the latest and greatest thing out there, just like you have people buying 1600 EUR iPhones every year. But for me, it was a purchase that massively increased performance in every game that used to struggle, easily maxxed out. It's just a shame Nvidia couldn't price it more reasonably, most likely the 30 series customers won't be convinced so easily and they could eventually drop in price due to the low demand."
"The 30 series at its launch was a great value and if you could get a xx70/80/90 for their MSRPâ€™s they were no brainers. 

The issue is most people didnâ€™t. I myself camped online every day for 2 and a half months to get my 3080 for MSRP, but I know for a fact most people: didnâ€™t have the time or patience to do so."
Game devs ain't nothing to do when it comes to  idiots w/ money to burn!   Do you remember the MSRP pricing for the 30 series?  Then in come the scalpers who bought up the first runs,    Even then people still had no problem shelling out 2-3 THOUSAND dollars for a card that was 700-900 MSRP.  After that,  Manufacturers didn't remedy the problem, they added to it!
I see gamers on reddit using the term "optimization" over and over again but they never seem to be able to quantify what it actually means.
A lot of devs get on here and gush about how much easier their life will be when raytracing is mandatory. It really feels to me that devs look at modern hardware as another way to pass costs onto consumers. It means they customer spends more for them to work less.
It's not the devs. It's sales and marketing pushing unrealistic release dates. There literally isn't support for QA and optimization at some companies.
"Oh they put 100% of the crypto miner cards into the gaming segment.  
Mostly because they got no clue after they sell the GPU dies to AIB partners where those partner cards actually end up.  

It is gaming cards, so they will count it as gaming.  

Also, if you look at the graph the gaming yellow ""line goes up"" (extremely) all during the pandemic, where GPU prices exploded and were scalped and gamers couldn't even buy any cards unless they got lucky or were using bots themselves. AIB partners were selling on palette directly to crypto centers, bypassing retail channels.  
No way if that line was actual gamers that it would not have crashed to rock bottom."
i mean it was the gaming cards that were being used for mining
"Consoles are a reliable source of revenue but they are low-margin products and take up quite a bit of die.  However, I assume they are using a less advanced fab process which is cheaper and has less demand (no need for TSMC N4).

In addition, Sony and Microsoft will re-negotiate pricing soon based on lowered manufacturing costs, which normally happens 3 years after the console release."
"A 4090 is also the most powerful consumer GPU on the planet. 

A 3060 or 6600XT is a great GPU and costs 20-25% of the 4090's MSRP. 

A Corvette C8 is an amazing sports car. A Bugatti Chiron costs literally 40x more than it. The existence of the Chiron does not make the C8 worse. 

People need to seriously stop acting like high-end luxury products are a necessity. You can game at 1080P/60fps for an extremely reasonable price. 

No, you don't ''need"" 4K resolution or 200+ fps. Nobody is being ""oppressed"" because they can't buy a 4090. 

I didn't buy a 4090 because it's far more money than I want to spend on a GPU. But I don't resent people who did buy it."
MOBO producers are getting there too. $500+ for a basic bitch diagnostic LED is BEYOND stupid. Bean counters and marketers need to stay THE HELL AWAY from the engineering division.
"When you control 88% of the GPU market share like Nvidia do, you set the price you want regardless of competition.

Every other company can't afford to be greedy, but if they could get away with it like Nvidia you bet your ass they would."
"There's less upgrade pressure for those components than there is for GPU's.

People might still squeeze by with a Ryzen 2600x in most cases, but while GPU's from that year are still hanging on, requirements for games are rising sharply."
"Greed combined with damn near monopoly control.  Every single corporation out there is trying to turn a profit.  For most companies that means beating their opponents on price or quality to maximize sales.  So it ends up being a race to the bottom on price with quality increases to try and keep the price higher.

That's not how PC GPUs work.  There's only a hand full of companies that make them and there's really only three that make GPUs for PC towers (under various brands).  All three of them decided to cut their production of GPUs specifically to shrink the market size and increase the individual sale price of each GPU.

If this was any other industry there'd be action taken against them.  But we're talking about specifically PC GPUs.  So this unfair greed will go ignored."
You're making it sound like a contest.
They are, but there is far more competition in all of those fields. AMD and Nvidia can pull this shit because until recently it was just the two of them. You either buy their GPU or you don't get one at all.
Yes
Everyone is, doesnâ€™t mean we have to like their pricing - so whatâ€™s your point?
So how much would you say it costs to build a PC equivalent spec to a PS5? How can PC gaming have a future if even entry level cards cost nearly as much as an entire console? It just seems to be an inevitable death spiral - Devs have no incentive to produce games to take advantage of the 4090 that only 1% of gaming pcs can use, and consumers will have no incentive to pay through the nose to upgrade.
"I think a lot of people will just stop upgrading and stick to games that respect their wallet. The success of the Nintendo Switch should be a sign how unnecessary the most cutting edge hardware is for people to have fun with a game.

Budget gamers that are trying to get into the scene should probably look towards the used market."
I don't really play MP so paid online won't be much of a problem. Discs can be resold to recoup money. I don't care to own most games. I suspect physical console games are going away next gen though. Game Pass is great for this.
You missed the point. The point being that all the 40 series cards are ridiculously priced but since the 4090 is a halo product and really the best of the best, it still sells well. But not everybody will buy that. The other 40 series cards are worse value for money ironically despite costing less. You could buy a 30 series sure. But buying a 2 year old architecture shouldn't be the norm. People who skipped 30 series so that they can build a PC later in the console lifecycle do that because normally newer and cheaper hardware comes out on PC and it outpaces consoles. Same applies for AMD, not singling out Nvidia. Intel is the only one making 300 dollar GPUs that are good anymore.
NVIDIA has been a leader in AI research for a while now, that isn't new.
At least that one is probably more useful than gaming. Fucking cryptoâ€¦
WHAT
"I've read rumor (huge grain of salt) that the NVIDIA wasn't entirely happy the 3090 and 3090 Ti stole some of their Tesla cards' thunder with ML and that the 40 series (4090 specifically) isn't a mindblowing improvement in the more popular pro/enterprise oriented models that are used: faster yes, but not by the margin that should be.

How true that rumor is? Dunno. I can't even remember the thread I saw it, much less the subreddit it popped up in (either r/hardware or r/Nvidia). Given how they approached FP64 back when that was all the rage, though, it's at least probable."
Oh really? I should have updated my info. Then I'm not sure why the price is so high.
Crypto and scalpers proved people were willing to pay current prices. Theyâ€™re exactly why prices for GPUs are as they are now. Companies have a financial responsibility to make as much profit as possible, theyâ€™re not lowering prices from what crypto and scalpers proved the market would pay.
It's all about those margins, baby.
how did someone with a sane mind think virtual real estate is a good thing to invest in
"Intel is the one who started this trend lol.

Intel cpus used to be the most expensive cpus in the market offering the least benefits."
Yep if they priced more competitively and tried to gain market share consumers would benefit quite a bit and AMD would probably benefit more long term but instead they'd rather go for quick profit picking up the scraps of the market left over from Nvidia.
One of these companies sell far more GPU's than the other.
AMD gave up on mid-tier this generation and has been talking about RDNA 4 for a few months now
60fps? Try 144. That is the only reason to game on a high end GPU. You can easily game on 60FPS with a lower end GPU.
"The low volume halo card got a $100 bump over its predecessor despite needing more of everything in its family: more silicon, bigger VRM, beefier cooling solution, more VRAM, and more PCB layers.

The step down has a $500 price bump over its direct predecessors despite having a significantly smaller die, with a weaker VRM, less VRAM, etc. than the halo card (by almost half). The step down below that is a $300 price bump over its predecessor despite having a die that *is* half the halo's size and also less of everything.

Two of these things are not like the other, and the other still makes a profit despite having substantially higher material cost to make.

*That's* the rub."
"The 4080 matches an XTX in raster though. For $200 more you're getting better RT, DLSS3, VSR, and CUDA support for workstation applications. For a lot of people it's a no brainer.

RDNA3 has been very mediocre so far and doesn't provide any real competition for Nvidia. At this rate I fully expect Intel to overtake AMD within a few years and maybe they can provide some actual competition."
It actually matches 3060ti in RT. In raster its like 3060 or a litle better now.
Commenting about the person instead of trying to comment or criticize their points, how amazing of you!
Difference is that Intel becme complacent and kept releasing 4 core cpu's every year. Nvidia on the other hand are innovating and adding new features to their products.
Because he's a hard-core gamer
"After using 4k playing at 1080p looks like complete shit. I already had the 1440p monitor, it was my second screen so I just went back to that as my primary. 
No reason to hang onto a $600 monitor to use it at 1080p."
To give you an example of how much VRAM AI uses, image generation using SD on low quality can take 64gb of VRAM.
GPUs are used as AI accelerators. Nvidia chipsets are the front and center of AI servers. You wonâ€™t see them lower margins on consumer cards because they make so much more money on enterprise sales. Every ML engineer gets assigned multiple 4090 for work. Every research lab at school has multiple racks running multiple 4090s. ChatGPT servers probably use hundreds of thousands of them.
Sure are but Intel's pricing is pretty damn fair since they dropped both the A750 and A770's MSRP. I think the amount of card you're getting for the price is very fair. Both 1440p capable cards with RT ability that is on par with 30 series.
">But games' requirements have gone up in much the same way.

Of course, requirements have gone up as visuals and overall game complexity have increased."
yeah, consumer activism pretty much never works unless it creates a PR nightmare and even then it's a maybe. if the government doesn't intervene you are at the mercy of huge corporations. breaking up Nvidia would be a good start but now that the US government is afraid of chinese competition in chip manufacturing there's no chance that's happening.
"You might not have to do anything.

With the current recession and rampant inflation, people buying expensive GPU will have a hard time to sell.

4090 and 7900 will sell no problem, the rest.... Not so sure.

Nvidia already sit on a 4billions USD inventory. A bit more and they will start feeling the pain."
Yup exactly, 2% of gamers use reddit, nothing we do changes anything
Thing is games even full price games can still fall into impulse buy territory. Four fucking figure GPUs are not in impulse buy territory for the bulk of humanity.
I don't think that's been a choice for most people, people with lower incomes can't afford to preorder if they wanted to lmao
">We've all stopped pre-ordering games...

Not from what I've seen. It's still going strong. Fewer, maybe, far from ""all"".

>blindly supporting early access titles...

Maybe your interpretation of 'blindly' is doing a lot more heavy lifting than I'm giving it credit for, but EA titles (especially Indie) is alive and still very strong.

>and loot boxes are a thing of the past.

I'd agree here, though. Loot boxes are on their way out but being replaced."
"> We've all stopped pre-ordering games, blindly supporting early access titles, and loot boxes 

I think you might want to take another look at the current state of the gaming world because those things are definitely still going on."
Haha yeah, about that.
And here I am, pre -ordering Diablo 4 like a dunce.
A+
Speak for yourself, I exclusively play demanding games like Old School Runescape.
"It's insidious how multiple communities, including this one, have no self respect for themselves and their money. 

Oh, you *only* have a 3080? Is something someone once replied to me when listing some random grievance."
People buying cards near or above 1000USD are an absolute minority. Steam hardware survey skews more towards potato than 4090. Just cause youtubers and people on the forums wax poetic about their hardware doesn't make it the norm or even high demand.
"I don't get shown that stuff, that's your algorithm cause they think they can sell you on it.   


I don't care about new cards, I get way more dopamine buying at the best performance to price ratio. They nabbin other people with the new new. If I really want to play something new right when it comes out i'll deal with low/med 1080, but usually i'll just wait for years when I upgrade my card to play it.  


Remember, they're driven by quarterly earnings and growth, wouldn't take long at all to pressure them into dropping prices if more people did that. Even if we aren't the ones benefitting directly from that price drop, we would still benefit from more people buying new cards and selling their old ones."
Steam Deck's runaway success is having the opposite effect, a lot of publishers are starting to prioritize Deck Verified status.
That's just plain bad development, not taking advantage of hardware. It doesn't work close to well enough on the majority of GPUs so there's no advantage. Especially since ultimately sales are taking a hit.
Also planned obsolescence by Nvidia. Developers work with Nvidia to develop ray tracking drivers, DLSS, etc. and Nvidia in turn can decide what GPU hardware tier would be required to optimally run the game. By help releasing drivers each year that require more VRAM per generation, Nvidia can force people to upgrade even if they have the previous generation mainstream card.
"See also: RTX.

â€œDevelopers will be enabled to make amazing things with Ray Tracing and save development timeâ€ actually means: â€œGames wonâ€™t look much better in the future, but weâ€™ll make people buy GPUs that are three times as powerful just to run them on their minimum settings, so we can save on labor costsâ€"
"I've seen this in the space more than anything. As far as I can tell, the more things have advanced, the more folk are just using one size fits all solutions. IE: Electron desktop apps that use a fuckton of memory, are in-efficent, rather than just developing native apps that would be so much better.

And there's no excuse. Something like a Discord still being on Electron boggles the mind. They have the engineering to push out actual apps, but it all gets focused into profit profit profit bullshit with microtransactions.

It sucks."
To be fair, anyone who believes in the rule of law and equal rights is bourgeois, but your point is well taken. These companies would love to make an equal profit while making fewer cards.
"Let me let you in on the real scoop behind all of this AI buzz: itâ€™s all marketing bullshit.

Predictive algorithms for text have existed for decades. Chatbots have existed for decades. Theyâ€™re better than they were before, but theyâ€™re still not good. The only reason that the AI nonsense has taken off is because both the news and 4Chan tend to get bored in March.

Skynet is not coming for you. All of the hysterical tech bros will calm down in a year or two, or be drummed out by people who arenâ€™t morons."
Downgrade?  That is my monitor lol
Just tax housing beyond your primary residence (excluding stuff like apartments and stuff needed for temporary living situations) at a rate that makes profiting off of it impossible. Problem solved.
"> public housing must be built at a rate that makes it nonsensical treat housing like an easy investment.

I don't see why governments that actively restrict building would suddenly start building a ton of houses themselves."
Its mostly the fact there arent enough tradesmen to push an increase in supply to where it meets demand.
"Talking about the technology, not number of houses. Houses are still made out of wood/stone/steel with similar building techniques, no nanotechnology needed. There aren't any crazy secrets there like in the world of semiconductors, where AMSL and TSMC are almost monopolies in terms of realistic competition. If Nvidia and AMD want to make GPUs, they have to use their tools and services almost exclusively.

Notice how the drop came after 2008 when there was a massive crash related to housing. Probably financially bankrupted lots of housing contractors too, meaning less supply."
That's volume, he's referring to how new GPUs are notably faster than the previous gen but houses haven't changed much in decades, we just build less.
They cannot drop prices much, the fabrication cost is high
12gb of vram isnt that bad, if it was 8 then yeah youâ€™d be a clown
"I did the same albeit 3070ti 12700. Needed an upgrade before Cyberpunk but just waited.

Remember seeing 30 series for almost 3k at times."
The 30 series MSRP is still grossly inflated, 10 series was where any great value with Nvidia ended.
That much time is absolutely not worth saving less than a few hundred dollars
"I'll try. How about this: ""the act of making sure a piece of ported software doesn't somehow run worse or hardware that is twice as powerful as the one it was originally designed for"". 

Nah, just doesn't roll of the tongue easily enough."
"optimization is getting the same results with less work, or the other way around more results with the same work.

Think a waterway with two corners, but you cut a straight line between start and end. You have optimized flow speed with that action.

You reach that with clever computing and well organized code, as useless code still requires power to run. It's why consoles do so well nowadays, because even though their hardware is technically weaker, they produce results that are well above what a comparable PC would achieve..."
That it can run well on most midrange current cards on near highest settings, scale well to weaker hardware with options to achieve reasonable FPS on the lower end without having the graphics look completely horrible. How far can you lower the graphics, how much does the appearance of the game suffer when you do, and how much does lowering those settings boost your fps, etc etc.
GTA 5 (the current version btw) runs on even Intel Integrated GPUs with passable graphics giving easy 30 frames. That's optimization.
"Yeah, it's a term that's lost any meaning. It inspired me to look into what is actually involved in optimizing a game and my non-expert understanding is that it mostly comes down to ensuring that the game only loads things that are immediately noticeable to the player, e.g. textures for things the player is currently looking at or only modelling AI behaviour for NPCs in the immediate proximity. This allows most of the resources to be dedicated to what's currently on screen vs what's 'behind' the screen and in the distance. 

This is also *incredibly* difficult and time consuming to do without things like loading screens or the player noticing pop-in etc. It's also something that has diminishing returns, meaning actual optimization comes down to how consistent the game is at running on the intended hardware vs how long it would take to improve that performance. It's why some games force you to slow down in certain areas or why 90% of game time in the original Mass Effect was spent riding elevators on the citadel; it allows things to load in the background without having to look at an actual loading screen. 

My (again, non-expert) impression of performance issues with the Last of Us is that it has something to do with how inefficiently the game allocates textures to VRAM. It was built for the PS5 which has amazing storage/memory capabilities not yet available on PCs and thus the game is fundamentally designed around not loading things the way most PC games do. Maybe someone with a better understanding can elaborate/correct this."
"It's easy: a game is optimized if it runs perfectly fine in entry level GPUs (currently 1650 and 3050) at a specific FPS/resolution target (usually 1080p@60fps) without compromising gameplay or graphics.

Game devs hate that though."
Because it's fairly subjective. I'd say things are very dire if a 1060 can't run a game at over 30fps ([1060 is still the 3rd most popular gpu on steam](https://i.imgur.com/EzvN0S6.png)). Good optimization would be if it can hit 60fps+.
Facts
"""Game which looks like a game from five years ago but runs at half the framerate""






It's really not complicated. Frankly, there's no reason any game shouldn't be able to run on a Polaris GPU at 1080p60, graphics haven't gotten that much better since then. 






Doom 2016 and Doom Eternal are examples of excellent optimisation"
By making sure that the code and textures run well on pc hardware. The best example I can give you is Doom Eternal Vs TLOU1.
"why did you take a swipe at him?

Optimization's meaning is obvious."
how about: it doesn't require several times more powerful hardware while looking marginally better than several years old titles
"A lot of development TIME is spend on the lighting. (and Dev Time of course costs money)  

Literally every Dev Team i hear complaining about their engines...  Anthem (FrostBite), Mass Effect Andromeda (Frostbite), LEGO Skywalker Saga (in house engine) to name just a few...  point out that they have to wait for the next lighting pass to see the changes they've made, in Unreal Engine they can see those changes live, for Anthem's big world map that lighting pass was done one time a day, over night. And if that process crashed, they had to wait another day.  

So lets say you fix/add/move 50 things on the map on day one, then come back the next day to check on the results ...you need to move some of these object 3 to 6 times before you are happy with the actual result.  
You spend the entire week messing with one object, which in Unreal would be done in 2 minutes, drag and dop, check the result live done.  

This sort of thing will drive you slowly mad.   
Especially when you are aware that a different set of tools does it so much better.  
And when RayTracing is omnipresent, all these clunky old lighting engines that need to ""bake"" a light or shadowmap or what its called, would be obsolete.  

Which is why all the above mentioned Studios instantly tilted back to Unreal Engine after messing with those engines.  

Ray Tracing yeah, well that will fix the problem by just not having to deal with lighting passes at all anymore.  
Of course Dev's would gush about this.  

And every dev studio using Unreal Engine, turning it into a monopoly for EPIC is also not the answer.  
I like when there are multiple competing engines on the market.  
I like Unreal Engine, but i don't want every game to look and feel the same either.  
Frostbite Games like Star Wars Battlefront looked damn awesome.  

We even had DICE devs from BF2042 publicly bitching about Frostbite... their own in-house engine!  
One of the criticisms about 2042 was the lack of cover on the big open maps.  
Development was clearly rushed again, now put 1+1 together, big new 128 player maps and still the same old lighting pass bakery stuff... i'd say there is a good chance the map design in that game was shit because they didn't have time to mess with cover enough.  
They are slowly going through the maps now in patches, one map at a time and rework them, adding more stuff and more cover.  
That is the flipside of it, Publishers will rush games out the door regardless, if the tools are better we get less buggy games at release.  

...  
i'm just saying, i think i get it and it is not as simple as shifting cost."
They got a clue. Its very easy to count geforce experience user and their card model for example
"A lot of the complaints have no context, either.

Do people just try to run their games at maximum setting, with ray-tracing, no DLSS and at 4K?

I'm still rocking a 2060 SUPER at 1080p because it works and I'm fine with it, but I also don't mind 60 FPS still because I spent half my life playing on that. 144 FPS to match my refresh rate is fantastic but 60 is still more than playable. At best I'd upgrade to 1440 and downgrade my graphics if I had to (which I often have to do anyway).

But I've always bought mid-range precisely because top end is overpriced and *always fucking has been*. People acting like this is new but it's not. The top of the line cards have NEVER been worth the price. They've routinely been 2x the price of a mid-range card for ~20% more power. They're not cost-effective at all."
Yes, in eastern Europe a 3060 is 20% of a 4090, but that means that it costs $420. For me this price is unacceptable for an entry level GPU.
"Yea I really don't get it. These guys buy 4K monitors and then complain about GPU prices? It's pay to play. Just like your example - imagine buying a C8 and then complaining about $2000 brake rotors. 

Gaming is a leisure hobby. If you can't afford the latest and greatest maybe it's time to lower your expectations. If you really want to get mad, get mad at your government for letting 2 companies have control over it."
Marketing across all industries has done an insanely good job of convincing people they absolutely need the latest and greatest. I know people who buy new fucking cars every couple of years. I know a guy who buys the new iPhone every single year. Marketing and overconsumption is so insanely deeply ingrained in western culture now. Iâ€™ve seen stories on here about people getting turned down for dates because their fucking texts are green instead of blue. How insane is that?? These companies hire PhD psychologists to maximize the effectiveness of marketing and ads and it works.
">extremely reasonable price

This is subjective."
That resinable price isn't resonable anymore that's the problem.
"You might be surprised to learn that engineers can be greedy, too.

In big corporations it's easy to blame the finance division but there's a lot of people doing the actual work who don't give a fuck either, many of which have no boat in the race and just want their payout.

Same reason why it's not always publishers who are greedy about microtransactions, for instance. Reminds me when people blamed Activision for the Destiny 2 MTX crap despite reports that it was the heads at Bungie who pitched the idea and then ended up doubling down on it after they separated from Activision.

Everyone likes money, because our dumb fucking society revolves around it. Capitalism is a disease that invariably infects every facet of the world, whether you're willingly part of the system or not."
"personally, i'm not sure why anybody really pays more than about $80 for a mobo. 

i can get it if the person is going for a all out build, and intends to overclock everything, but for most people that's not something that is really needed. 

i can't count the number of gaming rigs i've worked on that are these expensive, full featured boards, paired with a mid grade CPU and often not even having all the CPU power pins hooked up. 

i've been building systems for over 20 years. i usually pick out CPUs that are high grade, and then pair them with office class mother boards. my only requirement most of the time is that they're able to run the chip i want at full speed. 

my last system was a Asus H81m board, paired with a used 4790K/16GB of DDR3 RAM/GTX 1080. it was a fantastic system, my son is still using it now. 

my current system is a Gigabyte B365m board, paired with a 9700K/32GB DDR4 RAM/RTX 3070. it's awesome. 

both of these boards were cheap, yet had all the features i needed."
Example: Seagate and their exclusive deal for selling xbox storage expansion cards.
That explains NV, but Amd is out there too with almost non existent market share selling only marginally-less-ridiculously-horribly overpriced GPUs. Seems they are more content with selling barely any cards at an insane markup than a shitton more with smaller margins while also gaining market share.
"The notion that PC gaming is a worse value proposition relies upon a few assumptions:

That what gamers *really* want is to play games at 4K and 30fps,  which is what the PS5 and Series X are capable of unless performance mode is enabled, allowing dynamic resolution drops to 1440p to maintain 60fps.

That gamers who are the most enthusiastic about 4K are not going to regard the PS5 and Series X as underpowered machines. For players who don't want to compromise on either resolution or frame rate, it doesn't matter that these consoles are relatively cheap for their specs, because they find these specs as unacceptable as they are non-upgradable.

That the millions of people playing games on the Series S, or the budget gaming rigs that still predominate among daily Steam users, aren't getting by just fine at 1440p or even 1080p resolutions.

That a budget gaming rig is underpowered, meaning compromises in visuals or performance. True for only a fraction of a percent of games available, right now, for purchase. With many of these struggling to run well even on the new consoles.

And lastly, that the better sales offerings and free game offers from competing storefronts, vastly greater library even before getting into emulation, being able to use the controllers they have, and free multiplayer, ALL contribute nothing to the value proposition of PC relative to consoles."
"This is rough equivalent. It is approximately double to 1.5x based on my list. I didn't have the 3700X - could get it used for like a hundred. Could get a second hand GPU for cheaper.

[https://pcpartpicker.com/list/jRPTQ6](https://pcpartpicker.com/list/)"
"I don't find for myself consoles = PC. Yes console is cheaper, if it checks all your boxes as I said, buy a console.

Since the cost is high, the way I'm measuring and doing this is sell last gen and get the new gen around 50% off. This only works if you have a good card already and flip it. So the initial cost is steep. 

But maybe this will be what kills pc gaming, the thing is, it is thriving in so many ways that you really don't even need much of a GPU."
By buying a Nintendo Switch, you are also giving money to Nvidia though
"> But not everybody will buy that. 

Yet they are mostly sold out, so its priced accordingly. 

All of this sounds like a big old ""too bad"". These companies don't give a shit about you, they make products to sell. If you can't afford them then find something else to do. MAYBE if you're lucky this will force prices to go down. 

It wont though, because gaming is a leisure hobby that requires extra time and money to begin with. 

The mercedes subreddit isn't full of people being sad they can't afford an AMG now is it? Because they **dont even make it in there**

>  You could buy a 30 series sure. But buying a 2 year old architecture shouldn't be the norm. 

Buddy most people in the united states have less than $1000 in savings. 

This is the norm. You just got to it."
But it can surely grow sucking up more need for their products.
What... do you think they train natural language models on?
If you want to generate AI art locally you need a powerful GPU, how quickly it renders and how high you can push output res is directly tied to VRAM. So yeah, new competitor for demand that'll grow rapidly sadly. I'm like 2/3 wanting to replace my gtx 1070 because gaming, 1/3 because im sick of waiting 3 minutes for a single 1024Ã—1024 diffusion render...
The prices were high because scalpers and crypto happened, then manufacturers realized they could sell for super high, and now it's just because they refuse to lower prices
Maybeâ€¦ read the fucking article?
">Crypto and scalpers proved people were willing to pay current prices.

No it PROVED that mining operations were willing to pay for the GPUs and other people who scalped them AND regular PC upgraders were more than happy to get their money. The fact that GPU sales are hitting a slump with few willing to pay the price of more than a PS5/Series X console for essentially ""$300-$400"" tech is telling."
because nvidia has no competition. if AMD made greater cards at cheapr making costs, they would offer them for less
"Yeah I'm aware of the irony.

But they were in a one horse race, on GPUs they are entering a 3 horse race."
[deleted]
Why would they try to gain market share when their market share is like 80%?
Yes AMD ultimately knows how many GPUs they sell and make to sell around that amount at the max price they can charge. It's not worth it for them to keep selling consumer graphics cards when that silicon could be an Epyc or Ryzen processor.
Pay to play.
"The rub is that you have no control over what companies choose to charge. If its out of your means then too bad, it doesnt matter how many PCB layers or VRAM it has. Someone else DOES have the means to afford it.

Guess what, turns out it's not too much because people are still buying it. If you don't like it, please vote for better regulation. Too bad most gamers lean right wing and won't care until its $4K for a GPU and THEY are priced out. 

You are trying to logic your way out of a situation that doesn't care about your logic at all"
"Thats nice and all but once you go down the stack and budget becomes more important those features become less and less competitive and then you have to ask yourself why would you buy a 3050/60/60ti/70 when you can get an AMD alternative thats faster in raster for less money.

As for Intel. They haven't released a card thats faster than a 3060ti yet let alone anything close to RDNA3 and Ada. Their drivers and game per game consistenty is still pretty hit or miss and their next GPU's are over a year away and will launch only a few months ahead of Nvidia's RTX 5000 series and AMD's 8000 series."
"Sure, but what percentage of people buy 1k cards? 1%? 2%?

The 6000 range is better than the Nvidias equivalent at the moment."
Found another one!
"Right, for the ai group, not for the consumer side...

The consumer side is honestly not as fast or perhapd its the optimization of games instead that really lends help to my og comment."
Weâ€™ve always had the power to influence the top, we just never co-ordinate efficiently.
Consumer activism totally worked to keep WOTC from changing their D&D license.
Break up Nvidia into what, though? Datacenter is about to hockeystick. Even people who pay for GPT4 have limited access to it, so they need more just to hit demand. They'd rather sell $200K DGXs to help train GPT5/6, self-driving and robotics. There are investors that would *love* to see them cut sweaty gamers loose.
"If you want any type of movement to have a fighting chance you need to get it advertised where the average consumer is at. On Xbox/Playstation/steam and facebook ads. The places the average gamer is going to be. Reddit isn't a place for the average gamer, nor is youtube because you need to know specifics when you search, and the average consumer isn't going to be searching for specific things that would be related to any kind of tech movement.

That kind of campaign would take a huge amount of funds that as a collective I don't think we'd even scratch the surface for. That's not to mention then you also need to think if Sony, Microsoft, steam, Facebook would even allow those kinds of ads on their platforms, then you are also at the mercy of the algorithm getting those ads in front of people. And finally, actually getting people to change their ways. This would take years of getting it plastered all over the place, and once again funds become an issue.

Right now it really seems like were at the mercy of AMD, Intel and Nvidia getting too greedy and it backfiring on them, then any actual movement achieving anything.

This kind of movement is vastly different than others that can take to the streets.

Maybe I am just thinking about it the wrong way, but this is the only way I can think of that would reach the audience that you'd need."
Because most of the takes here are weird and stinky
tbh new GPus have never been either. It's just now that the middle class is disappearing redditors are starting to get what its like
Op was using sarcasm to reply to the wishful "Looks like itâ€™s time to play chicken." statement.
He was being sarcastic lol
"The steamdeck subreddit has flairs for the amount of storage their deck came with at launch. 

Imagine buying a PS5 and you have a pathological need to let people know you got the ""fuck me harder daddy"" 2TB hard drive instead of the filthy peasant 64GB version. Which a significant number of people are replaying anyway with 3rd party SSDs....."
"Some of the first major titles to breach 8/10GB VRAM were AMD sponsored, and RT is VRAM intensive to begin with. 

Let's not make up a narrative either, Nvidia ALWAYS skimped on VRAM. With the new consoles finally getting focus, and a number of AMD sponsored titles and non-sponsored titles that are pushing higher VRAM the cards that were skimped on are going to be falling down if people don't tweak settings."
"That's not really ""planned obsolescence"". If anything, today's hardware has more staying power than it did back in the 90s and 2000s, and even enthusiasts can go years without needing to buy new hardware. Even something like a 1080ti is still viable if you're doing 1080p gaming.

It's only if you're doing 4K gaming while trying to stay on top of all the cutting edge stuff like ray tracing that you'd be putting yourself on the upgrade treadmill and buying new hardware every year."
"I completely disagree. GPT-3 is not going to take over, but its a remarkable advancement in chatbots and GPT4 is a solid advancement over 3. The pace of innovation is quite fast here.

If nothing else, there is going to be strong demand to integrate this into word processors, search engines and customer support. It takes a lot of computing power too."
the problem isn't really individuals, you also have companies that exist to own and manage properties as landlords and unless you ban or at least heavily restrict that then your proposed solution won't work. when you account for the pushback that such a bill would face due to those who profit from housing it'd legitimately just be easier for the government to build more housing itself.
That doesn't solve supply/demand. Most houses are owner-occupied.
Canâ€™t solve any problems if BlackRock owns your politicians
Less houses are built than are needed, and many that get built are more expensive because that allows for profit. After regulations the cost of entry to building a house is high, so a cheap house doesn't net much profit. That's what I meant by comparing the GPU market to how houses have been built and recent changes in the automotive market, not enough volume, but priced high to still achieve profit.
They sell at multiple hundred of dollar markups, there's room.
But couldnâ€™t you get a 3080 with more vram for that price?
Maybe, but at the time It was either no gpu or pay 2-3x the price
">I'll try. How about this: ""the act of making sure a piece of ported software doesn't somehow run worse or hardware that is twice as powerful as the one it was originally designed for"".

How does it run worse? PS5 can only do 4k 30 or 1440p 60 fps."
">... near highest settings ...

I think this is part of the issue. The highest settings in a game absolutely *shouldn't* be playable by anything other than the best graphics cards at the time of its release. The highest settings should be pushing the boundaries so that when people play it years from now on better hardware, the game won't look as dated. 

I do agree with the rest of your post in regards to scalability etc. but the vast majority of complaints I've seen about a game being 'unoptimised' are from people who refuse to accept that 'high' and 'ultra' actually mean 'high' and 'ultra'. 

The only people who can truly know how well optimized a game is are obviously the Devs, but subjectively it should just be 'how well does this game perform compared to other games with similar graphical fidelity' and whether there's a good reason for any discrepancies e.g. open world is substantially more taxing than a linear one."
">That it can run well on most midrange current cards on near highest settings, scale well to weaker hardware with options to achieve reasonable FPS on the lower end without having the graphics look completely horrible

This is incredibly unspecific because not every game is copy paste in how it is build, it's performance demands and what not. 

&#x200B;

Just because it can run a game at X settings 2 years ago doesn't mean that it will 2 years later."
">GTA 5 (the current version btw) runs on even Intel Integrated GPUs with passable graphics giving easy 30 frames. That's optimization.

GTA5 was build with PS3/360 hardware in mind as the goal.  For modern laptops that hardware isn't much more powerful then an IGPU."
">My (again, non-expert) impression of performance issues with the Last of Us is that it has something to do with how inefficiently the game allocates textures to VRAM. It was built for the PS5 which has amazing storage/memory capabilities not yet available on PCs and thus the game is fundamentally designed around not loading things the way most PC games do. Maybe someone with a better understanding can elaborate/correct this.

The funny thing is I have brought this up multiple times to people and no one actually ever responded. But I do get a lot of down votes for pointing it out.   Personally I do think that this is a factor.  

&#x200B;

PS5 has custom dedicated hardware to decompress assets. Which frees up the CPU/GPU from needing to do those tasks.  Without a similar dedicated hardware in PC and without programs to utilize that hardware the load will be transferred to CPU/GPU.  It also means more assets will need to be kept in the buffer to allow them to load in time. Which will also have an impact on performance."
">It's easy: a game is optimized if it runs perfectly fine in entry level GPUs (currently 1650 and 3050) at a specific FPS/resolution target (usually 1080p@60fps) without compromising gameplay or graphics.

&#x200B;

That isn't what optimization is though.  That is a performance goal.  The reason why Doom Eternal is able to hit 60 fps even on PS4/One's shitty hardware is because the developers specifically targeted that setting and build the entire game around it. which is why it is all narrow corridors and open arenas so the game doesn't have to render anything complex while fighting."
Why do you think  6 GB of VRAM should be the standard?
No game developer should cater to PC hardware weaker than the weakest console they target. I bet most of those 1060 players have never played an AAA game released in the last two years (other than CoD or sports games) and either never will or wonâ€™t do so until they upgrade. Many of them donâ€™t play anything more demanding than CS and Dota
">It's really not complicated. Frankly, there's no reason any game shouldn't be able to run on a Polaris GPU at 1080p60, graphics haven't gotten that much better since then.

I mean they have.   Better shadows, better lighting, more detail, higher draw distances, etc."
"Doom Eternal was created from the gound up specifically to target 60 fps on Ps4 and One consoles. This goes beyond ""optimization"" as most people would apply it. Level design,  shaders being used all have an effect. 



You can absolutely make a game that runs at 1080p 60 fps on a PS3. You can make a game that runs at 1080p 60 fps in a PS4. But they are not going to look the same at all."
You sure about that statement?
"I'm sorry, but doing an entire lighting pass just to see how 5 or six objects look when you move them a bit sounds really stupid unless you are specifically working with something where it's important to get the lighting absolutely precise. You shouldn't be constantly rebuilding your lighting like that.

Also, Raytraced previews should drastically reduce the amount of builds you have to do to preview something even if you ultimately have to offer baked lights for the cheaper/older cards."
"You don't have to install that bloatware... all you need is the driver.  
...i don't install it."
"> Do people just try to run their games at maximum setting, with ray-tracing, no DLSS and at 4K?

I'm firmly in the camp that the people who constantly online-fight over specs, gimmicks and benchmarks spend far more time doing that than they actually do gaming, especially utilizing the things they scream over. 

I've been playing at 1080/60 with a 1070 for almost a decade now (including running a VR setup off it) and trust me, it ain't that important. It's not 2007 anymore, hardware has a lot longer life than it used to have and the tentpole games that people give a fuck about longer than a year, aren't super graphically-intensive.

The fact that the games people use to exhibit these new tweaks and resolutions tend to be either 1. Released in a heavily-unoptimized state where your fancy-schmancy new card doesn't really give you the oomph you were hoping for or 2. Are just busted in general in ways unrelated to graphics/processing power that decrease enjoyment only boosts my point. People want the ""flex"" more than the gaming experience."
"> Do people just try to run their games at maximum setting, with ray-tracing, no DLSS and at 4K?
> 
> 

Judging by the comments people leave, yes, that's exactly what they're doing."
"Totally agree.

Gaming at 1440-1080 and 60 FPS with medium settings is still an amazing experience. You really do not need 4K 120 FPS to enjoy a game and I am sure that you can hold your hardware for much longer.

Think about the games you play daily, think about your backlog. You really need that upgrade? Sure Hogwarts Legacy and TLOU have crazy VRAM requirements but the solution is simple: just don't buy them. Do not encourage developers to launch games in such state.

My approach right now is to hold onto my hardware (I know I know, my PC is quite powerful) until I cannot play the games I want at 1440p and 60 FPS medium settings."
"It baffles me how capable people is to create new needs from themselves, that create other needs such as feeling the need of a 4K@120 hz monitor and then realising you need a 4090 for that.

It happened to me with VR. I felt the need of playing flight simulators on VR so I bought a VR headset just to realise you need crazy amounts of power to have an enjoyable experience.

I would drop VR entirely before buying an expensive GPU just to enjoy some home flight simulation."
"Sure, it's possible that PC gaming is simply outside of the price range of some people. 

But I can buy a used 5700XT from a top brand like Sapphire today for $180 and that will absolutely crush any game at 1080p. No, it may not be max settings and 100+ fps, but again, those are luxuries. 

If you want luxury features, you pay luxury prices."
"A used 5700XT from a top brand like Sapphire can easily be found for under $200. 

It will run any game in existence at 1080p/60fps. 

You don't need the latest and greatest. Nobody does. 

If the new market is too expensive, either buy used or buy a lower tier than you have in the past."
"> engineers 
>
> it was the heads at Bungie

Huh? Why do you mention salaried employees and then provide an example of C-level executives being greedy?"
You might be surprised to learn that all the engineers and finance people are on salary and make the same amount of money no matter what price the company decides to charge for a product.
Why did you get a K CPU for a board that can't overclock? Could have saved money.
"> personally, i'm not sure why anybody really pays more than about $80 for a mobo

Please point me to motherboard that's compatible with modern Intel or AMD that has:

1. 2.5 GB ethernet
2. At least two PCIE 4 M.2 slots
3. Onboard wifi
4. USB 3 type C *and* A front panel headers
5. has a BIOS reset button
6. Has at least two case fan headers in locations that aren't awkward.

Bonus points if it's in ITX. Form factor. Last time I went shopping the cheapest board that had everything I wanted was $189."
Pretty sure you can't overclock your RAM on a B365 board. No XMP kinda sucks.
An $80 board will have garbage VRMs and will overheat when taxed, most, but not all in that range have no WiFi, BT, M.2 slots or heatsinks on VRMs. I'm not justifying the crazy markups lately but to say a cheap board is just as good is just wrong. Not to mention some people take pride in the appearance of their system and an office motherboard ain't it for a good looking system.
"Yep. NVMe prices are bottoming out at the moment, PS5 compatible 2tb drives are about Â£120-130 where they were over Â£200 a month or so ago.

Meanwhile a 512gb seagate storage expansion card is Â£109 on Amazon, 1tb is about Â£180... it's beyond a joke.

I don't even own either console and I'm offended on their behalf."
AMD doesn't sell to PC they sell console chips.
">That what gamers   
>  
>really  
>  
> want is to play games at 4K and 30fps,  which is what the PS5 and Series X are capable of unless performance mode is enabled, allowing dynamic resolution drops to 1440p to maintain 60fps.

This should be taken into consideration with every debate regarding the value proposition. We have to compare apples to apples, and consoles can only run games in native 4K at 30 FPS and medium settings. Many PC (and not necessarily with top notch hardware) can do this.

When devs want their game at 60 FPS, then they drop resolution most of the time by using variable resolution and that is not 4K. It still looks great, but if we are comparing the value proposition then we should consider this because 1440p upscaled to 4K at 60 FPS is not that hard to achieve neither it is 4K native at 30 FPS.

So a PC capable of running these settings is not that much more expensive than a console, plus games are cheaper, no multiplayer subscription, better upgradability, much wider catalogue with literally decades worth of games, and you can use your PC for working and personal administration stuff."
That wasn't my point. My point is that a lot of people still enjoy games that run on a system with worse specs than their smartphone. It suggests that you don't need graphics that push bleeding edge hardware to sell games in 2023.
Never thought about it... ðŸ˜¢
[deleted]
Thank you.  Now I hate Nvidia.
if people don't buy they will lower prices, as long as people keep buying they will increase.
Scalpers profited, then Nvidia decided they would charge as much as the scalpers the following year - and the idiots bought it up (sadly i fell for an overpriced 2080ti, i am one of thoses idiots, soz)
Yeah, that's what I need ðŸ¤£
"the mining scene is completely dead but the price don't change.

It is more proven that its mostly greed"
"For sure. 

And imo, i genuinely think we might see amd outright curb stomp nvidia, or another competitor come right in and do it to them.

Usually companies like nvidia or intel need to be taught a karmic lesson before a meltdown occurs of their share prices."
people want amd to be competitive so there would be reason to buy amd gpus .
"To some extent that's true. But if Nvidia won't see a challenge they won't drop prices. So AMD will slowly crawl back the market share. Of course if Nvidia drops prices or increases performance in the category(Jebaited) then AMD won't sell as many units and will get less money per unit. And from my understanding they went with low volume high margin tactic.

But what is the target audience though? Hardcore AMD fans? Right now Nvidia leads in software and dedicated hardware for this software. AMD kinda competes on price. But if Nvidia and AMD cards are close in price why go AMD? To piss off Nvidia? Maybe cutting down the price is not the best way forward. But they need to do something, anything. Because right now AMD is budget Nvidia which depending on the market and AIBs might not even be cheaper."
"It's like people complaining about other people on the road in a traffic jam when THEY ARE THE TRAFFIC.
No mystical buyers are going to show up out of thin air to buy enough AMD gpus to lower nvidia's prices, you'll have to do that yourself.
Remember that nvidia has full telemetry in their driver suite so they know exacly who's running what and how often people upgrade, they've got you by the balls."
Huh? We're talking GPU market where their market share is an abysmal 10% or so.
"Nothing in there changes what I said.

But downvote away."
"RT? Yes. DLSS 2? No. I'd argue the lower you go the more important upscaling is. Up to a point.

Although I'd be fine with XeSS XMX if Intel keeps working on it and Battlemage is competitive in raster and value."
"I thought I saw a 2024 release window for Intel's next line?

Edit: I'm an idiot and just woke up thinking it was 2022."
So what was DLSS 3 if not a new feature for the consumer?
There's a problem there with the fact a significant portion of people just doesn't care.
"Coordination is easier on corporate millionaires who only have to covince half a dozen of their friends to cash in and then watch every other corporate leader do it too, since they all want the same thing anyway.

Consumers will rarely be as coordinated because not only are there way too much of us by comparison but not everyone gives a fuck about the same thing.

$60 to someone is the equivalent of $1 to someone else. Means nothing to them, they'll spend it 100x over and never blink an eye. Or they're addicted and in huge amounts of debt and need a lot more help than we can provide them either way."
Because most people aren't willing to sign onto a vague crusade against GPU prices when in reality most people are perfectly willing to invest that kind of money into a hobby then spend a majority of their free time partaking in.
"of course, but if we are to through all the effort of making a mass grassroots movement, might as well make it be against the economic system which allows these perverse incentives in the first place instead of fighting for something as small as GPU prices, no?  
these same incentives exist for any other industry and can be much more destructive. even worse ones like companies making propaganda that the product they sell isn't a health hazard (lead, oil, tobacco, the food industry in general) are relatively commonplace. if you keep going after symptoms instead of their cause then you're just playing a perpetual game of whack-a-mole."
[removed]
"> Break up Nvidia into what, though

smaller companies that have to compete with each other."
The only remarkable thing about GPT is the big training set. The underlying tech is still not fit for most purposes.
Then ban and heavily restrict that?
">and many that get built are more expensive because that allows for profit

""For some reason, people who build housing became greedier"" isn't why housing is more expensive. It's become harder to produce because city governments have become the bottleneck that determines how much of it is allowed to be built. The result is scarcity and rising prices."
That has 10 gb vram
"Well The Last Of Us on release on PC would stutter on a 3070 ti at 1080p medium settings.

So probably like that."
I did say *NEAR* highest settings, not highest settings...
Yeah and what I'm saying is that game companies can atleast take inspiration from it if not building it the same way (PS3/X360) in mind.
">The funny thing is I have brought this up multiple times to people and no one actually ever responded. But I do get a lot of down votes for pointing it out.   Personally I do think that this is a factor.  

People don't generally like having their opinions challenged, so they tend to get defensive when anyone disagrees with them (regardless of validity). Combine this with people upvoting comments that support their own existing bias or opinion, and I suspect many people reading the comments assume that anything different from the 'established' opinion is either wrong or someone trolling. 

>PS5 has custom dedicated hardware to decompress assets. Which frees up the CPU/GPU from needing to do those tasks.  Without a similar dedicated hardware in PC and without programs to utilize that hardware the load will be transferred to CPU/GPU.  It also means more assets will need to be kept in the buffer to allow them to load in time. Which will also have an impact on performance.

It makes me eager to see how developers can take full advantage of it. Hopefully, everything that's getting implemented on PCs (and Xbox) will allow similar levels of improvement there."
An unoptimized game is a game that doesn't meet the performance goal. Cyberpunk was targeted at PS4 too, didn't work out.
The 1060 is still a very popular card, nvidia is releasing a new desktop 6gb card soon with the 4050, new gpus are very expensive, and many modern games like resi 4 remake, cod vanguard/mw2 remake, and hogwarts legacy still support them (in fact, they all support 4gb)
I'd imagine the number of people playing recent triple a games with low end cards is decent. The cards are more common and accessible, they can still run the games fine, and upgrading costs as much as a new console. The ps4/x1 gpu is roughly equivalent to a 1060 so that checks out too. Plus good optimization is ideal for stronger systems too.
"you dont have to but if you have people buying GPU that they dont install driver and GE on, you pretty sure know it's being used for cryptocurrency.

Maybe a +/- 5% errors at the most. 

If steam can know who is using which cards, no way these companies aint."
"After the first 30 minutes or so you stop paying attention to the graphics. Gameplay is what matters.

The only field where I THINK a 4090 purchase is justified would be video rendering and stuff like that. Gamers don't need it imo"
"because the K chips are better binned, and have a higher clock than the non K models. 

in the case of the 4790, the 4790 base clock is 3.6Ghz, and can boost up to 4.0Ghz. the 4790K base clock is 4.0Ghz and can boost up to 4.4Ghz. all of this without overclocking. 

the 9700, the base clock is 3.0Ghz, while the boost lets it get up to 4.7Ghz. the 9700K's base clock is 3.6Ghz and gets up to 4.9Ghz. 

all of this doesn't really make a big diff of course. but i usually buy K chips when i do my personal build. i find them to be stable and long lasting. they also tend to bring a little extra if i were to sell the system. 

i never do that though, my systems just end up becoming my kid's systems. 

looking at current prices the 9700K is cheaper right now than the non K version. why would you buy the inferior version for more money?"
"There's a pretty significant difference in boost clocks between non-K and K/F/S CPUs and the unlocked multiplier comes in handy if you want to *downclock* to hit a lower TDP without having to deal with the instability that can occur from dicking around with the base clock. And since it's better silicon, undervolting is also easier.

Is that useful to anyone outside of somebody trying to cram a desktop Core i7 or i9 into a system with the internal volume of a pop bottle? Not particularly. But, that is one use case I can think of."
"that's a special use case. you're not going to find a average/regular board that hits all those spots. 

1) most internet in the States doesn't go over 1Gbps anyway. so a board with a 2.5Gbps ethernet connection is kind of pointless. on top of that, a add in card could add that capability to any board. 

2) two PCIE 4 M.2 slots is another special use case, most people would be fine with one. also another case where a add in card could add that capability to any modern board. 

3) onboard wifi is nice, but a tad redundant. especially if you're going to hardware the computer in to the network with the 2.5Gbps ethernet connection. also another feature that could be added via expansion card, or even a USB dongle. 

4) USB 3 headers are common. this will mostly depend on the case you pick anyway. the board needs to have the connection on it to make use of it, but the front panel connections are going to be determined by the case you pick. 

5) BIOS reset buttons on the back of the board are nice, but pretty much all boards have the reset pins on them already. maybe a little more difficult to get to, but a button could be wired up fairly easily during the build and then left inside the case somewhere. i mean, realistically, how often do you need to use it anyway.

6) loads upon loads of boards will have the headers, but like you point out they may be in awkward locations. extension cables and harnesses are abundant and cheap, and will work with any modern board. you don't really even need two of them, just one, the plug in your harness, and split it off to other fans. 

if you read through what i wrote, i mention add on, or other, solutions a lot. the fact that you can get a board that does all those things WITHOUT all the different modifications is going to be great. ESPECIALLY since you're looking for all that in a ITX package, which is going to be quite limited on space. 

like we tell customers at the shop i work, small costs extra. so yeah, that's all special use case, that is not in any way representative of the average needs. the examples i pointed out of customers coming in with high end boards, where they're using mid range CPUs, and the power pins aren't hooked up correctly, could've just used the $80 boards i'm using/have used. 

these people are using standard ATX stuff. they don't need two m.2 slots, their internet is often much slower than 1Gbps, they're typically using wifi anyway, etc, etc."
[mine does](https://imgur.com/XEeJ4PU)
Same thing happened with the psvita and their expanded storage. Sony didnt want to just use sd cards. As soon as ?Microsoft announced this I knew their storage prices would be a ripoff.
It's like saying NV does not sell to PC, they sell to data centres.
It's fairly well known that that's the main use for GPU nowadays. Nvidia makes the big boy cards for Google and Microsoft, and we get the scraps in the form of the 4090 and whatever else
Our fight for lower GPU prices is far from over.
I mean, it's not like they actually put different chips in their professional cards, and it's not like those things are in massive oversupply. There's no difference to TSMC if the chip goes in an RTX 3090, or an RTX A6000. It's just one is 1000$, and one is 10,000$
You should probably hate AMD too, they aren't exactly fighting against this new standard.
"You're not to blame either - all of us enter this market expecting something wonderful.

Little me got what he wanted with the 1060, and I don't think I'm going for top end graphic cards anyway for a good amount of time."
"I don't think AMD are going to curb stomp NVIDIA.

NVIDIA continually offer new features and improve existing ones while AMD is constantly playing catch-up and rely on the goodwill for their solutions being ""open"" to make up for them being worse.

From day 1 Intel had a top tier ML-accelerated upscaling solution. They also had novel features like smooth sync that are great examples of outside the box thinking. If they keep up that momentum while getting their drivers and hardware up to par it's AMD that's going to be curb stomped."
[deleted]
">Remember that nvidia has full telemetry in their driver suite so they know exacly who's running what and how often people upgrade, they've got you by the balls.

Use NVCleanstall to remove telemetry"
Oh Yea, amd.
Im supporting your thesis on Nvidia only sells GPUs.
Did you read what i wrote in my last post?
"There will always be those people that buy a ticket to every teen summer flick and laugh at the terrible jokes. 

If ever there was a time.... HOLD THE LINE"
"People are impatient and go straight to thinking this.  


Truth is trends like this take time and a lot of persistance. You gotta be with us here saying the same thing instead of doubting. I've watch SLOWLY, oh God ever so slowly, the trend of not pre-ordering games steadily grow to where I'd say it's now a somewhat popular thing to do.  


But it's taken countless people making videos, talking in comments, ect.  


Getting a kid to brush their teeth or eat their vegetables takes work, something like smoking cigarettes doesn't need any effort, it will create a following of people very well all on it's own."
"But importantly, to what degree can you fault them?

Ultimately, gaming is a luxury leisure time activity. Something you to do to relax and calm down after work and whatever really serious problems your life throws at you.

Your ""problem care meter"" is simply maxed out if you come off an 8 hour shift + 2 hours commute and then spent an hour on the phone about some legal trouble or yelling at your teenage kid and despairing over how to be a parent.  
Something like ""Hrm, maybe gaming GPUs should cost less"" really couldn't register any less if it tried.

That is to say, it **could** be something people care about. But to do that, you'd have to solve a host of issues **first**, so that everyone has their plate cleaned enough they'll even realize this problem exists."
that'd be good, but it's not that simple. passing bills that go against powerful private interests is very hard, because those are the people who finance the campaigns of lawmakers. obviously passing this type of bill would be the optimum solution, but it's rare that such an opportunity arises. there are some good urbanism youtube videos that discuss housing, [this one is pretty good](https://youtu.be/sKudSeqHSJk) and discusses how government owned housing (or just housing that doesn't exist solely for profit making) may help.
Forgive me, I'm pretty new to pc gaming and I don't quite get these things. I believe my laptop 3080 has 16 gb vram though.
">Well The Last Of Us on release on PC would stutter on a 3070 ti at 1080p medium settings.

And I've seen other people with the same GPU have no issue at higher settings and well above the frame rate a PS5 could do."
You can thank Nvidia for only giving the 3070 Ti 8GB VRAM. It is weaker than the PS5 in the area that matters the most for that game
So itter technology stagnation is your argument?
"Xbox has their own equivalent to Kraken. If it is better or not is pretty irrelevant as it still provides a bonus of freeing up some processing power.


The question is if it will come to PC and what form. Or if direct storage is the closest that will happen."
So if a game can't run on an integrated GPU in a laptop it is also not optimized? I should be able to play the latest game on a non gaming laptop at 60 fps and medium settings?
">The 1060 is still a very popular card,

Popularity doesn't mean anything.  360p was a popular resolution and a lot of people still had it. Doesn't mean jumping to HD should have been delayed.

&#x200B;

>â€‹ and many modern games like resi 4 remake, cod vanguard/mw2 remake, and hogwarts legacy still support them

Every game you listed is a cross gen console game.  This game is not."
If you want to buy something and really not have to worry about upgrading in a long time, I can see how it might be a worthy investment. That's basically what I did when I built my rig, the 1070 was shit-hot when I got it and it's served me well for a long time.
Obviously if the K model is cheaper you would get that one either way.
Gigabyte did you a solid!
"Nvidia makes the best gaming GPUs and although data centers are a larger part of their portfolio, gaming is still almost 50% of their sales (depending on how you measure it I guess).

AMDâ€™s most successful chips are always the console APUs. If they didnâ€™t sell to Sony and MSFT they probably wouldnâ€™t make GPUs anymore."
[deleted]
This kind of happened to everything.  Inflation has a much smaller effect on the current prices of anything than corporations simply realizing they can get away with charging whatever and turn an even larger profit than before.
Your comment reminds me an alien vs predator.
Oh please... The RX 6600 was \~$450 for months after launch. Amazon wanted to charge me $469 for a PowerColor Fighter (no backplate) in Q1 2022. I ended up getting my nephew a refurbished EVGA 3060 12GB for $369 instead.
I'm aware, but thank you.
True, my b.
Love isn't always on time!
i been holdin the line since 2017 now...at this point im just gonna buy a 4060 and donÂ´t care. it is what it is
Idk if government owned is good, just tax exponentially for amount of land owned to avoid monopolies for land which the companies donâ€™t have to be drafted to protect.
Mobile GPUs aren't equivalent to the desktop versions. They should use different naming in my opinion. I don't know why they do that, maybe someone can chime in for the reason (I also don't know a whole lot)
[Maybe an hour long video](https://www.youtube.com/watch?v=xQ2emuUoxrI) about the topic will convince you.
nah but they should look out to make their games playable for atleast the last 2 gens (if the game runs at Max settings for a 30xx, it should still run well on medium settings for 20xx/16xx series, and so on)
"Well I know Microsoft is also introducing Direct Storage, which should obviously help regardless of hardware. But my logic is, since Microsoft has committed to releasing every Xbox exclusive on PC as well, developers won't be able to do anything magical on the Xbox if it can't also work on a PC. Games made exclusively for the PS5, however, won't have that restriction.

It's entirely possible the differences won't equate to much; I can't pretend to know what specific advantages there are for developers to utilise, nor how significant the differences are when it comes to actual implementation in games."
An integrated GPU isn't an entry level GPU, what are you talking about lol.
"Of course popularity means something, neither consumers nor companies will benefit much if lots of people can't handle the new games. Take a look at the windows vista situation

Days Gone: 3gb

Uncharted legacy: 4gb

Spiderman miles morales: 2gb

Returnal: 6gb

TLOU part 1: 4gb

Persona 5 royal: 2gb"
"Thatâ€™s kind of my thing on these issues anyway. Yeah you might save some money, but the amount you save is small, usually less than a meal out.

I advise a lot of people not to buy the F models either. Sure you might save, at most, $20. But if your GPU dies, most people would gladly pay the difference just to be able to continue using the PC while they sort out a replacement GPU.

Might as well just get the chip with the iGPU in the first place."
"i've been really happy with the board. 

when i bought it one of the things i needed was to support 9th gen Intel out of the box. i didn't want to have to update the BIOS with a 8th gen chip first. 

it was a bit of a toss up between the Gigabyte board and a ASUS one i was looking at. i decided to take a shot on the Gigabyte one, and it's been fantastic."
"Sorry, I think you've missed my point. 

""Besides, what makes A cards worth so much more is the VRAM. "" What makes it worth more is that the target audience will pay more. Doubling the ram does not make a card cost an extra 7000$. The actual chip inside those two cards are identical. They can chose what they put it in."
"There's a [paper](https://www.epi.org/blog/corporate-profits-have-contributed-disproportionately-to-inflation-how-should-policymakers-respond/) saying a big part of inflation is caused by increased corporate profits.

>Since the trough of the COVID-19 recession in the second quarter of 2020, overall prices in the NFC sector have risen at an annualized rate of 6.1%â€”a pronounced acceleration over the 1.8% price growth that characterized the pre-pandemic business cycle of 2007â€“2019. Strikingly, over half of this increase (53.9%) can be attributed to fatter profit margins, with labor costs contributing less than 8% of this increase."
People really like to quote 6600 price. But during crypto boom this card was crazy expensive and only came down in price after the crash. Yes. Right now 6650XT often can be had cheaper than 3060 and a better value. But this wasn't always the case.
Whoa whoa whoa
Ah, I see. So if all else is equal, a desktop graphics card 10-12 gb vram will usually outperform a 16 gb laptop graphics card? Or will at least be equivalent?
">nah

 But your argument is quite literally expecting games to remain the same forever so your specific hardware is supported. 


Never mind the fact the game was literally created for a system with a theoretical maximums decompression and transfer rate of 22 GB/s. And your 8GB GPU will have to compensate for that lack of capability."
">An integrated GPU isn't an entry level GPU, what are you talking about lol.

I am applying your logic that if a game doesn't play a certain way on X or Y system then it isn't optimized. 

&#x200B;

You know like how you said Cyberpunk couldn't be optimized because it ran like ass on the PS4. Not because it was a game pushing graphical limits with a ton of stuff that fried the 12 year old CPU. But because you some how think the game could be magically ""optimized"" to run on such out dated and weak hardware while still pushing heavy visuals."
"Yeah that's always a good move.

  The only time I'd recommend an iGPUless model for a normal gaming consumer, is if there's some regional pricing or deep sale thing going on."
[deleted]
"Yep. 

I got into an argument with a guy about the architectural weaknesses of AMD, why it's a mistake to stay the course even as their competitors lap them in RT and MMU development (tensor cores), and why that is crippling their marketshare beyond intentionally deprioritizing Radeon products for wafer allocation for so much of RDNA2's life.

His argument kept coming down to price and how he got his 6800 XT for $550ish and how I must have overpaid for my 3080. Or how awesome the 6600 XT and 6700 XT are compared to NVIDIA's offerings.

AMD's offers a much higher performing product at a lower price depending on the segment...*NOW*.

I bought my 3080 the same week the 6800 XT launched in 2020 and there was a $0 to $20 difference between the board partners with like for like skus (reference models were sold out). My 3080 FTW3 ended up being cheaper than the Sapphire 6800 XT I had been eyeing.

After that, they were basically vaporware for 6 months. If you wanted one, you couldn't buy one even at scalper pricing because AMD didn't bother to make any in any real volume.

When the stock was finally made and shipped to retail, they were priced just as high as NVIDIA'S offerings were if not more so: 6700 XTs were routinely selling for $900 to $1000 at Microcenter, 6800s didn't exist, 6800 XTs were $1400, and 6900 XTs were $2300+. 

People act like this phenomenon of RDNA2 being widely available and being priced down a tier has been a constant when it's only been a thing for the past 6ish months."
No, what I meant is that a desktop 3080 is better than a mobile 3080. And same with every other model. They're not equivalent. I'm not sure why they use the same name since they perform so differently. Maybe they use the same actual GPUs but they can't be clocked as high due to laptop size limitations (heat, space)?
II'm like 95% sure I'm right here. If you take the heatsink of either of these cards, the chip will say NVIDIA GA102. They're literally identical. I'm coming from the perspective of chip manufacture being supply constrained, so they have a limited number of these babies, and they're going to give preference to the higher profit card, meaning the price of the 3090 goes up.
So my mobile 3080 has 16 gb vram and the desktop version has 10 gb vram, but the desktop version is better?
"I'll tell you my dude
A gpu basically has 2 parts, compute and storage.
A gpu is responsible for rendering the scene which involves a lot of math and executing multiple shaders. A gpu will have compute units for this, nvidia has Cuda cores for this. Laptop has 6k ish, desktop has 8.7k or so.
Another factor that impacts compute is clock speed, which tells you how fast those units run. So desktop is 1.7 ghz laptop is 1.25.

So for performance, laptop is 1.25 x 6000 vs 1.7 x 8700. So that makes the desktop one twice as fast.

Where vram comes into play is for loading textures into memory. This means you can probably get 60 fps in a game with ultra texture quality in a laptop chip, but you can get 120 fps but with high textures on the desktop part."
It might depend on the exact task, but in general yes
Ah, I see. That helps! Thanks for the detailed explanation.
"I definitely think that consoles are more appealing now that every new PC has an arbitrary extra few hundred bucks on the price tag as a GPU surcharge.

The greed of GPU manufacturers, and it is purely their greed at this point, is damaging to the entire platform.

In 2016 the Nvidia 1080 launched at $599. In 2022 the 4080 launched at $1199.

&#x200B;

PC gaming cannot be sustained in its current form if a key component to a gaming PC can simply double in price over six years. That's not a stable basis for a platform."
"I think it will push some out.

My argument for PC Gaming was always that yes, at launch, a console would seem better value for the graphical power that you get, but within a year you'd be able to build a PC for the same or less that would compete, and in the long term, the pricing of PC Games and sales eco system meant you had more choice and spent less.

But for casual to semi regular players who are buying into COD, Battlefield, and esports titles like Rocket League, Apex etc, it's going to be hard to ignore.

My main problem with all of this pricing hike is what will pricing the mid range GPUs look like?

On it's year of release, the GTX 1060 with 6gb of ram was $299. That's not even the entry or budget GPU, those were the 1050ti that were $199. It was also a huge jump in performance. The 1060 landed somewhere very close to 980 performance levels.

Even adjusting for inflation a 1060 would be about $370 today. Can you imagine Nvidia launching the 4060 at $370 or $399? I think it's more likely going to roll out at $499 at least given where they're positioning the 4070ti.

The 1060 was a solid choice for anyone wanting to game at 1080p. And the pricing going up from there made sense, you were getting less frames per dollar, but you paid more and more halo tier pricing to go up a level.

Now that has been flipped on it's head. The best value per dollar is the $1600 monster, and the lower tiers don't even reflect the drop in performance in their pricing. A 4070ti at $799 (which you'll never see since they didn't make any founders models, all the reviewers were sent ASUS TUF which are $850), what the hell are we going to possibly tell people wanting to start into PC Gaming? Go buy a 4 year old secondhand GPU?

It's not the end of PC Gaming, but it's definitely going to be a black mark on the road map."
4070ti is â‚¬1000 in Europe. Piss off, nvidia!
"It's been this way for a few years now - prices went up as miners bought most of the GPUs for cypto, add in chip shortages during covid.  Chip shortage issues are mostly sorted, and crypto is having a bit of a reality check, but Nvidia has yet to apply that to its pricing, so it is still pricing thinking miners will buy cards in their thousands at these inflated prices.

Nothing we can do but try to hold out and wait for some competition or reality to set in and prices come down."
They're even more ridiculous here in the UK at the moment since a lot of companies don't even convert the price to GBP.
It's extended the life of my 1080 without a shadow of a doubt, was ready to get a 4070ti but they're 200 to 250 quid above mrsp - not doing it.
Its a racket. I really want a new GPU but I refuse to pay so much.
Yes.  Now that the supply chain problems are finally being addressed.  A PS5 - found one at Gamestop and BestBuy in my area - are around $559 for the disc version which made it an easy purchase for me instead of a new GPU.
Iâ€™ve ceased to upgrade my pc since the prices went haywire. In my current build, I bought everything else new, except the GTX980, which I took from the old rig. Now Iâ€™m stuck with it, as GPUâ€™s cost an arm and a leg here.
Jensen's one man crusade to kill the PC Gaming scene continues apace..
Well my RX 6600 cost me Â£225 which came with The Callisto Protocol and Dead Island 2. It performs near an RTX 3060 in rasterisation for a lot less money. Itâ€™s not all doom and gloom out there but you have to research a bit.
"Its definitely killing the low end of the market.

Honestly i dont think its worth dropping PC gaming altogether. You can still get amazing deals on like 6600s and 6650 XTs but yeah if not for AMD having reasonable prices on their last gen cards right now, id be riding my 1060 until it died.

I think something is gonna have to give or PC gaming will become very niche in the future."
If you want to see insane, move to Australia.
Still rocking my 1050 because I can't afford a new GPU lol.
"Yes, they will.

Most people do not need 4090 class GPU. Most played games can run on quite weak hardware. Entry level GPU prices just got so high that it is easier just picking up a console at this point.

Not just GPU prices got out of hand. Just did a half upgrade on my secondary PC. New MOBO, CPU, RAM, SSD... 500â‚¬ for these 4 parts alone and that is just a ryzen 5600 (non-X), 2nd cheapest B550 motherboard, 16 GB RAM and an ADATA SSD, so no fancy Samsung or anything like that. If i would need a new GPU, 6600 XT would be my bet at 320â‚¬. Add PSU, case, etc and your at 1000â‚¬ for an entry level gaming PC.

A PS5 is 500â‚¬.

I built many PCs to family and friends in the past, but today i would mostly recommend a console if someone just want to play AAA games. If you like me, only play things like 4X, grand strategy, Factorio-likes, Rimworld, etc, then yes, an entry level gaming PC is a must..."
"I wonâ€™t switch to console as my main games are between a few MMOs and Counterstrike. 

1070 still kicking everything fine at medium to high settings. 60-200 frames.

If publisherâ€™s actually released a game worth upgrading for? Something like a half life 3 then yea I will start looking into upgrading. But we all know thatâ€™s not happening anytime soon."
my 1070 is on deaths door and the thought of having to get a newer card is daunting
"Gamestunt explained it best but the fact people will still buy it, will have for consequence to put the lower tier buyer out of the market.

The argument of the second hand will save the day is a temporary bandages.
The stock of affordable 1080p GPU will dry up with time and the next card on the second hand will be 3000, 4000 serie or 6000 and 7000 and card.
They might sell at 50% off but that will still be 400 500$ at minimum.
Finally pushing the entry level to be a thing of the past.

I had high hope for iGpu, but and (and Intel) curent gen iGpu kill that hope..."
"Switched to Xbox Series X and PS5 Digital late last year over upgrading my rig and couldn't be happier.

Graphics card prices in South Africs are insane.  

Most games are designed to run on console first, which is why optimization will always be better.

Forza Horizon 5 looked better on the Xbox over my 2070 Super example. 

Another thing that will help with Ray tracing is Nanite and Luman when more games start using the tech. See Fornite for example. Looked as good as high end PC Ray tracing. 

I have also never played a PC game where graphics was better than Horizon Forbidden West. 

PC gaming is just to expensive now..."
Honestly we're at the point of "is it worth it to play pc?". It costs me several times the cost of my PS5 to get to an even comparable status. A 3060 would be close, i think - add a 5600x or something and you'll be around $1000 before monitor, mouse and keyboard. That's literally a 5 minute build on pcpartpicker but even using a 3600x and a 6600 i feel like it's impossible to deny the lack of value in the sub $750 range compared to a ps5.
Nah this just helps the Steam Deck sell. There's something about gaming at 2 watts vs 1000+.
I think it's been a good while since consoles provided so much bang for the buck.
What is 80 class pricing?
We kind of regressed back to 90s, when gaming PCs were insanely pricey, especially if you wanted 3D acceleration. Well, kind of, you can still grab Steam Deck or basic gaming laptop with GTX 1650 for a decent price, just don't expect 4K@240 fps gaming.
"The argument about pc gaming being way cheaper than consoles are long gone.

PS5 - 500â‚¬

CPU+PSU+RAM+SSD+CASE = 500â‚¬ (And looking at low budget specs, not fancy parts or brands).
Not even talking about Monitors and GPUs."
"There's no question about it. Budget PC gaming will be decimated with budget PC gamers switching to consoles and many mainstream gamers will switch to consoles too. AMD and NVIDIA are going to screw not only us, but themselves as well by shrinking the PC gaming market.

Even if you can afford these GPU prices, **skip this generation** (if possible) because if we support this pricing in this generation they won't change in the next generation. And so there will be less PC gamers and so game devs will make less PC games - especially games that take advantage of high end hardware.

And BTW today's mid range is costing way more than last generation's high end. Inflation doesn't explain the absurd jumps in MSRP. Also remember that NVIDIA is marketing their cards in the wrong classifications making the pricing even more egregious. Yes, they're faster than last generation, but **that's how it's supposed to be**! That's how it's always been. It's no argument for drastically higher prices.

The pricing is not responding properly to high supply and low demand. This requires proper competition but it's obvious that AMD was happy to respond to NVIDIA's absurd prices with (only slightly less) absurd prices of their own.

Intel Arc isn't there yet (in the mid to higher end) but they are A New Hope for proper competition in the future. In response to NVIDIA and AMD's Pricing Menace, we'll be watching Intel Arc's career with great interest."
No real PC gamer will ever "switch" back to a console. People will just buy an AMD or older-series GPU.
I though about this and hypothetically, if prices were to stay like this, I'd stop playing video games on PC honestly, and just stick with igpu. 4080 is $2000 in my country. They are that high which makes me think I am comfortable not playing video games on pc anymore.
At the moment there is no real competition.  Moving forward if AMD and Intel can get there acts together then it will start to affect pricing. At the moment you can only vote with your wallet and not buy them whilst they price them stupidly high.
"There's always some that will pay any price for the top of the line.  That works in the automotive industry, but for computer parts I don't know if that's a workable business model.

In any case it's really a shame what they've done for the budget consumer in gaming capable desktop builds. Personally I've always gone for the low to mid-range.  My current desktop build I did a couple couple years ago has a 1660 Super I bought new for around 250 USD.  It's perfectly fine for 1080p/60 but for anything better it's going to struggle.

So I'm going stick with my current hardware for a while yet.  I seriously can't justify the cost of a new video card as a casual PC gamer.  In times past I would be looking at updating my card and monitor about now.

As far as my gaming needs I just like to sit at my desk and fire up a single player computer game for an hour or two here and there.  Portables and consoles don't really fill a niche for me.  So I would probably give up on video gaming altogether if PC gaming becomes cost prohibitive."
Eh, the big thing is, you don't need these new GPU's. I got a 6800 xt for $550 last year in November and that will last me for a few years at least. Maybe a decade if I don't mind playing at lower settings later and it doesn't break.
I am still rocking a 1070 and I always get one of the next gen consoles anyways, because I like having that option. I will be rocking with the 1070 and series X until I feel like building a new PC is fiscally reasonable. I do not like what PC gaming has become.
">  do you think that will make people shift to console

Yes.  I will either buy a console or stick with my 3 year old card for as long as possible."
It will, and is, hurting the PC Gaming market. Sales of GPUs are the lowest they've been in a really long time. I hope that this is a Nvidia 20-series style misreading of the market, and the 50-series corrects things. AMD also needs to cut prices pretty heavily. Here's hoping that Intel can eventually force some competition back into the market.
At this stage, I have a 1080 in my pc, but I also have a PS5, and I can't picture any scenario in which I can justify dropping 1k+ on a GPU. It's just stupid money. It's not even that I can't afford it, but by buying I'm telling Nvidia I can love with being bent over, and screw that. My PS5 has me covered for years to come. When pricing normalises, I'll bite. Until then, no way.
"Guilty as charged. I own a 1070Ti. Bought it on premiere for 499$. 
2000 wasnâ€™t a huge upgrade for me so i passed. 3000 series were totally unavailable. 
4000 is a complete rip-off and a joke. 
I was reallyyyyyyyyy waiting for good 4070Ti. After seeing it how it ended, I thought â€œwell fuck those guysâ€. 
Ordered today an Xbox series X. 
Tomorrow the package will arrive. 
Middle finger for those greedy mothertruckers from Nvidia."
"GTX 980 reporting in and rocking on Steam Deck.

I'm going Goblin mode on PC for at least three years.  Hopefully Emperor Jansen will be gone by then."
"Just by a GTX 2080 used on ebay and it can play every single game ever made at 1080p60 at max settings.  

You don't need a 4xxx series card unless you're using a 4k monitor or 240hz or VR or something."
Take the Deck pill.
Wait until you realize that society is collapsing and that the Age of Abundance is over. It's all downhill from here, my guy.
"no lol.

last gen is plenty good.

if you have a 4k rig, then of course you will have to shell out more money to push it. But 1080p you dont need top end. Yes latest gen has gotten expensive, but I dont really remember a time where everyone and their mothers were buying the latest cards every year. Plenty were content buying whatever fit their needs/budget even if a gen older."
is it worse than last year's spring?
I'm still happy with my 5700 OC cost me less than 400eur, I'm too afraid to upgrade with these prices, doesn't seem worth it unless I really want Ray tracing at some point.
"Open field for Intel in the low-to-mid-tier, and nVidia clearly doesn't do anything about it (yet), as they've gone raytracing-crazy, yet hardly anyone cares.

I expect Intel to have a sizeable part of the market next year, paradoxically due to nVidia."
"Their intention is for us to buy either old gen, or used if we are looking for a bargain.

Well hey, do hell with that, I'll just play old games instead. I don't need the shiniest newest garbage with maxed ultra or shit, playing my old titles fills me with a different kind of joy.

If the 4060Tie is priced at 600$, I sure as hell hope linus torches the hell out of it, suggesting to buy a console instead, along with an old 100$ work pc, if you want to do standard pc tasks."
"Supply and demand, is the reality of the market.

If someone chooses to overprice something, demand will fall. And they will have to eventually lower the price.

NVIDIA can try to price gouge all they want. It won't work in the long term. Just wait and see. AMD is there and even Intel is getting into the GPU market now. Competition will be much better in the future."
It's because everyone accepted it.
Def considering a series X and a new 65 inch 4K tv instead of a new rig and a cheap laptop for the admin stuff I do on my pc. Still cheaper than building a new rig!
Price of food is insane, considering you need it to survive.
The console shift happened a generation ago already.
 Built my with the 2070 super when it came out. Was planning on upgrading every 2-3 generations. With the current state of gpu prices we're looking at the 4-5 generations so I "get my money's worth". (Also waiting and buying last gen will be an option).
"Prices for GPUs that will run 4K at high frame rates with ray tracing, make you breakfast, tuck you in at night and read you the Hobbit doing all the voices, are insane.

But if you're playing your games at 1440p, you can put a build together that's cost-competitive with consoles, and blows them out of the water once the superior deals on games, better giveaways, ability to use previous-generation controllers, free multiplayer, and larger selection are accounted for. Then there's emulation.

And if you're playing your games at 1080p, there's a glut of power at the low end of the market, and you will be in good company for the next several years, going by popularity of GPUs among Steam users.

Some people will go with a console, for a box they hook up to their TV and play on the couch. Others will get a console for the kids, probably a Switch, and play their own games on PC. While still others may have put a Steam Deck under the tree, and are waiting a bit to upgrade their own PC, satisfied with its performance.

Anyone curious about PC gaming, but working with a limited budget, go binge some Linus Tech Tips before you assume you've been priced out."
"Prices aside.

I don't even know what brand to go for once the warranty on my EVGA RTX 2070 (upgraded from 1070 TI) runs out in year or so and it eventually dies on me.

I haven't purchased another brand since 2005."
Funny enough a laptop with a gaming-grade gpu doesn't cost much more than a desktop gpu at this point. Not that \*\*real\*\* pc gamers play on laptops...
My computer is doing fine for work tasks, my PS5 plays everything fine, and even if I could afford a 4080 or something right now, I feel like the pricing is just wrong. I'm rooting for this generation of GPUs to crash and burn so that they have to make the next gen either cheaper or a big enough leap to be worthy of the pricetag.
Used Radeon midrange or new RX 6600 is the way to go, I know a lot of people are biased against them but they're genuinely the only game in town for price to performance.
Get a used 3080
I havenâ€™t played a game on my PC since I got my Steam Deck. I picked up a 2060 Super when it came out and if prices donâ€™t come down to Earth within a couple more years then itâ€™s gonna be Steam deck only.
"I think they just stunted the GPU market for a few years, on purpose.

They colluded to sell to bitcoin miners and screw customers, then that fell apart.

So now they're in bonkers territory, trying to save themselves and not scare investors by doing it in a cutting prices and value way.. but just passing on all the disarray to the dumb rich whales and acting like the reason there's not a normal market is making gpus got 3x more expensive out of nowhere.

It is what it is. You'll be fine not upgrading for a while. Shame when there's a very small amount of competition though, this is the sort of bizarro shit that can happen."
Check your locla Facebook marketplace, found my 3070 for 400 months ago
"one of my friend jumped on the steam deck train because he couldn't afford a gpu at a half decent price, and the steam deck is basically a better PC than his current outdated one

and I just have to say, thank god for steam deck, sure not all games can be played on there, but the ones we play together can be ran perfectly"
I went with a ps5 and a series x. That was around 900 bucks for both. They also play 4k 60fps, and a lot of xbox games are crossplay, so I can still game with my PC buds. It even has discord. I'd love to upgrade my pc, but it's just too damn expensive now.
"At the end of the day, people will still pay for them and nvidia will keep laughing cause the gpu market belongs almost entirely to them.  

Get used to xx60 GPUs going for $600 and xx80 for $1200"
No idea how GPUs are the only component that has gone up in price over the last 30 years. Everything else is cheaper.
"You don't even need mid to 80 level cards to compete with a console, you don't need the current gen, you don't even need brand new. 

Please stop with these stupid whiny posts."
Definitely pushed me to go console
"Linus was talking about this the other day, and I agree with his take on it. Basically, it's your fault. Well, maybe not you specifically, but consumers who desperately bought GPUs at grossly inflated prices during the pandemic. Nvidia's primary responsibility as a company, as any organization, is to deliver the best ROI for its investors. As the dominant market leader, they're going to charge the maximum amount the market is willing to bare. And the market showed it was willing to pay 50% higher prices than in the past. Other companies like AMD were of course going to follow suit, because they also have to maximize ROI. 

The good news is that GPU sales are at a 20 year low. Anecdotal reports indicate that the 4080 isn't selling. Response to the 4080 12GB, now the 4070 Ti, has been tepid at best. Both directly due to being poor value propositions for consumers. I expect Nvidia will lower prices in the next year. And keep in mind you can finally get higher end Turing and Ampere cards for relatively cheap nowadays. You don't need the latest and greatest."
"A lot of this is demand driven though, and really Nvidia demand driven. The entry level gaming RTX 3050 has a $250 MSRP but has never retailed for that. Gamers are buying them like candy at $300-$350 and so thatâ€™s what theyâ€™re being sold for now. Meanwhile AMD actually has very compelling options a tier higher for nearly $100 less.

I think itâ€™s the RTX feature set thatâ€™s driving the disparity between brands on the lower-mid end. That, and maybe brand confidence. I think a lot of gamers trust Nvidia drivers more or feel like AMD is a â€œpoor manâ€™sâ€ alternative to the real thing, which is unfortunate."
GTX970 is my Ride-or-die.
There are a ton of grown ass adults whoâ€™ll gladly pay the price of a beater car for a GPU for some reason. Its a status thing I guess.
"The prices will be significantly lower sooner rather than later.  Nobody is selling any of these overpriced cards, nobody is making money.  That always causes business to lower prices.

The next 12 months will be an endless series of ""look at what a great deal you're getting compared to the original MSRP!"" advertising campaigns."
"PS3: $599 (2006)

PS4: $399 (2013)

PS5: $499 (2020)"
"There are so many cars sitting unused from dead mining companies its a matter of time till that flood hits the open market and all the prices crater. 

Kinda surprised it hasnt already."
I recently built a new PC once the z790 chipset dropped, but I'm hanging on to my 1080 for now. 7xxx and 4xxx gpu prices are over the moon. Inflation only explains part of it, greed explains the rest.
Honestly just buy a 30-series. Better prices with plenty of performance AND it sends the message that it's not okay to make these newer GPU's the price of a Craiglist car.
No as long as $200-300 gpu still exist pc gaming will still be fine. Yes you will only get 4050 for $300 but as long as it can run games well enough, its fine.
"I bought my 7900 XT new on launch day.

 Was I happy about the price? No.

Do I have an addiction to new stuff that interests me? What are you, my mom...?

In all seriousness, GPU prices are unreasonable.

If I didn't need to constantly keep my interests as a priority in my life, I wouldn't have overpaid (There's a larger/personal story behind this for me).

Really what we need is to boycott sales but I'm a prime example of some people out there. Because of something else going on in my life, I don't always care the value/cost.

I doubt console sales will skyrocket either because of their prices in general. I know a lot of people who wouldn't want to drop even $500 on a locked environment that will become outdated and not upgradable in what seems like a short amount of time for some people.

Just my 2 cents and personal experience for the ongoing war against greedy PC manufacturers."
"No? GPU prices are really good if you go AMD, or second hand for Nvidia.
For example for 250 dollars you can get an RX 6650XT that beats a 1080 ti by a good margin.
For 500-550 dollars you can get an RX 6800 XT that is double a 1080 ti performance for less money than it's MSRP, or you can pick up a 3080 second hand for 500 dollars.
If you only think about buying nvidia 40 series first hand than I guess you are out of luck, but those will go down as intended after 30 series cards are sold out."
It already made me want to switch. At this point Iâ€™m saving money to get a ps5 and then a series x down the road and retire my 3060 laptop for a normal laptop for work/college. No point in sticking in this hobby when all the hardware gets more expensive and even games are getting to the $70 point which was previously console exclusive
I have already shifted. I have a 1060, but it's majority just a strategy game machine now.
Absolutely will
Not really, there are still great performers at every price point from $200 and up, all faster than the console
"Prices are going up everywhere from massive inflation. Food prices are up over 40% and more, as well as gas prices. Prices of lumber and steel and literally everything else is up much more. Microchips and manufacturing are up even higher, especially for new nodes as TSMC hikes prices as materials they use are up over 70%.

Graphics card prices are up and will likely stay up for several years until inflation comes down. That's if it comes down. I've heard inflation will go up much much more, causing the collapse of the financial system from reckless spending during the pandemic as well as the war between the east and west superpowers.

If prices everywhere will continue to go up...these graphics cards will look cheap in a year or two."
"This is part of a pet peeve of mine. The game companies are missing a lot of opportunities due to short-sightedness.

Getting games is now a trivial matter. Instead of dealing with skull/crossbones stuff, now for something similar to USD 60 one can get a top of the line AAA Game, with a minimum of fuss.

But today's AAA games are basically unplayable on today's most used GPUs (onboard GPUs such as Intel UHD series). So, game companies are missing a lot of people who would gladly pay them money for their games, because such people cannot play their titles. If they really wanted to, they had to get a box with a discrete GPU, which is now expensive and (yes, even now) scarce.

The game companies should make the games playable in the lower end. For the higher end, if they want let them require a dual RTX 4080 cooled with liquid nitrogen sucking 1200W out of a power supply. But make games playable for the low-end millions, there's good money to be made there!"
Wtf is level of 80 pricing mean? Literally never heard this term before this post.
Console companies will be looking at the GPU market very closely along with the scalping scene. The next Gen consoles will be priced by a lot more.
"I bought a 4090, so Iâ€™m more likely to stop using my consoles for new games and focus on PC gaming.

Am I an outlier or am I the norm?
idk."
"There's lots of options - get a lower tier card, use DLSS, use a lower res monitor (i.e. stick with 1440p), or just get a card every other generation.  My 3070 is honestly fine for the games I play in 1440p.  Fingers crossed it'll last me another 2 years.

Not interested in paying Â£70 for games on PS5, that soon adds up."
I see a lot of people complaining about $600-$800 video cards, carrying around a ~$1,000 cell phone in their pocket, no problem.  I'm sure a good amount of folks base their gaming off price, but not me.  PC gaming brings enough value/joy to me that I will never switch to console, regardless of price.  While the entry point for PC gaming may be more expensive, the price of console games are fairly static and priced much higher than the amount I'm paying for games I'm picking up during Steam sales/key sites for cheap.  Seems like a wash to me in the end.
[deleted]
No because a new PS5 is still expensive too
"It made me switch to laptops

It also made me realize that i'm ok with using several years old hardware so from now on i will only upgrade when i really need it"
Consoles are great for playing fifa when friends are visiting, or you donâ€™t have the money for a PCâ€¦.otherwise PC gaming is just so much better.  have not turned on my PS4 in over 2 years (and even then was for the playing fifa thing) and not even considered the 5
It's all because of the crypto craze and having a huge surplus of 3000 series cards still.  The performance of the "4070 Ti" is more like a xx70 or xx60 Ti card depending on if playing 4k or 1440p.  It should have been a 4070 and priced at $599 at most, but there are too many 3000 series in inventory still which would crater their pricing.  In the past xx70 cards tend to have around the same performance of the top end Ti card of the prior gen (excluding the crappy non-super 2000 series that was only about DLSS + RT) and the 4070 Ti sort of meets this unless you are trying to push 4k gaming.
grabbed a 6650xt for $200. its fine, youre all just aiming too high
Consoles are sold at a loss as far as I know. Phil spencer said each xbox console has about $700 manufacturing costs. What if they increase the price as well? It may be seem viable to people who don't care playing games with m/kb or mods. It will suck for the rest of us who need those
"I would like to see that happen. Firstly, this sudden â€˜hey i could be a pc gamerâ€™ trend has done nothing good for my hobby of 25 years. 

Theyre all such suckers and seem to think all they need to do is stick together the most expensive hardware they can find. (Causing huge price jumps) 
They dont rwally want to get involved in software configuration (pretty much the entire other 80% of using homemade conputers to play games with) so they come here amd post one after another  â€˜i have a 13900 and a 4090, why are my franerate not in the three figures?!â€™ Post. Never even talking about their drivers, or game settings, oh no, why? Well because Linus doesnt talk about drivers he talks about hardware and maybe the firmware bios! (Which he is sponsored to do, dipshits!)
And then they take it all to our developers and blame them for not having the latest hardware features like DLSS when the fucking gameâ€™s engine is 6 years old. 

A xbox or playstation is 600$ and works right out of the box! It has exclusive games olus almost every game we play onPC except RTS games. You can play it woth your girlfriend and you can even trade game disks! Theres a VR headset, all solidstate and everything tested to work together!

I dont know what drove people away from consoles, (because ive never owned one) but i really think it suits alot of peopleâ€™s needs. Especially now today when its internet enabled and supports mods and like peripherals! Its perfect for you young upwardly mobile young folks who have small dorm rooms and etc! Buy yours today!"
Buddy, nobody is stopping you from picking up a lightly used GTX 1660 for $100 USD, ok? Relax.
"""80 class"" is not a thing.   Stop thinking it is.  That's just letting companies manipulate you.

Go and buy something you can afford that has value to you.  Don't be mad that products exist that aren't for you."
"My first computer 20 years ago was $3000, and that wasn't even mid range. put together a midrange computer in 2008 for $800. Built a midrange computer in 2015 for $800. Built another midrange computer for $800 in 2018. Started going more high end and built a computer for $1350 in 2020.

I went on pcpartpicker and built a computer that i thought was good enough for $800 for the hell of it the other day including monitor, keyboard and mouse. I can bring the price down to around $500 with swapping a few parts with stuff on the used market. 

I've had PCs all my gaming life and the price to build a PC is still about the same."
I dont understand how you can play a console nowadays, i had a PS3, PS4, Switch and sold them all within 2-3 months because i ran out of games that interest me.
For me the prices are very low! Bought a 3070 yesterday for 600 and sold my old 1080 ti for 300
[removed]
[removed]
Even with a decent job I can afford to upgrade from my 3070 to a 4080 but due to the prices being so high itâ€™s a serious turn off and Iâ€™ll just make the best of 4K with high settings. Not the end of the world Nvidia! Had you priced your card in the â‚¬700 range I would be buying them yearly.
"I think it will, especially with the plethora of cheap 4k TVs and consoles being able to run their ""fake"" 4k modes, which still look much better than 1080p.  Worst case you run the higher performance mode to get 60fps and it still looks better than 1080p or even 2k.  Looking at Steam's December [stats](https://store.steampowered.com/hwsurvey/Steam-Hardware-Software-Survey-Welcome-to-Steam?platform=pc) the vast majority are still on 1080p, with some on 2k, and 2.68% on 4k. I'll bet most of those 4kers (myself included) are unhappy unless they are running a 3080 at minimum (my 3070 struggles with a lot of 4k situations), especially if you add RT to the mix. 

On the flip side PC gamers seem to game a bit differently, usually in desktop settings on a monitor versus couch gaming, although I don't have anything to back that up, just my thoughts.  Having to use a technology invented in 1926 and retrofitted to gaming in 1974 to aim inside a game really grates on me, and with no mandated keyboard/mouse controls on consoles it's never going to get better.  The only exception/shining light is the Steam Deck with its touch controls, but then that's technically a PC and not a console. Due to that I think there is a subset of PC users like myself who will never go console, I'm just not sure how large that subset is."
My broke ass looking at brand new RX580's for $110, and 1660 Supers for $220-250.
"It will but it really shouldn't. A console is $500 plus the console manufacturers lock markets down. On the consoles you cannot go to some website like Fanatical and grab a preorder for $45. There are no resellers, even Humble, who carry stuff like Switch keys, can't charge what they want.

Plus you pay to play online, so people are better off buying a used GPU... or you know, be happy with 1080p/1440p gaming."
Cheap pop.
This year is not going to be a good year as things are just starting to find their footing business wise. Gasoline is still all over the place and cars are getting more processing power to deal with all the sensors. Combined with a push for electric and hybrid cars, there's a lot of silicon going around. Consoles are a good couch option, but they will never fully capture the flexibility and adaptability of a PC. More laptops and handheld adaptations are becoming more and more available and affordable, like the steam deck. Prebuilt computers with competitive pricing and prime selection on parts will also play a vital role. As PC gamers have in the past, we will find creative ways to survive. I know I'm sitting on 2 Rx 580s currently and I think I'm just going to have to stay with them until the market straightens out. Others may just have to go with used hardware and Nvidia and AMD may just have to deal with low sales if they think people will fork over cash that we need for living expenses. Once sales are down, retailers and manufacturers alike will lower prices to make sales and move inventory.
If it weren't for emulation and mods for games, I'd probably only have consoles. However, those two reasons are why I keep up with having a PC as my daily driver.
As I understand it, the prices right now are largely a function of a still limited manufacturing capability after some facility burned down a few years back.
My 2060 super is still fine. And I've been hearing bad things about the 4k series, plus their pricing is horrendous. I can wait, I am patient.
"Honestly it depends if there's really anything to play with modern graphics. There's really not a single game in the top 15 Steam games that is a ""next gen"" title. Games like Elden Ring and Warzone 2 are cross gen at best.

This also ignores the popularity of games like League of Legends and Fortnite. What's the point of even spending $600USD on a PS5 when you'll just be playing the same games.

I'd say a 1070 or even 1060 is all you need to play the games that are being played 99% of the time. Spending $1200USD on a modern graphics card doesn't give you much more than being able to play the same games at a higher resolution."
Gonna buy a second hand gpu because I refuse to pay the price of a car for a part for my pc ðŸ¤·â€â™‚ï¸
"I mean you can buy an Intel ARC GPU, those are pretty damn cheap and you CAN game on them.  

Gamers Nexus said they'd be doing another round of benchmarks with the updated drivers soon... i'd wait for that maybe before making a decision. Especially if you are in that budget / performance range where such a card would make sense."
Nah, I'd rather buy sencond-hand than a console. I'm not rebuying my games on another platform.
"Checking Newegg: 4080 are fully in stock, same for 4070Ti and the 7900XT. Clearly not much demand at current prices. Pandemic/Crypto demand is gone.

I don't think we'll have to wait long for price drops, and then Nvidia/AMD will panic and have to speed up the release schedule for lower end models. The PC market will see an adjustment in the first half of 2023 once the earning reports come out and show a massive drop in revenue for all manufacturers.

Don't be misled by the doom and gloom here, these companies need money, they're beholden to shareholders. If they don't sell GPUs, people will start selling their stock, same as they did in 2022. 

Nvidia stock in the last year: -46%. AMD stock: -50%. This is their last desperate attempt to squeeze money out of consumers, and the unsold GPUs show it's failing."
"I think itâ€™s already begun. In a sense. Some people are sitting on extremely old hardware now like 980 or 970s or they have 1060, 1070s and 1080s these cards are still great 1080p cards with tweaking of settings but really need to be refreshed soon by these owners if they havenâ€™t flat out died on them yet. The 1070 came out June of 2016. Thatâ€™ll be 7 years this summer. 7!

Many suggest upgrading your GPU Iâ€™d say at least once per 5-6 years so now these GPUs really need a replacement. But what can people with these mid range cards do? They could go Intel but the drivers are still buggy. If you are like me and have 4 hours in a week to play do you want to run into issues? No I donâ€™t. 

But is pc gaming doomed? I donâ€™t think so itâ€™s just gone from its better for the same price to itâ€™s better with a premium. I personally love pc gaming compared to console. Keyboard mouse in shooters top tier plus the number of games I have available to me that I canâ€™t even play on console. Games such as WoW and total war. In addition I can work on my pc so now it has two use cases. I can store all my families photos and videos on it with an expansion drive just for saving that. This doesnâ€™t even require a GPU. 

These prices for GPUs are ridiculous. They are getting away with it because there is a ton of money floating around. Can I afford the 4090? Sure. Will I pay it? F*** no. I still believe in value. I believe in limits. No matter how much I love my hobby thereâ€™s a limit where I go this is crazy and I will not indulge in it. 

My two friends have been priced out of pc gaming so thank god for cross play becoming more prevalent. But itâ€™d be nice for them to afford a rig so they could experience pcgaming at its finest"
So true, I won't pay more than 400-500 dollars on a gpu so I'll just hold out until then.
"If you just want to be able to play the majority of games at 1080P 60 fps, APUs with onboard graphics are good enough. Integrated graphics used to be a joke, but now they're becoming the entry level.

For 1440P high refresh rate gaming, the RX 6700 for $299.99 is a pretty good deal as well."
I have a 2080rtx and i wonder how long will i be able to keep it. I don't want to buy a new gpu anytime soon. Maybe in 2-3 years? I really don't know
Yes for some people, especially those who mostly play AAA, but this doesn't affect the majority of the consumers. You don't need a 4090 to have a very good gaming experience, a 3060 or 3070 are still very solid, and you can get the AMD alternatives cheaper still. And this is only for people building PCs, which we love to think is the norm, but really isn't. Didn't steam statistics show that like 70% of their users game on a laptop anyway?
How much do you need to spend for a GPU that gives you 60fps/1080p in modern AAA games.
RX 6750 XT and RTX 3070 are the best bang for the buck I think. They can be hard for $500 or less. They perform better than consoles too. People wanting RTX xx80 and RX x800/x900 series for gaming only are in minority.
Donâ€™t underestimate the power of the consumer. Both Nvidia and AMD are going to have a come to Jesus moment soon with their current pricing structure. Contrary to what you may see on Reddit, most people are not willing to spend anywhere close to $1000 on a GPU. Barring something crazy like a crypto bull run on the horizon, I just donâ€™t see this business model being profitable in the long run. I think once the dust settles and all the new cards have been released we will see price drops on most of these cards except the 4090.
">Seeing the price of new GPU even mid range going to level of 80 class pricing , do you think that will make people shift to console aa they cost less than a entry level GPU going forward ?

I think this is still mostly an issue with Nvidia graphics, that a lot of negative propaganda is still out about Radeon graphics, and that aside from DLSS and RT heavy calculation, that a jump is totally worthwhile if you are OK with learning a different approach to graphics driver processing. Been running a 6000 series card for almost 6 months, no serious compromise I can identify."
There hasn't been any new midrange GPUs announced or new gen consoles so i'm not sure what makes you think one is a better deal than the other.
"Where were you last year?  


PS6 / Next Gen Consoles are likely to be much higher in price also."
"100%.  
Not buying one until they return to normalcy. 
In the meantime, a 6800XT and a SteamDeck will hold me over quite nicely."
Nope, the GPU prices were insane ~~4~~4Q2020/1Q2021, now they're just very expensive. I paid much more for just Radeon 6800 than what now you would need to (same performance / next gen).
You can get a 2060 SUPER at a decent price looks like. That's what I'm running and haven't had an urge to upgrade yet.
"Consoles got like 30% price increased, or more even. 

So not likely."
"Depends on their current platform and what they want to play. Used market is also an option, however that depends on where you live. 

If you want to just play Fifa, some 1st/3rd person action/adventure/rpg game and some racing then console seems like reasonable choice."
That's why I bought a ps5, I will postpone upgrading my PC indefinitely.  The price of gpu is to ridiculous, and I have other bills to pay.
Well, I have been a PC gamer all my life and I recently I'm shifting towards a console. It just too expensive to own a PC, so once this one dies ... we will see.
Doesn't NVIDIA & AMD provide the video chips in console as well?
buy ps5
Nvidia wants you to buy RTX 3xxx that's why they even showed them in their slides during 4080 and 4070ti presentation.
"The problem with consoles is you can't really use them the same way as a PC. Meaning, when I come back home, I want to sit on my comfy desk and use a keyboard and mouse to browse the web, listen to music, maybe have CAD or something open to draw, commune on discord, watch youtube, chat on Facebook, and then alt-tab back into a performance intensive game.

The PC, especially medium-higher end build, is the only thing that offers this. It's like the swiss knife of tech.

If I had to pay a few grand extra to get a console and all its required extra hardware, then pay a few hundred to a thousand to have a decent non-gaming PC or laptop, I'm still spending more for it all combined, rather than upgrading my PC to remain relevant.

So in conclusion, the price hike of GPUs alone is not nearly enough of a serious reason to completely jump the bandwagon to console. That said, yes, the pricing on pretty much everything is abhorrent and like somebody said lower down, high-end gaming is now mostly for the wealthy. It used to be my main hobby, but now cars have taken that space so I had to downgrade to a laptop which I probably wont replace for quite a few years. It's sad, but necessary."
We either will see comeback with next gem (like 2XXX series overpriced vs 3XXX series, but without C19 and shortage fuckup) or they get even more expensive. In general it all depends on consumer. If people buy for those high prices (which we already shown companies that we will buy, check PCMR countless posts about - "look I bought new shiney 3060Ti for 700-800 USD" almost on daily basis) nothing stops companies to increase them more. I understand where it's comming in your case, my PC build back in 2017 cost as much as 4080 nowadays (it was Ryzen 1600 and gtx1070)
"> Seeing the price of new GPU even mid range going to level of 80 class pricing , do you think that will make people shift to console aa they cost less than a entry level GPU going forward ?

80 class GPU are now what, four times as fast as a console GPU all things included? 

Those are simply not comparable and your question is equal to somebody thinking people wouldn't by cars anymore because you seen an ad for a 100k luxury vehicle. 

You can get a noticeable above console performance GPU like the 3060ti for 400 Euro, cheaper than any console but the Series S. Yes, that is only one component but the overall price of a gaming PC with console level hardware or reasonably above isn't much higher as it always have been that early in a console generation."
Good God these posts every week. Just get a console and go to /r/ps5. PCs have never been a more affordable gaming solution than consoles.
"There is a fundamental issue with how you are ranking things. Please ignore prices for a bit, I will get back to them. The way the chips are ranked from Nvidia the chips ending in 0 are flagship, 2 are high end, 3 (used to be 4) are mid range, 4 (used to be 6) are low end, and 6 or higher (used to be 7 or higher) are graphics accelerators aka not for gaming. With the ad generation they added the 3 and bumped everything bellow it up one to confuse people. 

If we look at the 4k series they have made the 4080 on a mid range chip, that isn't bigger than the last few gens of mid range, put it on a $400 pcb with $200 worth of ram, and want $1200  for it.   There is no reason a mid range chip should be going on a board that expensive or should be 300  watts. We are also not seeing a $850 4070ti on a low end chip. 


This whole thing seems to be Nvidia price gouging on the silicone by overclocking the chips so the retail gaming cards cost 2-3x what they should and are labled as the wrong tier. If we look at laptops at ces this year we are seeing the low end ad104 chip (4070ti desktop) used in the 4080 mobile going into $1200 laptops. They are clearly not pricing those chips like the 4070ti so there is some huge disconnect between the level of stock overclock on desktop and what the chips are designed to do in their sweet spot. That is what is driving the prices way up and why evga left the gpu game."
It's similar to the situation where countries with poor (and getting poorer) foreign currency exchange rate. Most people can't afford PC gaming anymore and finally go to mobile phone gaming.
So don't buy it. I see this post every day, the only way they change is if people don't buy the overworked cards.  They clearly didn't care when everyone complained about the 4080 price and repeated it but worse with the 4070ti.
You don't have to buy latest gen you know.........
"It actually pisses me off how much GPU'S are right now. I've been trying to upgrade my 1660 to a 3070 for about year now. I refuse to pay even $600 dollars for a 3070 which is a 3 year old card. I'll pay $500 or less for a 30 series, anything above that is non negotiable. Imo $500 bucks is already expensive for a single component on a gaming PC. 

I am under the assumption since both NVIDIA and AMD have not released their mid ranged cards yet, that will be the moment to strike for a 30 series. My guess is prices for those cards will drop around summer of this year"
Don't worry nvida will keep the exact same price for the 5000 series or maybe only increase it slightly and suddenly $1200 for an 80 class GPU will be great value. From there these prices will be the new norm until the next round of price gauging. Gotta keep raising those profit margins.
what is shameful is that AMD entered the bandwagon of overpriced GPU just because Nvidia is overpricing.
I remember buying GeForce 2 MX400, I also remember buying GTX 285 for only 300â‚¬ and that was the whole chip without any cutdowns or whatsoever, would be equivalent today to a 4090, nvidia over the years been upping the price and I don't think they are going to throw it now. Sadly but reality check.
We will know for sure if sony reaches its crazy 30 million console target for 2023, which to compare, the msot a PS console has sold in a year is 20 million so 30 million is I think way out of reach normally, but....with these gpu prices, I can see many people who build a pc years ago, instead of updating it opting to buy a console in the meantime, if you want the ps exlcusives the ps5 is 400 dollars, if they just want to play newer games and dont care for sony games they can get a series s for less than 300 dollars.
"I myself just looked to upgrade. 

No fucking way man. No fucking way. 

I built a machine around my 1080 in early 2017. That card cost me $800 Canadian and it was the 8gb OC strix edition. The total cost of the pc was around $1400 Canadian. 

The prices of the 4080 right now for me are $1,900. 

Fuck that. My ps5 is good enough if these are the prices of just the damn card. The 4080 isnâ€™t even getting full 4k performance in some titles. Why the hell is it over $2k after taxes? 

Guess Iâ€™ll stick to playing new games on my ps5 until the pc market pulls itself together."
[removed]
I used to upgrade every 2 years to a 70 series untill the 420 pounds for a 1070 and that was the only one not at 500 uk pounds back then.  When I upgrade its because of a great battlefield game or and a new gta game.  Luckily for me gta 6 isn't anywhere and battlefield now sucks so bad beyond recognition.  I was never spending 40 series pricing which is why I bought a series x and ultimate game pass and a ps5.  L9oks like I won't be buying a new gpu any time soon. Yes I think the 40 series pricing and amd following suit is gone hurt the pc market and hurt it beyond repair as al9t of people won't bother coming back.
To think 10 years ago you could get a good GPU for under $200.
"nVidia and AMD to a lesser extent got addicted to cryptomining fad prices...

Thank the gods that Intel isn't a shit in their prices...

also tyvm nvidia for kicking evga out of the AIB market... hopefully they come back on board with intel

EDIT:

Hell even mobo prices are ass right now. They're at a MINIMUM double of what they SHOULD cost and AMD is trying to hide it with their truly ass lowend chipset for AM5...

e.g. I purchased a x670e PG lightning for $250 which is the lowest of the low x670e mobos, meanwhile a few years ago I purchased a x370 fatal1ty gaming pro for the SAME price.  Go ahead, compare the feature set.

PG Lightning is a $100 board at best and there is NOTHING on it to justify its 2.5X price point, look at all the corners cut v. x370 fatal1ty gaming pro at SAME price point ffs! And I don't mean they just cut a FEW corners, I mean that they threw the baby out,the bathwater out, the bathtub out, you threw yourself out etc.

EDIT2:

MEANWHILE the TRULY INCREDIBLY EXPENSIVE component, i.e. CPUs have NOT SKYROCKETED in pricing, go figure... as a matter of fact they kind of tried to raise price but that phailwhaled almost immediately...

EDIT3:

And I guess w/nVidia at least the problem is that they gave up on consumer only silicon and are fusing off pieces of their stupidly expensive stupidly large w/incredible defect rate monolithic chunks of silicon... sorry nvidia let your business customers carry the freight for your poor design decisions...

EDIT4:

I mean after all evga said fsck this shit, and slammed the doorknob on nvidia's ass..."
[deleted]
Can confirm and the Xbox Series S/X can also run emulators now. For the price of a very mediocre GPU you can run retro games + Gamepass games.
"> PC gaming cannot be sustained in its current form if a key component to a gaming PC can simply double in price over six years. That's not a stable basis for a platform.

I have no doubt Nvidia believed a few months ago that by 2028 they could charge $2000 for a xx80.

4070 ti's aren't even selling, overall gpu sales down 20%. If this trend continues, especially with the 4060 which will probably cost $500-$600 it will hopefully wake them the fuck up and we'll see 30-50% price drops on the 5000/8000 series gpus.

For me, I picked up a used 3070 and am sticking at 1080p for another year or two. Went with the 3070 so I can enjoy some basic ray tracing on some games. I'd of loved to go to the 4080 and if it was even $800 I probably would have considered it but fuck these prices. 4070 ti I wouldn't even consider unless it was $499 at most as it's a damn 60 series card."
"And it's not a proper 80 series card per previous generation's categorizations. 80 series have always been on their highest end chips - the 3080 is a cut down version of the 3090 chip. The 4080 is a second tier chip. It's a 4070. The 4070ti is a 4060 tier card that they tried to original market as a 4080. It's disgusting.

There is no actual 4080 (by historical standards) in this generation. At least not yet. Maybe they'll sell a card based on a cut down 4090 chip. Whatever they call that, that would be a proper 80 series card."
"Between 2016 and 2022 we know what happened, mining boom. It's been like, only 3 months since the gpu mining has gone out of the picture.

Nividia isn't stupid, but certainly addicted to sweet mining juice. Give them couple of months and we will see price drop whether Nvidia like it or not."
This is the wild thing. I remember preordering the 1080 FE, thinking it was expensive but worth it. But seeing the 4080 at $1200 price point is just mental disorder levels of insanity.
I'm currently looking at getting a new gaming PC with the release of Kerbal Space Program 2. GPU prices to have a good experience are ridiculous. I'll probably just get a console and give on PC gaming.
I'm sorry the 4080 isn't a comparison for the 1080.  The 4080 16 gig is what the 4070 would have been so 4090 at 1700 dollars is a better comparison to 599.  960, 1060, 2060 and the 3060 all have 196ish bit bus the 4070 has 196 bit bus.  They have moved the entire stack up while still charging extortionate.prices.  but let's remember today's 650 pound or dollars 4070 is likely a 4050 series card and that's just plain insulting.  After all going from a 70 series from one gen to next yields close to 20 and 30 percent performance I crease from the 70 of last gen to the 80 series neq gen.  Its nolonger even a gap. So we lost that percentage in performance or we are being sold the chapters cars in the stack.
"""Every new PC"" is not a 4K build.

If you want that power, you pay the premium. If you don't, you can build a whole rig, all-in, for high performance at 1440p (aka 2K) for what someone else spends on their GPU."
"The market determines the price of GPUs.

If a GPU would sell for $1000, pricing it less than that is charity. Pricing what the market supports isn't greed."
Can you connect a keyboard and mouse to consoles? Not that I don't like controllers but some games are beyond awkward with controllers
"> PC gaming cannot be sustained in its current form if a key component to a gaming PC can simply double in price over six years. That's not a stable basis for a platform.

All games can still be played on a GTX 970, which is a mid tier card from 2014."
Still, you don't really need 4080 to play most of the games. I am still enjoying my 1070ti.
The reason why Nvidia launches at those prices is because it *knows* people will complain *but* buy it anyway. They're obsessed.
I think a misperception exists about the current state of PC hardware costs. While the top end is clearly & obviously overpriced, there are significant deals to be had at the lower end. You can build a very capable 1080p-1440p system that can play all of the latest games at medium to medium-high settings for $600. The requirement for PC games has not increased very much in the past several years, so it's possible to build a capable gaming PC using new parts from the past 1-2 yrs.
">In 2016 the Nvidia 1080 launched at $599. In 2022 the 4080 launched at $1199.

RTX effectively doubled GPU prices. I'm not above buying used, but someone else might just say screw it and buy a console (or two) instead of a 4080 and still have a little money left for a 70 dollar game or two."
"I bought a 1080 in 2016. Still sitting on it. Sure I'm not 4k gaming, but at these GPU prices there is no chance I'll upgrade it for anytime soon.

The fact is that 7 years later, to get an equivalent GPU (3050), I'd have to shell out like 60% of what I paid for my 1080 back then."
"https://gpu.userbenchmark.com/Compare/Nvidia-RTX-4080-vs-Nvidia-GTX-1080/4138vs3603

 Based on this comparison, the effective speed of the 4080 is 184% over the 1080. Your complaining  because your getting almost 200% effective speed increase @ 100% increase in cost? Not sure how that isnâ€™t fair, sounds like a good deal to me. Now factor in the cost of materials being up compared to when the 1080 was manufacturedâ€¦

With your argument in mind, take a look at the current series 4070 ti that starts @ $850. This is 30% increase in price than that of the 1080 @ $599 and offers a 135% increase in effective speed over the 1080. Again, cost of materials are up now vs 2016:

https://www.statista.com/statistics/301564/us-silicon-price-by-type/

 I donâ€™t think people understand how advanced these cards have become over the last couple generations. Equally most donâ€™t appreciate the development cost that goes into GPUâ€™s nor understand the economical mechanics of pricing them. I appreciate peoples opinions on wanting costs to come down, but at the least make a valid reason as to why or how you expect that to happen."
"I will add that these are US prices, in the EU we pay so much more for a GPU. In the EU PC gaming is becoming a hobby that is solely affordable by wealthier people.

I know...get a better job etc, but we still need mid range PC's and mid range is becoming less and less affordable. Mid range is now the price that high end PC's used to be.

I have never hated consoles, but I would hate to see the PC landscape be lost, it has revolutionised gaming.

Who knows what is around the corner though! I have a nice PC, it is one of the luxuries that I afford myself, but I do not like the direction that it is going."
Ya this is super sad. I was able to work a summer to buy a new PC, now working all summer gets kids a new component. Especially considering salaries haven't increased much at all in the Last 15 years. The following generation is really getting screwed over by large corporations.
Intel will hopefully challenge the low-midrange market. With further driver updates which has already improved performance, weâ€™ll see an honest alternative.
"What about the RX480 por $200?

Ah man the good old times...."
Agree with what you're saying here, but even the 1000 series wasn't sensible pricing. Compared to now , yes but at the time it was still kinda gougy due to mining starting up. The 900 series was the last sensibly priced generation, I think a lot of people remember the Â£250 970 fondly and the competition between the 290x and 970 was good
"To compare - the PS4 was released in late 2013, and is still in use today, with titles still being released. 

In 2014 the top GPU available was the 780 ti, how is that faring in 2023?"
Very nicely put my friend.
1060 still working fine for me, lol.
"no. 

buy a 6750xt. 

the sad part is. though even it is a great bang for buck card... its still too expenive.

the flagship cards go for 2200-2400 bucks. thats insane. thats a complete high end pc system right there.

combine this with the fact that if you want to even uses this card to its potential, you need a top tier cpu, ram and power supply as well. 

pricing is totally nuts."
This is a good take, sadly.
I bought a second hand 4 year old 3GB GPU 2.5 years ago. I can play every game on nearly maximum graphics without issue. If I have them cranked to max there is some frame rate drop so I dial it down and itâ€™s fine. I paid $150 for the card. I wonâ€™t play these price games with these companies, Iâ€™ll buy old and used or Iâ€™ll rob a computer store, but Iâ€™m not paying these prices.
"> but within a year you'd be able to build a PC for the same or less that would compete

That's never been true if you look at the lifespan of a console. Even last gen which had the weakest console hardware relatively speaking. A PC built for $400-500 during that cycle probably wouldn't even still be working up to 2020 for all the corners you'd have to cut let alone running later multi-plats in a playable manner.

These GPUs are overpriced as hell, and will def push people out. But those ""competitive potato builds"" of past years are propped up with lies, cut corners, oversights, and funky math. Some people include like a decade worth of PS/XBOX subs at full MSRP to try to favor PC there even. 

>On it's year of release, the GTX 1060 with 6gb of ram was $299.

MSRP was adjusted to $250 before it even launched. Only the FE was $299."
"I agree with all of this, but on the other hand I think that graphics are at a point now that you really don't need a new card to run the newest games well. 

The only major evolution in graphics this gen is about Ray Tracing. If you can live without that (which I'm sure the majority of people can workout even noticing) then you will be totally fine buying a 3-4 year old card for your build.

I just bought a 6900xt (which was still the most expensive card I've ever bought) but I'm sure it will satisfy my every need for the next 5 years, as long as I don't want to push RT."
I get the new argument like linus made the other day but what I'm missing is that I thought the ps5 or new xbox has an equivalent of a 2070 and nothing crazy cpu wise so why are we comparing them to new gpu lineup prices
The worse it gets the more I feel like my investment into GeForce Now is justified. It prolongs the lifespan of my local hardware and is giving me at least a chance to play some newer games, and now I'm getting (roughly) 4080 performance for $18 per month once they upgrade it. I'm not the biggest fan of being tied to the internet to use it, but I'd have to save up that $18 for almost 6 years just to get to the MSRP of a 4080.
[removed]
Why second hand? There's tons of older stock that is still *new*. 1080p/120hz is cheaper than ever and will look and feel a lot better than idiotbox fake upscaled tiled blurred controller inputted so level design can't be complicated 4k vaseline smear.
"> The best value per dollar is the $1600 monster, and the lower tiers don't even reflect the drop in performance in their pricing.

This is completely incorrect. 

The 4070 Ti & 4080 are better price/performance  

GPU | Resolution | Price (US$) | avg FPS | $/FPS
--|--|--|--|--
4090 | 1080p | 1600 | 279 | 5.73
4080 | 1080p | 1200 | 251 | 4.78
4070 Ti | 1080p | 800 | 221 | 3.62
4090 | 4K | 1600 | 159 | 10.06
4080 | 4K | 1200 | 124 | 9.68
4070 Ti | 4K | 800 | 99 | 8.08

[source for avg FPS: TechPowerUp review](https://www.techpowerup.com/review/pny-geforce-rtx-4070-ti-oc/33.html)

This isn't even getting into how the 4090 **[has spiked to above $2000.](https://pcpartpicker.com/products/video-card/#c=539&xcx=0&sort=price&page=1)**"
"Iâ€™ve always been interested in more powerful hardware, installing a 1080 in the first iteration of my rig before starting university when money was tight.  
Now that Iâ€™m in a job that lets me live quite comfortably I took the plunge and upgraded to a 3080.  
The 3080 cost me over a grand. While it hurt, I could afford it and my 1080 was starting to suffer in some titles due to the shit Iâ€™d thrown at it over the years.  
While I spent so much on a GPU and could on the next iteration of cards again, the only reason I was willing to this time was due to market conditions with mining (I didnâ€™t buy from a scalper however let it be noted) making them scarce.

Now itâ€™s Nvidia artificially inflating the prices? Iâ€™d soon rather shit in my hands and clap than validate the artificial price hike theyâ€™re attempting.  
The only way PC gaming thrives is if itâ€™s accessible to all, at all price ranges.  
Companies wonâ€™t see a point in developing native versions if only a small proportion of people can access those titles."
"Hereâ€™s an $800 4070ti in stock (which weâ€™d never see) 

Check this out on @Newegg: MSI Ventus GeForce RTX 4070 Ti 12GB GDDR6X PCI Express 4.0 Video Card RTX 4070 Ti VENTUS 3X 12G https://www.newegg.com/msi-geforce-rtx-4070-ti-rtx-4070-ti-ventus-3x-12g/p/N82E16814137773?Item=N82E16814137773&Source=socialshare&cm_mmc=snc-social-_-sr-_-14-137-773-_-01072023

A 3060 for $350, under the inflation price you mention for the 1060. 

ZOTAC Gaming GeForce RTX 3060 Twin Edge OC 12GB GDDR6 192-bit 15 Gbps PCIE 4.0 Gaming Graphics Card, IceStorm 2.0 Cooling, Active Fan Control, Freeze Fan Stop ZT-A30600H-10M https://a.co/d/dWSVUM9

And as an added bonus, a great 1440p card for $390

XFX Speedster SWFT309 AMD Radeon RX 6700 XT CORE Gaming Graphics Card with 12GB GDDR6 HDMI 3xDP, AMD RDNA 2 RX-67XTYJFDV https://a.co/d/4uxdruh

Would I tell people to buy a 4 year old used GPU..? Noâ€¦there are plenty of other good cards to buy besides the 4000 series. 

The 4080 and 4070ti prices are too high but people are also being a little too dramatic. They arenâ€™t the only cards on the market."
I stuck with my 970 for almost 10 years, i'll hopefully stick with this 3060TI for another 10 years.
">what the hell are we going to possibly tell people wanting to start into PC Gaming? Go buy a 4 year old secondhand GPU?

I mean, yeah. The RTX 2000 series provides decent performance compared to consoles, especially when paired with a recent CPU. If you're aiming for last-gen GPUs of equivalent performance, you can still get awesome deals without buying second-hand. While it's underwhelming compared to what we're used to, and current-gen GPU pricing is completely indefensible, there are tons of good value options for newcomers.

Of course, it's more satisfying to shoot for 2x performance or more compared to consoles. Why pay around 700 USD just to match a PS5?"
"The slump will force them to develop for more than hardcore hobbyists, and to make their cards affordable.

Affordable will most likely be a major selling point as the recession hits."
"> Can you imagine Nvidia launching the 4060 at $370 or $399? I think it's more likely going to roll out at $499 at least given where they're positioning the 4070ti.

Sure, because the 3060ti was $400 and many of them are around MSRP at the moment.

> I think it will push some out.

I completely disagree. I think the issue here is you guys are too focused on the best and not what delivers good performance. For the VAST majority of PC gamers there's absolutely no reason to even go above the 2xxx series, something which is reflected in steam hardware surveys.

Yeah sure, we don't get shiny settings like RTX (at playable framerates) or native 4k, but why am I buying anything above the xx60ti lines to play games? At some point there's a huge amount of diminishing returns and I have to believe people are just doing it because they enjoy taking screenshots. Almost no games require that power to run at 1440 < and with DLSS 4k is achievable without needing them either. Unless someone wants to brag they can do native 4k at 60fps or whatever then I guess that's cool, but for the vast majority of gamers I don't think they care that much.

GPU pricing isn't an issue with the mid ranged cards since they are at MSRP (and are reasonable) and most games aren't even pushing those, so why bother? I guess enthusiasts are pissed but since I've always been a mid range andy I guess I'm not as annoyed as others."
"What you will end up seeing is that Nvidia/AMD just isnt going to sell many cards at that higher price point, gamers will end up forgoing display upgrades because their GPU cant drive the higher resolution, and they will still spend $2-$400 on their cards, those cards will just be lower in the stack. Either that, or both Nvidia/AMD begin a price war with eachother but I find that unlikely, as I doubt AMD is really interested in stealing marketshare from Nvidia. Their GPU division plays second fiddle to their CPUs and Intel is a much bigger threat to their survival as datacenters are a big part of their business growth.

Does this make consoles more attractive? Yes, till you can put together a pc that outperforms them for a similar price. This usually takes a year or two after a console launches but this GPU price bubble is pushing that out."
This post rules!  Wish I could give it a 1000 upvotes.  This to me is also what PC gaming "was" all about.  Not spending 1000's of dollars, but doing it yourself and getting something good at the end.  Tinkering and upgrading here and there.  The price of entrance for DIY is getting out of reach.
If I remember rigth the 1060 6gb was 240 dollars at launch or am I missing something?
Nvidia is not stopping anything, if people keep buying them. They are keeping the price where they are at because there is a demand for it, last week there was like 19 4090 in my local microcenter. That thing sold out in a couple of hours
People need to stop buying 1k+ subpair GPUs. Anyone who buys a overpriced GPU is part of the issue here. Once AMD and Nvidia bottom starts to bleed they'll come down real quick.
[deleted]
I'd be more up for that if I wasnt essentially required to subscribe to my console and $70 game launches.
This is pretty much where I'm at. My pc is pretty outdated (i5-7600k, 16g ddr4 2300, 1070) so for me, a complete rebuild would likely end up around $2000 to get a capable setup for my wants in a rig. An xbox series x is $499, capable of 4k, and familiar enough on the controls that my whole family can easily use it.
Consoles are sold at loss. It's like an incentive to buy them. You pay it all back by purchasing subscription and software.
An RX 6650 XT is 250 dollars in the US and that beats a PS5, and I'm guessing your cpu is up to task so why spend the extra and make your pc a waste?
with PC I can play my free games on epic
"I did basically the same a year or 2 ago and kept my 970. I'm currently thinking about getting a 3060 Ti, if they drop below 450 EUR. That's an ok price for vastly improved performance. The bigger cards don't even fit in my PC case anymore (266cm max length) and I'd need a new PSU as well, soâ€¦ seems like the most sensible upgrade still, since the 4060 is only supposed to release in summer and who knows at what unholy price.

In the meantime, I'm cleaning out my backlog, also through my Steam Deck."
He's not alone. Lisa Su is right here beside him.
What if his AI has calculated that and thinks PC gaming will live?
Unfortunately most people won't buy AMD, it's a sad thing but they only want AMD to be good to reduce price of Nvidia cards
I bought a 6750xt for 450â‚¬ and it has been performing amazingly for 1440p gaming so far
"Define the low-end market. Because I feel like that perfectly describes where I'm at.

I turned an office PC into a 1440p gaming build, spending $350 between  the GPU and SSD. Reusing the rest of the components saved me about $200. So we'll call that a $550 build. Linus Tech Tips could probably have shaved that down further.

And at Christmas time, I was tempted by an AMD rig on sale at Costco for $700 which would be a pretty big step up in performance. And only held off because my current system doesn't complain no matter what I throw at it.

Stepping up to 4K is expensive, and paying for extra frames at this resolution can be extremely expensive. That doesn't trickle down to slash piss all over the low-end market."
Try Brazil. The 4090 costs around R$16.000 while the minimum wage is R$1.320
Their cigarettes and console games are super overpriced, as an American who visited there once. No surprise they're getting hit with massive hardware prices.
I think I'm going to throw my 950 out the window and then jump out myself.
"I have a hand-me-down 1060 that i just cleaned and repasted after using it for years with my old rig. 

At 1080p it's solid. I want to bump up, and even at higher resolutions I can maintain 30fps in things like RDR2 if I turn enough quality settings down. But I don't want to have to push the card like that, and prefer it only being limited my my TV's 60Hz refresh rate.

Someday I'll upgrade, but prices will have to really come down for me to bother (especially with Nvidia's pricing)."
If prices keep up like this they just might find out their game does poorly because no-one can play it at reasonable settings...so it gets passed up...
"> Gamestunt explained it best but the fact people will still buy it, will have for consequence to put the lower tier buyer out of the market.

The people that buy it are a complete minority. If people were all that willing capable of dropping even 700+ in prior hardware cycles things like Steam Hardware survey would skew much higher than they do. The bulk of the market is on old as hell hardware or budget hardware. High priced or high end cards only ever have a couple percent of the gaming market. 

Unless all those other people flip around and suddenly wanna pay 4 figures in complete defiance of historical data this shit isn't going to sell."
Will still buy it? 4080 stock is hardly moving and I couldn't be happier to see that, fuck those greedy pig, I hope some scalpers burn with them as well.
"Honestly it was a natural conclusion of the r/pcmasterrace mindset ,where buying the most ludicrous/overkill set-up was praised and everything else gets brushed aside.

Well they can go ahead and do just that,any reasonable person will go with a current console instead and get a very good experience."
That setup is much better than a ps5... To match a ps5 you'll only need a zen2 and a 6600xt. Then realize that you need to pay money to play online on PS5 and the game prices are more expensive there and you can easily build an equivalent pc for around $700.
Consoles are very much a "you won't own anything and be happy" - deal though... Damned if you do, damned if you don't ðŸ™
I've yet to find a game that doesn't run on my deck. This thing is incredible.
GTX/RTX 980/1080/2080 etc.
Gaming PC power comsumption is also way much bigger when compared to consoles. You can lower your games to 60fps and play 1080p and save some money but still PS5 or Series X eats less power. And atm in Europe, electricity prices are high af.
"It was never argument

PC was always more expensive upfront"
Thought I was going to upgrade to a 4090 and 7950x3d this year but as japan opened up I'm saving my money and finally taking a vacation
"true for me - upgraded my 980ti to a used 2080.

this is just on the brink of bottlenecking my i5 4460 but ill do a whole upgrade in a couple of years time"
"AMD decided to go the Nvidia pricing route. So now you've got lackluster AMD cards for Â£1200+ thats propping up and justifying Nvidias laughable prices.

If AMD said fuck it, 7900XTX for Â£800, Nvidia would almost have no choice but to drop the price of their 40 series cards. But no, they're both greedy cunts and think they can still cash in."
AMD should dominate the mid range with the 6000 series but people are dumb and only look at Nvidia and end up paying more for less
"A ps5 is \~550 dollars.

It has an utterly zooming SSD, runs most games upscaled 4k at 60fps.

That price includes a peripherals you'll ever need in the form of 1 controller."
"It depends on what youâ€™re using your computer for and want you want it to do. 

When I had a 24â€ 4k display, my 1660 ti was fine, right I had to turn down settings on some games, especially modded Skyrim as your can really load the GPU with various graphics mods.

When I upgraded to a 49â€ display and wanted all the pretty graphics mods at 5120x1440 resolution and 60 fps, even a 3080 10Gb wasnâ€™t enough! 

So your requirements very relative to what you want to do with your PC. I use apps that are designed around CUDA cores, so AMD is out, for example. 

Now , if I go back to 1080p gaming and donâ€™t install graphic mods, I could probably put the 1660 ti card back in and be fine."
(Deck not available in the majority of the world.)
How is emulation of MAME and other consoles on the deck these days? I have dozens of GB's of roms lying around, might as well take the step if it comes to it.
Depends what you do for work i suppose.  Some people will be needed amd paid well.
No but if current prices are the new normal we might be in trouble long term.
Wife just got me a series x for xmas.  Its shit compared to pc.  And I use it on a 65in lg oled c2.
You can always enter in a GPU diet, I heard the 4080 is gluten free
That 4080 has 48 tflops. The ps5 has 10 tflops. It's has almost 5 times more graphics power. Your comparing a basic entry level console to a luxury video card that can output consistent 120fps for high refresh rate monitors.
May be you'll, but you're probably wrong, plennty of 4080 around means people aren't buying that shit pricing.
"Using my local Micro Center as a reference, so YMMV, 4080â€™s have been in stock more often than 4090â€™s, but the are fiber by the end to of the day. 

Interestingly, 4070 tiâ€™s have been in stock since launch. So, at least in my local area, people are snatching up 4080s & 4090s , but leaving 4070 tiâ€™s on the shelf."
"There is real competition in the CPU market. The GPU market is completely dominated by one brand.

Remember before Ryzen when Intel was your only real choice for CPU? 4 core CPUs with ~5% performance bump each gen, for 6 straight years. What was an i7 in 2017 became the i3 in 2019 because of Ryzen's pressure. We need a shakeup in the GPU market like we had with CPUs."
[deleted]
">consumers who desperately bought GPUs at grossly inflated prices during the pandemic

>And the market showed it was willing to pay 50% higher prices than in the past.

Let's not forget that many people were flush with extra cash during that time from the government stimuli, making paying extra for a new GPU, when faced with the prospect of staying inside for the foreseeable future, seem like a good deal. Nvidia clearly didn't consider that the outdoors still exist when pricing the latest generation."
"Turing is coming down in price.  I just searched Ebay and 2080 Ti's can be had in the 200-400 dollar range.  2070 Super 150-250.  Ampere is still overpriced at say 70 Ti and up.  At that point you are going to pay around 700 dollars for a 3080 Ti, maybe a miner GPU?  Who knows.  

I agree that you don't need the latest and greatest, but prices are out of control.  I would cede that the 2080 Ti launched back in 2019 for a ridiculous price of, 1300 US dollars I think?  But a 2070 Super, when new, was only 499 and now this 4070 Ti is at least 800 dollars or more.  Just because it does more doesn't make it good value in my opinion.  A perfect storm of shit destroyed the market for normal folks, and will it ever normalize and lower?  I can't say.  But if people keep buying at overpriced levels, it will never go down."
I mean the driver thing is at least somewhat justified as AMDs drivers for awhile were a dumpster fire, sometimes causing more issues than they fixed. Nvidia you could generally be confident after the new driver had been out for a few days if people didnt catch any problems where as AMD had patches of "DO NOT UPDATE" since it just hurt performance.
Not just the drivers, but also the hardware. If you do any work with machine learning, the CUDA cores are very useful.
I literally just found an RTX 3050 right now for $250 with no effort. It was the first result in Google.
yuuup, not really having any issues either. Not running at a crazy resolution or anything but looks fine, plays fine.
Buying shit to get that rush of excitement and dopamine is unironically a major cope for a lot of people.
"literally no one but ""grown ass adults"" can afford what are top of the line tech meant for enthusiasts. 

To anyone who doesnt play games, spending more than $300 on a windows machine is batshit crazy."
A ton of grown ass adults actually have good careers that pay well cause they decided serving coffee and working retail is shit.  I bought a 21 f150 a 21 escape for thr wife, a 21 camper last year not for status.  But because I could.  I spent over 4k on xmas for my kids and wife.  Got myself a new oled monitor and 13700k.  Have a 3080 but will wait for the 5080 to upgrade my gpu.  Just because ppl buy nice things doesnt mean its a status thing.  Its cause they decided to make something of themselves and have the financial means.
The 30-series prices are still inflated 2 years after launch.
Unfortunately, this is exactly what Nvidia wants people to do and is a huge factor in their current pricing strategy. If someone is going to go this route, I would highly recommend buying used instead of helping Nvidia clear their excess inventory.
Better idea, buy amd. Their 6000 series is an insane value relatively speaking.
This sounds super adorbs and all, but you're still putting money in nvidia bank account. They want you to buy all the cards don't matter what number is on the label.
1060 is still capable on low settings, especially with fsr on.
I canâ€™t see that since theyâ€™re targeting a different consumer.
the [slient] norm. I'm surprised so many ppl own a 4090 on here lol.
"Well even if they are all carrying around 1000 dollars phones (they aren't). Phones have a lot more utility than a single piece of gaming hardware.

Maps, phone calls, texting, filming, photograpy, writing, gaming, editing (photo/music), emails, web browsing, shopping,  watching video content, listening to music, smart device control all on a small, portable handhold device with good UI.

By all means criticize people overspending on luxury items, i just don't think phones are as good of a target as you think they are."
Your numbers are wrong, rx 480 was $200-250, 1060 3gb was $200-230
You get so much less price/performance in laptops over desktops though. If you need a laptop it's one thing, but you're implying that gaming on a laptop is more price efficient?
"Only $200??????
HOW, WHERE?!?!?!?!?!?!
Here in EU its like 380â‚¬ ....
FML"
"A last generation 70 model for six hundred seems low to you?

The 1080s were $600."
What?
Learn to read.
Dude WTF? Do you feel like a big boy saying shit like that? Just because he isn't a native speaker, you can still understand what he said. Don't be an ass!
Don't be so rude. A user name like PrashanthDoshi doesn't sound like a native speaker, probably Indian/Pakistani/Asian. There was nothing in that post I didn't understand, and you forgot to end your sentence with a period/full stop.
"Especially as more and more TVs start becoming not just 4K, but 4k120.

Hell, top end TVs like the LG C-series and Samsung TVs also have gsync/vrr support...making them nice as hell for someone who doesn't want to sit in front of a monitor for every game.

I occasionally hook my 2080 super up to my C1 for RPGs/other single player games, but usually play on a 1440p monitor.  2080 is okay at best for 4k (dlss helps, but the card won't push above 4k60 with HDR on a TV due to the hdmi spec anyway). 

 The new cards are awful value, but I do laugh at those that are like ""my 970 is still usable, I can't see why anyone would want to upgrade"".

Like...thats nice that your 970 is probably doing well enough on a 21"" 1080p60 monitor.  If that works for you, and all you wany....thats awesome.  But don't pretend there isn't a benefit to upgrading (someone actually told me here there is no visual benefit to playing games at 4k let alone high refresh rates)."
Nobody has to spend $1000+ on a GPU the same way that nobody has to spend 100k+ on a sports car. If you want the performance you have to pay the money for the best of the best.
AMD provides the APU for every console, but the switch. The switch is powered by NVIDIA. These consoles are basically sold at a loss because they can then lock you into their marketplace of $70 games and it probably gives Sony some TV sales.
I thought the prices went up because Cryptobros were buying up all the cards to mine bitcoin.
"I'm not even bothering with PC gaming anymore, I still have my 5700xt, but as games get more demanding, it's going obsolete like a free falling safe. My GPU comes up on Steam's recommended section whenever I search a game, that's a sign your PC is old. I have never liked Nvidia as a company, however their destruction of the used GPU market is their fault, they love scalpers putting their product out there and artificially raising prices. 

I still use my PC for older games, no issues emualting whatever I want. I'm currently doing a ton of mods of the Mass Effect trilogy and having fun, but any newer AAA game is either poorly optimized or I need an overpriced GPU to play it in 4k. AMD is always behind the curve on GPUs, how they toppled Intel but can't compete with Nvidia confuses me."
"The thing is that right now we can't know if this is a blip or if Nvidia have decided that it is perfectly fine for a graphics card to cost as much as the rest of the PC.

It's not enough at this point for prices to remain where they are. They need to come down. If they don't, of if they go up even more? That next console generation is going to be greeted with a lot more interest by PC gamers."
My 2080ti died after 3 years... I am a PS5 gamer now.
"You might consider running Linux.... it's lighter weight, doesn't really run ray tracing (from my impressions from some threads here), and your hardware might run better as it's a pretty fast experience operating in big picture mode. Also, depending if you go PS4/Xbone, you're still stuck with mechanical drives as opposed to SSD. My PC was built in 2015 and aside from the graphics package, which I updated in July, things still work great. 

Personally, even tho Xbox/ Playstation controllers are the most accessible, I'm kind of annoyed how everyone is supposed to be accustomed to expecting joystick drift etc. I've been using Steam Controller as my primary, another reason I don't really want to change anything."
"> Some people in this sub has said this is all hoopla. 

Its not hoopla but a lot of its overstated and also geography dependent. With your current setup you could plug and play a 3060 and be largely fine for well under the cost of a PS5/XboxX in certain areas. I can get a 3060 for $300 in my area which is cheaper than the primary consoles. The Xbox S is a closer competition and the rest of your comment makes it sound like you'd prefer that form factor so it seems like a good choice over upgrading."
I think it's a little bit of both. Yeah the prices are absolutely too high, but keep in mind to have the "equivalent" of a console you can be running a fairly old card, which costs way less. So yes it's going to cost you an arm and a leg to stay ultra current  but you can get something that will run everything reasonably, just like consoles do, for a fraction of the price
I hate controllers so much tho ;-; and really wanna play satisfactory
"The Xbox Series S is such a beast of an emulator unit.  You can play Atari, NES, SNES, Playstation 1, Playstation 2, Gamecube, and a ton of other systems games on that console just by following simple instructions.

Never mind, the fact as you said you have Game Pass.  It is a hell of a deal, add to the fact that you can also stream the games on that system and the server blades that Microsoft uses for their streaming are Series X."
"Where consoles offer a very obvious benefit is if you are building a PC *from scratch* and you are okay with console parity.^1 This situation sucks since you are going be paying a significant premium for what is essentially an Xbox/PS5. So you either have to be committed to PC exclusives or your existing PC library. 

But too often this discussion reaches out farther than it has legs for.

- Buying a console because the enthusiast card is high priced. If you are looking at a 4070TI, which is a whole generation ahead of the consoles, I don't see why you would go buy a Xbox instead? You likely wouldn't settle for a 3060 instead of a 4070TI but the consoles are in effect 3060 like in their power. 
 
- If you have an existing PC with an older video card then its broadly cheaper to just upgrade. 3060/6650/A750 all give you console parity for $200ish dollars below console cost. Now obviously the price delta will vary based on your previous hardware and everyone will have their own path. But broadly speaking you can get console parity below console cost *if you have an existing PC*.


Again where the consoles offer a clear advantage is if you are building an entire PC from scratch since the video card will end up being an increasingly large cost and drive up overall cost for just parity. But there is a little bit of the sky is falling going on in these discussions. 

^1 Essentially a PC that can keep up with the console graphics march. For this generation the current console parity card is the 3060/2070. The 4050 *might* be the console parity card but we'll have to see when that releases."
My 1070 amp extreme can still run alot of great games so can game pass.  Hogwarts os great, cyberpunk is great and I didn't need to mess around with settings but on PC I would have gladly done that, just not paying extortion for gpu. I'm certainly not paying 650 for a 0%performance I crease fro. A 3070 to a 4080 in terms of historical.
"I can imagine the Nvidia board meeting.

Boss: guys, our graphics cards aren't selling, suggestions what to do.

Board member #1: We introduce new products with the exact same name but with different specs and different prices.

Board member #2: We introduce a new ultra premium extreme edition GPU that can render Pixar movies in real-time.

Board member #3: We lower the price of our entire range of GPUs to reasonable levels.

Boss looks angrily at board member #3 and throws him out of the window."
"We've seen price cuts before and I suspect we will see them again. I can afford a new video card but I'm not going to pay the current prices. 

Something to remember is that Nvidia/AMD made projections based off of the pandemic pricing cycle. The drivers of demand are dead but that doesn't mean they are going to roll over immediately. They are going to try to meet those targets and only after failing will they recalibrate."
Gpu sales down by 20 percent doesn't seem at all bad, it needs to be down by 90 percent.  Down by 20 percent but pricing up 200 to 300% means nvidia is making tones more money than it ever has with these gpus and that's the fing scarey thing about it.  Every one needs to stop buying nvidia gpus and amd gpus.  Sad thin is if battlefield 2141 was any good my moral ground would.turn to mush and I'd ha e bought a 12 to 1400 uknpound nvidia 4080 and hated them for it
This is not true. The GTX 1080 from the above example was based on the GP104 chip where as the 1080 Ti chip was GP102. Same deal with the 2080 and 2080 Ti. It's the 3000-series that is the odd one out.
"Eh its been half a year mate. Even then crypto burst in decemeber 2021 same time as stock market. I have a feeling that as long as new ampere stock keeps being listed. Nvidia won't change Lovelace prices.


Check back by summer 2023. We should know by then the market is going to be like for the next 2 years."
That's why we are all so mad it's atrocious.
"We're not talking about paying a premium for higher grade components, everybody in the world understands how that works. We're talking about paying an arbitrary surcharge on every graphics card because ""Muh Yacht.""

Every generation of graphics cards improves over those before. Same for motherboards, CPUs, monitors, whatever. None of the people making those have decided arbitrarily that *this particular* incremental improvement warrants doubling the price over six years.

And frankly, if other component manufacturers went the same way, we're done out here. Prices are tied to relative performance of the current generation, not historical comparisons, otherwise it would break down scarily fast."
While this may sound like wisdom to some, it's just plain wrong.  As is generally quite obvious, the *seller* determines the price of any good that is sold.  The only the thing the buyers do (collectively) is determine how many are sold at that price.
Probably, but I'm not sure the games would be as adaptable as they have to be on PC.
You get that everybody using cards from 2014 isn't PC gaming in its current form right?
You don't *need* plenty of things, does that mean their price should be doubled?
Sales are down 20%, which considering the Crypto sector went up in a puff of logic suggests you're not wrong.
"You could get a 1440p PC for $700 in 2016 and it wouldn't be that much slower.

[https://www.youtube.com/watch?v=3I2xJLpVAk8](https://www.youtube.com/watch?v=3I2xJLpVAk8)

&#x200B;

The 1070ti actually holds its ground vs the 3060 was fairly similarly priced. The 3060ti was never at MSRP and the 3070ti was often even a little below MSRP. I don't care what tier of GPU something was in or the supposed MSRP. The reality is $375-425 was pretty common for a 3060. And after 5 or so years you hardly get an upgrade in games where pascals architecture doesn't hold up so well like Resident evil there's a good 30-35% advantage for the 3060 but for a lot of games, they're within 14-20% of each other even in recent games like sniper elite 5 which surprised me.

A 70-class card these days is more than a 1080ti was and a 1080ti was probably closer to a full 102 die than a 4070 is/will be."
"[https://www.youtube.com/watch?v=3I2xJLpVAk8](https://www.youtube.com/watch?v=3I2xJLpVAk8)

&#x200B;

probably closer to a 3060 tbh"
"Yes, and the 1080ti was 70% faster than the 980ti and it didn't cost 70% more... Do you not understand technology progresses. 

&#x200B;

I think you don't understand the costs of making these GPUs, for example, an rx 480/580 cost AMD in 2016 about $70 not including R&D to make. They managed a sell cut-down GPU (rx 470) for under $150 and the full die for $190-230, 6 years later we got the 6500xt with a GPU half the size not even a 16x pcie slot a bunch of hardware encoders etc disabled or not included on the die and it was only about 8% faster and cost $200 in 6 years you've gotten at a push a 10% performance uplift."
"I'm actually in the UK and know the pricing problem only too well, but sometimes particularly for historical data, or just general discussion it's so much easier to do USD.

I'm glad I bought a 1080ti when I did, my original plan was to use it for 5 years in the hopes something in the mid range would more than outperform it, just didn't expect this hell-scape we're in.

I passed 5 years in Nov 2022, and I don't see me upgrading soon."
">	I will add that these are US prices, in the EU we pay so much more for a GPU.

Thatâ€™s only true to a very small degree. Most of the price difference is simply taxes, which in the EU are always included in the stated price whereas on the US itâ€™s only added at the time of purchase. So you canâ€™t compare EU prices to US prices, because theyâ€™re not the same kind of prices. Most of the time, the launch prices are almost exactly the same as US MSRP, plus taxes. It varies a bit depending on location (and therefor buying power) and currency conversion, but overall, the prices really are pretty much the same for the actual GPU itself, minus taxes and other fees."
"I would actually argue this in a different way sort of. Let me explain.
First you are right, I have a 1070 and yes. You are 100% right of course but let me be realistic when it comes to games. 

I'm 3x year old. I am wealthy now. I remember when 1070 came out. I spend so many months saving. Now I could buy a 4090, I have the income, I have no kids, but why? I actually did spend the money mind you. I bought myself a Sony A6600 with entire Sigma Prime Lens Trio and A 18-50 Zoom lens and that wasn't enough so I went and got a Super Zoom Sony 70-350mm. When you sum it up it's more then a 4090.

But thats not the issue. Since I'm also old let me put it like this. 

When UT came out you needed a GPU or you were stuck at playing a 320x240 Software 400x300 if you had a really good CPU. UT was the game everyone played. It was the top game. Same goes for everything down the line. Half Life, Half Life 2, Starcraft, Warcraft 3, Red Alert 2, CnC Generals, Company of Heroes.

All these games needed better, newer, more capable hardware. 

Oh your GPU can't do the new version of DirectX too bad, oh your GPU can't do T&L too bad. You were blocked from playing games.

Now on my 1070 with 1080p60 am I blocked by anything? 

New games keep coming out, I see no issue or reason to upgrade.

Elden Ring? I could run it.
Ghostwire Tokyo? I could run it. 
TL Warhammer III? I could run it. 
OW2? I could run it technically but I am never playing a Blizzard game in my life every again no matter the quality.

The point being. Aside from the occasional mega hit that requires a good GPU most of the game that people play these days don't need one.

Vallorat, CS GO, Genshin Impact, Apex, Warzone, LoL, Dota 2, Minecraft, Roblox, Fortnite, PubG, Fall Guys and so on for the normal people.

All of these will run really well on 1080p60 with a 1060 6 GB. 

For the older guys like me Paradox Games, Dyson Sphere, Civ, Humankind, Anno 1800, Cities Skylines, and so on... they're all good with 1060.

And I mean you're seeing this with the PS5. I have one. It's sitting collecting dust because what am I going to play on it? People seem to be playing 95% PS4 games on it. And then an occasional hit comes out you play it for 1~4 weeks and you go back to the social games like FFXIV or Genshin or Apex etc... 

So it won't be the ending of PC gaming, it's going to be a stagnation of PC gaming. And more and more people will join because the entry level is going to be really cheap."
I think we may see better things by 2024, when Intel releases itâ€™s higher end gpu. They may be able to pull the market down if they can offer 4070ti performance at 599 or lower.
Yeah tax in my county is insane. I'm looking at a 20% mark up even before the retailer gets to mark it up.
"nah, I disagree. I get it, youâ€™re supporting the narrative, but in no way do you need a brand new GPU on the first months of a generation launch to afford PCgaming. There are still RX6600 out there that can totally run everything in 1080p for a couple hundred euros. You donâ€™t need to have a beast PC to play games, and in no way it is â€œsolely affordable to wealthier peopleâ€. There are plenty of systems on the second-hand market too, you just need to look further than NVIDIA â€œlatest releasesâ€ page. 

Obviously it doesnâ€™t take the blame off of NVIDIA with this shitty launch, but I donâ€™t like when people imply that you need a new and fancy GPU for an average gamer."
"Mind if I ask how much more? I'm in South Africa and we typically import from the EU and have absolutely bonkers prices. However I've never been able to know if it's our local warehouses (I have a dealer account so I get market prices but they're atrocious market prices) and importers, if it's duties, taxes or the EU price is the same?

To put it in perspective the absolutely cheapest 4090 here (with 15% VAT) was USD $2511 with a dealer discount. $2700 through retailers."
"It's not really that unaffordable if you don't upgrade every one or two generations. Let's say you buy a 3000â‚¬ PC which is pretty high end even at current prices every 5-10 years, If you go this high end you'll still have mid range level performance after many years, my 6 year old PC is still doing great. Now if you calculate that as a monthly cost you end up with 50â‚¬ a month for 5 years and 25â‚¬ a month for 10 years. While that is certainly very expensive 25-50â‚¬ aren't impossible to afford that's maybe 2-4 streaming services worth and that's for an extremally high end 3000â‚¬ PC you can go a lot lower then that.
I'm not saying that these prices aren't crazy overpriced and that these GPUs aren't bad value, they certainly are way to expensive, just that it not impossible to afford."
As if better jobs are found around the corner lmao
"I used to be able to put aside a bit of my wages every month and have enough for a completely new pc (not totally top of the line, but say 80% of the way there) at the end of the year.

To do the same today, I'd have to keep this up for 2.5 years for the same baseline performance. Fuck that. I'll just spend my time playing older games and doing other things instead. Plenty of books left to read and movies/series to watch."
"The issue is the same of smartphone business. Technology has peaked, so people won't keep changing their devices as fast as they used to, so corporations are compensating for that lower demand with increased prices.

What else people would want? UE5 graphics in 16k on 3200fps?

Companies realize the industry peaked long before customers with blade runner zoom and fly eyes can ever notice."
">  Especially considering salaries haven't increased much at all in the Last 15 years.

At the low end, they definitely have. 15 years ago, summer jobs were paying 7.50 an hour.  Now Walmart cashiers are starting at 10-12 bucks an hour."
And especially if you got the 8gb model, it's still a viable card today. Yesterday's high presets are today's lows, they're still fighting in there.
"You're right, perception has been skewed. I bought a 1080ti for Â£650 in Nov 2017, and I already considered that way too much, but told myself I'd use it for 5 years, which I have, and seems I will be doing for the foreseeable.

My whole end game for that was that hopefully a Â£300 mid range card in 2022 would outperform it and give me a refresh, just didn't expect the situation we found ourselves in now. :/"
"970 and 980 were released in 2014, but I take your point.

The 780ti is doing just as well today as it was then, much like the PS4, the graphics haven't changed, so you can still play games on it just as well.

The cards don't get slower, expectations just change."
"My 670 runs Cyberpunk 2077 better than PS4. That's for sure. Lol

The inverse is also true when the PC port is worse, but that's the point.  The power is comparable but performance depends on the game."
They'd more or less be on par with the PS4 and Xbone which, lets be honest, are pretty fucking bad.
"I'm sitting on a 1080ti, and at these prices I'll be sitting on it a bit longer. Originally my intention was to use it for 5 years, and at that point hopefully even a low or mid range card would out perform it, just didn't expect low or mid range cards to cost the same as a former halo tier product.

Got several friends still rocking the 1060 as well, they're so good."
My 980 founders is still kickin. Cant play things on tip top graphics anymore, but she can still run things the way i want em.
Exactly.  2070 Super 8GB works just fine for me on newer and older titles.  Not changing anytime soon, not at these "thieving" prices!
And you could buy a 3060 right now for 350 and have better performance than a console. I have no idea why OP is acting like the price of the 40 series would drive you towards a console.
You forgot top tier monitor as well.  Yeah I just don't see that as practical for any gamer or hobbyist.  I mean perhaps if you're a literal professional gamer.
[deleted]
"Bought a pc for 1250â‚¬
Self built

With corsair 4000D, gold80+ 750w psu, amd 5600x cpu, 32gb ram (3600mhz), 1TB SSD, mobo with wifi and â€¦ the 6700xt as graphics card.

It was slightly more expensive than last time i put together a pc, but hope to use it for about 5 years again.
Maybe by then not anymore triple A games on 1440p with every detail on high, but should be okay with meddling on texture details and lowering settings here and there.

I bought it end of November, the same pc would have costed around 1700 a few months earlier. Crazy what just a few months did in terms of market pricing. Still not what we hope them to be of course ðŸ™ƒ"
"> MSRP was adjusted to $250 before it even launched. Only the FE was $299.

Cool, was just going by a [quick look at the wiki.](https://en.wikipedia.org/wiki/GeForce_10_series#GeForce_10_\(10xx\)_series_for_desktops)"
"The big advantage was the game sales. PC games had way better sales than console ones unless you were reselling them.

Plus, you can pirate PC games and save quite a bit that way."
Every console gamer still needs a computer.  The correct  cost comparison is console+computer vs just 1 gaming computer.
"Super budget builds were always a bit of smoke and mirrors but the Xbone/PS4 generation was a huge gift to PC gaming. For a little bit more than a console you could build a PC that just ran the table the consoles were so weak.

This console generation feels more like the aughts where the consoles are clearly superior if you are doing a brand new build. 

> Some people include like a decade worth of PS/XBOX subs at full MSRP to try to favor PC there even. 

A decade is wild but I don't think its completely unreasonable to consider fixed costs that you get with consoles."
"Because as others say that's the total price of admission. If you buy a 4070ti, you still need a processor, motherboard, memory, storage, case, PSU etc.

If you buy a PS5, that's everything you need to start playing. And yes, the tech may be equivalent to a 2070, but developers will still be targetting it in 5 years, so it will still get new releases."
I agree with most your sentiment. But the â€œit shouldâ€, I do not. It should in the sense that because the conditions have changed, it technically should. But in general, it shouldnâ€™t. Lower pc gamer numbers is not good for the industry - on both the consumer side and the manufacturer & developer side.
">and will look and feel a lot better than idiotbox fake upscaled tiled blurred controller inputted so level design can't be complicated 4k vaseline smear.

Thanks for the good laugh to start my day."
"> last week there was like 19 4090 in my local microcenter. That thing sold out in a couple of hours

halo product. Nvidia can almost charge whatever they want and still sell out of it. in part since a lot of people use it for work due to cuda and the time saved means you will quickly earn the cost back. and in part since some people have one hobby so nearly unlimited cash to spend on that.

it is the 4070ti and 4080 sales you should look at which have been god awful."
"The 4090 will sell out because there are enough people with huge incomes that just want the best. Anything under that though is gathering dust on the shelves. 

My local PC stores sell out of 4090s immediately but always have 4080s/4070TI in stock."
Very sad but true.  Gpus are 200 to 300 % more expensive and nvidia l9se 20 percent of the market.  That's gona be a huge cash cow increase for nvidia all while producing less and selling less.
People aren't buying them. There was news jus a week or 2 ago that GPU sales are at a 20 year low!
I'm not going to blame anyone who has money and wants strong gpus. It is what it is.
Canada is in the same boat. A 4080 is $1700CAD, and most people make roughly similar salaries without conversion (or lower) to Americans where it's $1200.
Yep that sucks.
"To play the devils advocate - playstation plus is 60 dollar a year. It offers ""free"" games while you have the subscription, generally at least a few absolute banger games a year.

Game launches suck, but unlike PC you can buy used for FAR cheaper. Some ps4 games are also upgraded for the ps5 to run at 60 fps. Most of those games can be found for 5 bucks used."
"that $10 hike in console game (and already some pc game prices) is agonizingly the reason i'm not buying nearly anything for my Xbox at launch now. The sales take longer to settle in, and they're never as deep as pc discounts either.

That said, gamepass's subscription library is pretty worth the asking price, and if you have a pc and console it is doubly so, cherry on top being a very workable cloud gaming library too to enjoy on my phone with or without controller."
Games launch at $70 on PC too...
[deleted]
That's not at all outdated.
Yes, that's true and it's working: I bought a PS Plus subscription with my PS5.  Worthy it, IMO
"Well they are both related after all. ðŸ‘Ž

Going to wait until I get future proof 4K 60 Ultra for around Â£600-700 whenever that's available, and then I will replace my 2080 Super."
"Lisa Su doesn't even care as a console sale is also a win for AMD.

This is how an oligopoly looks like."
..but only one of them has the power to do it on their own.
Then complain when market share plummets to 8% how there isn't any competition and we are getting price gorged
I'm trying to learn Blender and Nvidia cards are just better by a significant margin for that. The issue with AMD GPUs is that they are competitive in gaming and nothing else.
Sub $200 is "low end" for me. $200-300 is "mid range" for me.
Honestly, I expect even more. One year ago I saw a 3090 @ kabum for R$25k. 16k is still insane, though.
Basically 3200 dollars, abusive, completely abusive pricing.
Is that min wage per year or something?
Prices scalability is all but lost. I was able to build a roughly PS5 matching machine + peripherals for the price of a PS5 here, so you can kinda observe the situation. But the Xbox, particularly series s, makes 200% more sense for gaming. Since it's available at a ..decent, price. It's both a tax thing, our devaluating coin, minimum wage not moving as it should, and the online stores being shitty. I've never seen a GPU at MSRP here, past 2019, for instance. While all other parts seem to do just fine at that.
Yep, 66.43% still on 1080p according to Steam, with only 2.68% on 4k.  Even ignoring how 4k is upscaled on consoles, having some sort of 4k for probably 1/4 the price of a 4k gaming rig build has to be incredibly enticing.  And that's a crappy 4k gaming rig, if you want full 4k with RT and a solid 60 fps you are going to be spending half that 2k budget on the video card alone.
I visited my local pc shop yesterday for unrelated reasons and they only have series 3xxx cards on the shelves. They carry 4xxx too, but only in the online shop. Too much 3xxx stock left to market the 4xxx's at this point.
Popular free2play games like warzone, Fortnite are free to play online on ps5 I believe
I get that, but the value is still there on the ps5 for being so simple and cheap compared to a corresponding pc.
Can it run PUBG, MW2, etc? I've had one for months but still haven't played a game on it yet. I've downloaded a handful of single player games but when I was looking at multiplayer shooters from my steam deck library like PUBG it has a little symbol that shows its unplayable, which I assumed meant that it isn't even possible to download it. Haven't had much time to really mess with it but it would be awesome of those online shooters I assumed were un-downloadable, are in fact playable!
Realistically, it was probably AMD assuming that Nvidia's mindshare means a lower price wouldn't increase their market share much this generation, so they might as well keep the prices just under Nvidia's.
Looks like that may bite AMD on the ass with the 4070ti release at $799.  You're right, they should have undercut Nvidia by more, put more money into their drivers, and just marketed the hell out of the price advantage.
"AMD, unfortunately, have had issues which have a knock on to the entire range (vapor chamber, etc.).  

Like they did with the CPUs they have to make them cheap enough yet feature-full enough to make people want to change which then forces the competitor, Intel in this example, to actually do something .  A lot of what Intel did was border-line illegal and bribing CPU comparison sites, but at least it's forced them to also improve there line up.

nVidia, at the moment, would rather sell 1 card at $1500 rather than 10 cards at $500.  It's just pure greed.  I'll be getting a second hand card but only when I'm struggling to play games.  Before, when they were reasonably priced I'd upgrade most times.  The vast jump in price vs the jump in performance just isn't there."
[deleted]
"Yep, I bought one of those too a year or so ago. I'm absolutely not a PC is best type, they each have their purpose.

But what I wanted to do couldn't be done on a PS5."
I wanted to be able to play any game, as well as heavily modded games, at the best or close to the best settings. So far so good. I downloaded Nolvus for Skyrim, with 1500+ mods and I can run it 1440p with all the performance intensive additions at a nearly steady 60 fps. That's basically all I was looking for and it worked well.
"Only office monitors are still 1080p 60fps nowadays, anything above 200$ either has +144Hz refresh or is Quad HD resolution.

Even 4K monitors have gown down in price A LOT.4K 144 Hz monitors are starting at 500$ or cheaper if there's a sale.

This crowd of people still insisting on 60fps 1080p are either willfully ignorant or don't know better how much the monitor market has evolved."
:(
The majority of people in this sub can buy a deck though
Do you think the bullshit jobs are going to be around when the planet is starving unable to grow food or have enough drinking water?
Why do you say that?
Have a 3080, upgrade every other gen.  Will buy a 5080 when released.
You put a lot of groceries in your computer?
Well, working in Blender with it sucks all the ass, but there just arent any reasonable options.
[deleted]
"*a few grown ass adults

I think median full time wage is $25/hr."
Just wanted to say this, 30 series is also overpriced at MSRP two years later, Jensen wants you to buy up their overproduced old stock. Only not buying is gonna kick his ass.
">1060 is still capable on low settings, especially with fsr on.

It's ok, but it's not as good as a ps5. 

Like I said I am still using it, but I decided not to upgrade the whole machine and got a console instead."
And that consumer was willing to pay the scalpers their asking price. Not to mention they will be able to justify a higher price now thanks to the increase in the prices of GPUs.
The business model is also different. With consoles you can sell them at cost or even at loss while making your money with games sold, you can't do that with PC components.
Plus they make a lot of their money back on selling games and subscriptions.
"It was a tough call for me. I had a 3060 that worked fine ( ok, it will stuttered a bit ) on a 27"" display and then upgraded to a 49"" display. Modded Skyrim went from 45 fps to about 22 fps. After 6 months of trying performance tweaks and overclocking to get my fps back up, I decided to pay the the piper.

I first tried the 3080, it could get me close to 60 fps. But even with undervolting, the GPU fan made it difficult to hear the game, lol. I returned the 3080 and got the 4090. It runs the game at 60 fps and is dead quiet!

Now I have a new problem. I cut back on graphics mods conserve VRAM for the 3060, but now I have the resources to go ham on them, lol. idk if you've ever modding a game, but getting 950 mods to play nice with each other is no small task. Once you have it all working, it's a good idea not to touch it!"
"There's also tons of competition in the phone industry. From $30 garbo phones all the way up to the $1500 shit, and the mid-range is a battleground of options outside of iOS. Android based phones from Samsung, Google, Motorola, etc also regularly go on sale so you can pick up what was a $700 phone for like $500 like 6 months after release (I did this with the S20 FE 5G)

GPUs on the other hand, not so much. I mean 30 series GPUs are still going for ABOVE their MSRP, let alone back down to it or below it. 

Buying used is an option but a lot of people have understandable reluctance to that and would prefer to just buy new, but with things being so high now I wonder if that will just end up driving up the price of the used market too."
"Well, i managed to get a Lenovo Legion 5 with a 3060 in a time when a desktop 3060 alone was half the price of the whole laptop.

Initially it was a temporary solution, i wanted to replace my 2018 Acer Nitro 5 and my desktop (Ryzen 3600, 32Gb of ram) was stuck with my old 1060, so the Legion 5 looked like a balanced temporary solution while i was waiting for desktop GPU to lower in price.

But damn, this laptop really surprised me, is so damn powerful that i'm not planing to buy any of the current GPU for my desktop anymore.

its also more power efficient, wich i really appreciate with the current European energy cost situation, and i also can carry my main device with all my games and savegames anywhere instead of having 2 installations in 2 different devices with 2 different graphic settings."
amazon before last christmas. deals dropped it way down and then amazon was giving a $60 credit too. $200 for the 6650xt and a couple games
Heck my under $1k LG CX had 4k/120hz with Gsync/freesync 3 years ago, PS5 games look amazing on it.
"Crypto has (largely) switched to ""proof of stake,"" which means GPUs are no longer being used to mine (not to any significant degree).

No, the new normal is that Nvidia makes more money selling it's chips to AI businesses at 20+ times the price they would sell to gamers.  Effectively, the new Nvidia business model is that we should all be sucking their digital dick in gratitude that they even deign to acknowledge our existence at all.

Sadly, AMD, rather than stepping up and providing competition, has looked at what Nvidia is doing and decided there is room for them on the bandwagon, too, despite being unable to compete in Ray-/Path-tracing tech.  Their GPUs come with more RAM, so somehow that justifies enormous mark-ups on product."
"> The thing is that right now we can't know if this is a blip or if Nvidia have decided that it is perfectly fine for a graphics card to cost as much as the rest of the PC.

I'd bet its a blip. What is important to realize is that Nvidia and AMD made projections based off of pandemic/crypto demand and they want to meet those projections. 

They aren't going to rollover and say oh well but you can walk into any PC hardware store and see stock of 4080s, 4070TIs, 7900s sitting on shelves. That would be completely unthinkable this time last year. Like literally people were buying 3060s for the price of a 4070TI. So Nvidia/AMD are going to try to make it work and make their projections and they are going to fail because stock is sitting around and they'll have to lower prices. But they are going to try very hard to make it work."
When mining goes away and gamers are the primary customer the prices will go down to reflect demand anbd currently Crypto seems to be imploding.
It already is.  The timing of gpu costs has happened at the wrong time for gpu manufacturers and I see pc gamers already leaving.
Sorry for your loss
You could probably pick up a used one for cheap and have much better graphics than a PS5. And use a keyboard/mouse.
What brand was it? Msi? PNY? gibabyte?
F
[deleted]
[deleted]
Yep I was a nay sayer but game pass is great value.  Thk you Microsoft for keeping gaming alive for me.
How?
"Your math, unfortunately, doesn't work in Europe. 

I'm looking at local prices for a 3060 and I'm seeing cards in the range of 350 - 400. Meanwhile the Series S is 242,- at Amazon.de with a controller and a month of gamepass.

Sure, with some effort and a bit of risk you might be able to score a capable GPU at the second handed market but for 95% of the people in Europe, at the moment, PC gaming just isn't where the value is at the moment."
I doubt they care much about the consumer market considering the growth in the server space.
GPU's are a small segment of their business at this point they might not care.
"$300 used to be my hard limit for a GPU because I always bought mid to upper mid range. Now I'm willing to go a bit higher but $500 is my absolute cut off.

If I have to buy used for the rest of my life than oh well but I'm hoping that won't be the case."
"Well weren't those (1080 & 2080) the highest chips at the time of release?

Also, in regards to the relative power compared to others in the same generation the 4080 is weaker relative to the 4090 than the 3080 is to the 3090. That's according to several reviews."
What is true is the the 4070 ti was gone be called 4080 12 gig and with a 960, 1060,2060, 3060 size 196bit bus or simular.  I do think the 4090 is the real 4080 and the 4080 16 is the real 4070 and the 407ti is the 4060. I cand get past that.
"No, we absolutely ARE talking about paying a premium for the highest grade components.

With the 3080 and 3090 being where cards start to command a premium, over likely fair value, that one can credibly describe as being hundreds of dollars. And the 4090 being where prices go crazy.

[https://www.tomshardware.com/news/gpu-pricing-index](https://www.tomshardware.com/news/gpu-pricing-index)"
"As you so pointed out, the good has to be *sold*.

In order for it to be sold, the buyer has to be willing to be willing to pay the price. If the 4090 were priced at $10,000,000, it would not be *sold* at all.

So no, the market determines the price of a product because the market determines if it's sold."
"Yeah, it's 1080p60, which is still perfectly fine. 

A vast majority of people don't see the difference (see: Playstation 4 sale numbers up to only recently, with mos of the games being 30fps). 

This shit has been hashed out a billion times over the years. 

The number of people wanting 4k144hz is incredibly small, and NVIDIA is banking on them because they can."
No, but the 4080 is a high-end card, you can get 2nd hand one or some cheaper GPU. For example, the cheapest new 3060 in my country is 350-400 euros (depending on the brand).
Only 20% is disappointing. But tends to validate the sad reality of things.
"Lol sir/maâ€™am, your metrics are inaccurate and comparisons all over over the place. I described technologies price progression throughout time pretty thoroughly. I would re-read my comment as itâ€™s apparent you missed some things. 

 Using inflation adjusted dollars, please show me the comparison of MSRP of the 10 series vs 40 series. Then review the performance differences for the 10 series vs 40 series. I would recommend using the site I originally referenced. Hopefully this should start making a lot more sense to you."
I find it best to complain in USD so more people understand my moaning and bitching in general, so I can't see anyone faulting you for using it too.
from what I heard US sales taxes in most states are much lower than European VAT taxes wich are for example 19% in Germany or 23% in Poland for electronics
"> US itâ€™s only added at the time of purchase

That is something that I did forget about, so thanks for the heads up."
PC gaming already "stagnates" coinciding with the end of a console cycle. Unless the game is a PC only title, the developers performance target is whatever the current gen console is, and rarely nowadays do studios go beyond upping texture resolution and some superfluous graphics settings to PC games to push the envelope. Hell, we usually have to cry bloody murder just to get an FOV slider or a toggle for mouse acceleration.
The thing is, suggesting a used piece of tech to a non-enthusiast is really hard to do. My brother has basically stopped gaming on his PC because he's needed an upgrade for so long and now he's basically priced out of anything he can go into a store and buy. I can scream until I'm blue in the face about used 3080s, but until he can go into a store with a visa card, he's not going to upgrade.
When I see new gpus launch at affordable prices, not years old cards marked down because nvidia controls 85% of the market, I can get onboard with midrange being reasonable.
generally I have to buy most things from the UK or Germany, but the 4090 in the UK is Â£1,899.00. Then if I put shipping on top it is obviously more.
[deleted]
"That's the future, media does not decay or lose value because it's old. Series, movies and games from 10 years ago are at a standard that are still good nowadays, especially the heavy hitters. In 10 years you can still go back to Witcher 3 and be heavily entertained. And you could probably play it on a phone at that point in terms of hardware need.

I am really curious about the future of gaming/hardware, we will reach a point where progress in graphics will be less and less appealing because it just won't matter if you can see every single hair on a character or only half of it in high fidelity.

Are there really people out there that say Cyberpunk or Witcher 3 are ugly games and can't play them?"
[deleted]
"That is false. We are still a long way away from â€˜peakingâ€™. We are still a long way away from having displays that saturate the human eyeâ€™s capabilities in terms of dynamic range, resolution, framerate and depth. We are still decades away from having a display so high-end in every respect that you need augmented eyes to perceive any further improvements.
  
The ideal display would be something akin to a microLED lit 32:9 super ultrawide curved display with 8K resolution, HDR capability up to 10,000 nits, and glasses-free stereoscopic 3D capability (glasses-free 3D exists today in Acerâ€™s recent Predator Helios 300 SpatialLabs laptop or that new ProArt OLED laptop ASUS just announced at CES with glasses-free 3D) with a 240 Hz or higher refresh rate.
  
4K only recently became widely available in monitors and HDR1000 is finally taking hold in monitors, but microLED and glasses-free 3D all at the same time are still years away. Samsungâ€™s recently announced 57-inch, 32:9 â€˜dual-4Kâ€™, HDR1000, 240 Hz super ultrawide monitor announced at CES is a good step in the right direction. That will probably be the best display you see in 2023."
He means real purchasing power
"The 390/390X or even 290X 8 GB are still viable cards today with NimeZ drivers, lol. Which for 8-9 year old cards it's just insane.

These cards are holding up even better than legendary cards like 9700 Pro, 8800 GT or HD 7970."
"Yup, that should have been a viable strategy. This is really just companies cashing in while they can at the end of moores law. They can't offer improvement anymore, so there's been a bit of a psy op to usher in higher prices.

I got a 3070 during the pandemic at msrp (Â£460). Annoying overpriced, and since everyone was scrabbling for them they're overvalued for sure. The performance bump over the 1080 I had wasn't wild, and it feels a little underpowered now compared to what I like to run. It's all a bit of a shame. It also got superseded by the ti because nvidia are dicks, so there's a big stack of models above it that make you want to upgrade."
Iâ€™m sitting here wondering if Iâ€™m going to use the 1080ti for one more generation. I also thought at the time that 700$ was insane. Now itâ€™s 899 for the bottom of the 40 generation. Just wild.
"Not at all, lmao. 780 Ti (the entire Kepler line) aged like opened milk carton. A 780 Ti is often at the level or slower than a 7970. 

390/390X from 2014 are what aged the best from that era."
Same but with 1070 here.
Thankfully my interest in gaming has waned in recent years, aside from some indie titles. Thus, really, the 1060 is good enough and the absurd prices are just saving me money.
My 1080ti died or i'd still be using it. Ended up getting a gigabyte 3080 master edition cause it had the least percentage of price hike  at the time :v
Yeah same with the 1080ti. At the time I got it in 2017 it felt excessive but im glad I paid extra now.
6650 XT has similar performance for ~$300.
"Coworker did the same years ago and his 690 was still very useable and putting good looking graphics when I first got my 1070.

For my use I don't see a ton of advantages in the more current cards that merit the huge leap in cost.  Ive gotten many of my cards used and while the older models don't run as cool or energy efficient, they will still do a vast majority of games in at least mid-tier settings at 1080 resolution."
"Errr I think you mean can still run things.

I don't think you mean how you want em unless you don't want tip top graphics for some reason?"
Same, no reason to upgrade yet. There needs to be more truly next-gen games to motivate us.
"$350 GPU vs. $399 console

You still need an entire PC for the GPU"
"Seriously. A year and a half ago, I bought a pre-built with a GTX 1660 Super, which was already a somewhat old mid range card at that point, and it runs every game wonderfully. Maybe you can't turn on every single bell and whistle, but performance is great on high settings for every game I've ever tried.

You can get a 1660 Super right now for under $200 if you know where to look. It's about $270 retail, which is also not that bad.

The freakout over pricing is overblown and unwarranted, much like every other gamer freakout."
"I'm with you. People are looking at these enthusiast cards that blow the consoles out of the water and wondering about buying a Xbox instead?

A 3060 is the performance equivalent to a PS5 and actually outperforms it if you are running a 1080p screen (which statistically anyone reading this comment is using). Anything about a 3060/6650xt/A750 is an enthusiast card that is doing things the consoles *cannot do.*"
"> The big advantage was the game sales. PC games had way better sales than console ones unless you were reselling them.

They've been on par for years now pretty much the entirety of last console gen. And retail copies on the shelves eventually can go cheap as hell even new. Nintendo being the exception."
A number of people fill the computer gap these days with phones or tablets. And hell a console plus a netbook tier laptop is still cheaper than a decent PC.
">For a little bit more than a console you could build a PC that just ran the table the consoles were so weak.

Define a bit though. Half those a ""bit more"" builds are way late in the gen, including optional expenses for the consoles, completely excluding input devices, some acting like people are gonna buy half a dozen gamepads on the reg, excluding software costs, and etc. 

Like yeah Windows was free for a long time, but if you don't have an existing license to sub in it's not actually free. I can get excluding screen/panel costs cause that comes up no matter the platform though, but excluding input on PC also doesn't make sense... console includes input for one person PC does not. Even cheap ass input is gonna cost about $20 on the budget. Also worth mentioning how many of those builds cut corners on PSU and mobo which have a hell of a lot of impact on the lifespan of the build. 

Budget builds can be decent, and there is nothing wrong with them... just trying to match consoles economics with one is practically a fools errand. Around 2017 or 2018 (it all blends together) you could get a PS4 Pro new with games for like $400 if you shopped around. Trying to get near that on a budget build would be a nightmare and involve a lot of caveats and some hardcore wheeling and dealing.

> A decade is wild but I don't think its completely unreasonable to consider fixed costs that you get with consoles.

It's unreasonable to completely throw it in. If all someone does is single-player exclusives they aren't buying the online subs in all likelihood. And there are avenues to get them at discount even if you are."
It makes sense I doubt I could get a pc running for 500$ with a 2070 in it
Presumably a PC gamer has a PC already?
[deleted]
Yet 4090s are sold out everywhere
Those numbers are from before new GPU launches so they tell you very little.
Is it a supply issue? Earlier this year there were literally no GPUs to buy and they were double their MSRP. Seems to me the new cards are selling well enough.
"On the Xbox side - $60 a year for basic gold membership that lets you play online. They run sales and give free games and content to gold members that tend to be pretty decent. Gamepass Ultimate is I believe $120/year, and worth it in my book. 100+ titles available to play as you please with plenty of day 1 release titles and older big hits with a healthy mix of indie and 360/OG Xbox titles. Gamepass members get 10% off many games and dlc as well as discounts on some subscriptions and odd freebies. 

Xbox also has a massive backwards compatibility library, and used copies of games are also pretty cheap and readily available. 

To be frank, I much prefer my pc over the Xbox one in my living room - but my pc isn't in my living room, and the ease of switching between games and streaming apps like hulu or Netflix is really nice."
[deleted]
"Also used games! When these conversations take place in PC gaming spaces used physical media is ignored completely, but it's one of console's biggest advantages. You can often get a second hand copy of any AAA single player game for at least ~30% off a few weeks after release, usually way before it hits those prices on PC. The reverse applies too - You can usually get a huge chunk of that $70 back once you beat it if you were so inclined to pick it up on launch.

Used/physical game sales are very pro consumer and that is why both Sony and Microsoft are working to kill them through digital only consoles and things like Gamepass."
Not all of them have made that jump and theres usually more deals on PC as stores try to compete for sales.
A $499 PC wonâ€™t be doing anywhere near 4k without upscaling
[deleted]
Absolutely valid point!
the base resolution in many games is 1440p+
That stuff launched 6-7 years ago. It's getting up there. Basically a console generation behind, in terms of time.
"They have so much incentive to sell a console over a lower end GPU or even mid-range.

Remember, with the console, they sell you not only a GPU, but another CPU. This means they either sell you two CPUs or sell you a CPU when you werenâ€™t planning to buy AMD. A win win for them."
And now that they fight for business segment with threadripper and epyc they don't give much damn about CPUs either
That is not how markets work.
"Many RX cards weren't good on release. The only reason they are decent now is AMD started dumping them. Buying a 6600(6650) right now is fine. It's more or less 3060 in non RT performance for like 250-280ish bucks. But on release it was priced 320, the same as 3060. Which is just stupid. A lot of these cards are arse at MSRP. HUB retested 6700XT which was supposed to rival 3070 for 20 less, but turned out more of a 3060ti competitor for 80 more.

I'm vendor agnostic. I had enough AMD and Ati cards in my life. But at this point AMD needs to do something to bring me back. They don't want to be a ""budget brand"" but they are. Budget DLSS2, budget RT, etc.

AMD isn't a victim. Their graphics division is unloading mag after mag into their foot. And looks like it's contagious.

BTW not buying 4000 OR 7000 series GPUs. At least not untill refresh with price drops if it ever comes. Both are shit."
Maybe it is AMD's job to make good cards, the consumer shouldn't need to buy worse cards to even the market share.
"Right on. We're on the same page.

Some good news from December is that sanity has largely returned to pricing for those low end cards, moving some GPUs that are still respectable as we start 2023 back under $200. Reusing a link I posted elsewhere:

[https://www.tomshardware.com/news/gpu-pricing-index](https://www.tomshardware.com/news/gpu-pricing-index)"
"Same.   
I live in the UK and the RX 6650 XT converted into USD is $371 and a GTX 3060 is $423.28. I'm stuck on a GTX 960 because I cant afford to get a new GPU but I need a computer for university."
One year ago scalping because of mining was still rampant. Today, that situation is decidedly different. Guess the manufacturers haven't caught on to that yet though.
Yep thats Brazil for ya
Monthly most likely
Monthly
Crazy stats!  And to think that 4K has been around over a decade now, but 2.68% on Steam tells us a lot.  That 66% tells me even more.  Most PC gamers seem happy with 1080-1440P and I agree.  Wasn't AMD pumping up 8K gaming before the RDNA 3 release?  LOL, that shit isn't happening for quite a long time as 4K adoption has been pretty slow, on PC that is.
"Crazy to me, since finding anything above a 3060 or maybe 3050ti at msrp is next to impossible near me (and seemingly online as well, 3070 and up area is a ghost town).


You actually still see some scalper prices for higher end 30-series cards that basically make the 40 series look more enticing to the average user."
Simple yes for some people, but cheap no. It's more expensive for comparable perf imo.
"Youâ€™ll have to use Windows for that, but you can easily just have an SD card with Windows installed just for games you canâ€™t play on SteamOS.

Or wipe the SteamOS install and replace it with Windows."
"Nvidia spends 50% more on R&D than AMD spends on cpu/gpu/server combined 


And you want AMD to further cut their margins which are already lower than Nvidia AND invest more at the same time."
I'm not sure I agree as both have had issues that have made headlines but maybe it just doesn't stick with Nvidia? I honestly don't know what the problem is for AMD. They have the better cards and price points in the low-mid range which should be an easy sell for people looking for cards in that range but it just doesn't happen for them. I mean you could argue that RT performance and dlss sell it for Nvidia but both of those are pretty poor on the lower/mid range. RT hardly runs on anything below a 3060ti and dlss doesn't perform as well at lower resolutions. So people should be looking at raw rasterization performance numbers which AMD trashes Nvidia on. I'm on a 3070 which I was lucky to get at rrp last year and will be likely skipping until there is a card the doubles the performance of the 3070 at around the Â£500-Â£600 price point.
Can you explain this? The 4070 ti is $800-$900 and performs on par or better than a 3090 which had a launch price of about $1,500. Isnâ€™t that a substantial improvement in price for performance?
What compatibility issues, exactly? Honest question. I've had an Nvidia gpu in my rig since the 7970 and have had plenty of issues with Nvidia drivers, especially with multi display set-ups so both have their problems. I sold my Vive last year without using it with an AMD card but it wasn't exactly great on a 3070 either.
"Yup, PC has several uses, PS5 is solely for media. But so are more and more PC's.

But as you wrote in your post, a decent, not top-of-the-line, GPU cost the same as a whole dang console."
"Nice. My 1660 ti just couldnâ€™t drive the huge display. 

I got a rtx 3060 12Gb which couldnâ€™t handle native resolution, but using Nvidia image scaling it could hit 45 fps at 85% scaling and a bit higher with more scaling. But it didnâ€™t look as good as native.

The RTX 3080 can hit 60 fps but dips to 50 fps in places.  

I decided, after many hours of tweaking game and drivers setting and reading forum posts about tweaking performance , to go for broke and get the 4090. 

I donâ€™t have the 4090 yet, so I canâ€™t say it will run at 60 fps will all of the bells and whistles turned on, but thatâ€™s my hope. ideally it lasts me 4 years or more as from what Iâ€™ve read Iâ€™ll be CPU limited from the start. So Iâ€™ll have to upgrade my i7 12700k CPU before even thinking about upgrading the GPU.

I do use lots of 4k and 16k texture mods along with Rudy ENB.  Turning off just Ambient Occlusion adds 10 - 15 fps on my setup. Itâ€™s crazy how other setting maybe only add 1-2 fps,and that one makes a huge difference!"
We wont see this in our life time.
Bad performance and visuals compared to my pc unfortunately.  Only game i play on it is nhl23 cause i cant get it for pc.
Egads. I read that as the only thing that has gone up in price in 30 years. lol. I did pay over $240 for my current PC case. My first PC case cost about $40.
Im not worried at all.  I have a very secure job.  Get indexed raises yearly.  I dont like all the price hikes though.  I think its definatly greed in most areas not just gpus.
Yeah it's not, but eh, if I couldnt afford a GPU this year, I wouldve stuck with it for another year. I just held out until the performance metrics I wanted could be reached for the price i wanted.
"Exactly. As Long as People pay the price they will increase it. Next gen consoles gonna be 800â‚¬ at least at launch i fear. 

For example: you still canâ€™t buy a ps5 here in Austria from a store. Itâ€™s been more than two years ffs!!!!! You either have to register for pre order or pay scalper price around 900â‚¬ . I know a lot of people who pay the pricey"
I doubt it. They would rather just get people in the door with lower prices and raise the price of subscription.
">now I have the resources to go ham on them

yup. I can't wait to own one and actually finish Skyrim myself since playing it wayyyyyyyyyy back in 2011 on Xbox 360.ðŸ˜…"
That's kind of the point I was going to make. With laptops, you have to replace the entire laptop, rather than buying just the GPU. And from 2020 through a lot of 2022, I could see more reasons to get a laptop as they were probably easier to get. But that's changed now where I am at least.
as someone that only plays games occasionally and recently played with the thought of building a new PC for the first time in 9 years I think I should wait another year. If I can wait 9 years I can wait another one. In the meantime I'll just play games that excel in their gameplay and design on my laptop. There's still plenty I haven't touched that don't rely on flashy graphics to be captivating.
Nvidia are probably thinking we are stupid enough to use their cloud gaming.  I'll be avoiding it like any other current nvidia product.  Like the plague
"GPU mining has been dead since sept 2022.

This created a godly amount of excess 30XX series that needed to hold value. The easiest way was to inflate the 40XX even more.

GPU prices should come down but nowhere even close to what they used to be. Sales numbers are still very good, all things considered, and the investors will want higher profit numbers every quarter. 

Honestly, I would be shocked if we ever see a XX90 nvidia gpu MSRP below 1300 at launch, ever."
EVGA
Interesting. Yea.... I still haven't really seen the point of getting an Xbox.... just too many games in my library to justify it.
"> I agree, although a 3060 ALONE is 300 where you are right now, you still need the rest of the parts for a PC, and that will probably put you over 600, more expensive then any console right now. 

I've made comments elsewhere that the real problem for PCs right now is building a completely new system that you want to be at console parity. If you are upgrading an existing system the current market isn't great but not horrific."
"There are different methods to doing it.  One such method is purchasing a dev kid license from Microsoft for your Xbox account which allows you to install Retroarch into your Series X or S and then install and play classic old school video games from Atari to Ps2 and further,

I've also heard you can install Retroarch without the Dev mode but I have not done that.

[A few months ago, Digital Foundry did a video on installing Windows 98 on a Series X an S.  They were playing Windows 98 games on the new Xbox Series consoles.](https://www.youtube.com/watch?v=lrpkFuKh4CI)

There is a lot that can be done with these systems.  You can find tons of videos explaining how and what to do on youtube."
"The steps are pretty much as following:

1. Subscribe to the ""Xbox Emulation Hub"" discord
2. Check when the emulators are uploaded (they tend to get removed after a few days)
3. Enter the link in your Xbox webbrowser
4. It will open the emulator in the Xbox store and you can download them!
4. Enjoy :)

It's fully legal and takes about 15 minutes to do so. Honestly configuring Retroarch is more work than installing it."
"> Well weren't those (1080 & 2080) the highest chips at the time of release?

The 1080 was, but not the 2080.

The performance difference is also similar with the 1080/2080 vs their Ti/Titan equivalents. The biggest difference of the 4080 and 4090 relationship is that the 4080 is so expensive that the 4090 is actually better price/performance wise in many scenarios. that's kind of crazy for a $1600 halo product like the 4090."
Exactly, 'over likely fair value' is key. They were already not cheap. Nvidia have decided to take the piss, likely high on the smell of their own farts after the profits they made in the crypto era.
The 1080 was a high end card. High end cards used to cost $600. If they want to charge this much for a thing they probably need to think about rebranding accordingly.
"It doesn't matter because AMD is charging what should be a $100-130 card for $200. There's no way a card was released 6 years ago with a larger GPU so more silicon and a larger bus memory pool considering the time it was released. If you going to make the inflation argument why was a GPU that cost $200 in 2010, not as a GPU that cost $200 in 2016? And surely if inflation from 2016/2017 to 2021/2022 was at a push 15-20% surely then we'd get a GPU with the same die size as the RX 480 at $220-250 instead of close to $400. There's never been a period of time in which the same amount of money couldn't have got you an upgrade.

&#x200B;

You talk about silicon prices going up explain the 5700xt costing $400-450 in 2019 which had a similar GPU die size to the rx 480... The die size is clearly what matters the most it's the big silicon GPU.

 You clearly have no understanding of how these things are made because the 5700xt couldn't have really cost much more to make than the rx 480. It was built on TSMC 7nm so there's a little bit of a difference in cost per wafer but the actual GPU was smaller than the RX 480 and cards like the 5600xt had less memory and a smaller bus but still cost quite a lot more.

I wouldn't even bring MSRP into the discussion because the gtx 1070 and 1070 ti were always at or often below MSRP. During the entire lifecycle of the 3000 series and even the 2000 series, GPUs were never below MSRP. The 2080ti was a $999 MSRP GPU but it was always $1200 the 2070 super was $50 over MSRP.

Go look up the supposed MSRP of the GTX 1060 it was $299 but the GPU was often sold below $240 so if you're saying MSRP is unfair because of inflation. The MSRP of pascal was almost the highest price anyone paid for these GPUs whereas they were the absolute lowest prices people paid for the 3000 series.

[https://www.techpowerup.com/gpu-specs/geforce-gtx-1060-6-gb.c2862](https://www.techpowerup.com/gpu-specs/geforce-gtx-1060-6-gb.c2862)"
"And? Thatâ€™s still a taxing problem.

EU taxes are high (and pricing includes it). US sales tax is low or 0 (in some states), and pricing doesnâ€™t include it. Thereâ€™s no reason why post-tax pricing of hardware should be identical between the two."
Yes, thatâ€™s correct, but irrelevant. Taxes reflect local buying power, higher taxes often coincide with higher average pay, as tax pressure and wages balance themselves out over time. Higher taxes may also coincide with lower costs in general, for example Swedes donâ€™t need to pay a lot for health insurance, so we might have more money left over for things like PCs ;)
0% in some states.
"First where are you from, second whats his resolution. Because if it's 1080p he can just get a [https://youtu.be/wF7Vm4fIxTc?t=176](https://youtu.be/wF7Vm4fIxTc?t=176)

As you can see here 6650 XT is going to run anything for a long while on 1080p60"
Exactly. I was browsing through the comments with big WTF on my face. Like come on guys PC has best base of games from 3 decades. Number of legacy games with quality content is way above any console. Wink wink emulation...
[removed]
Not with ever-increasing energy prices and people being squeezed financially long-term. The stuff you are talking are luxuries of a society with large surplus of cheap energy that is disappearing.
"Technology peaking doesn't just depend on whether something could be theoretically better. It depends on how mature the technology is.

For cars, internal combustion technology has sort of peaked. Sure cars would be better if they all went 200kph but realistically improvements are getting more and more incremental."
That is an increase in real purchasing power.
They can offer performance improvements though. Ada was a pretty big generational leap, especially in Ray Tracing Performance. And most those games don't use Shader Execution Reordering yet, which should further improve things. Plus, with the large cache you can actually see the 1% lows of the 4090 beating the highs of past generation cards in a lot of games. I think it's more that games haven't came out that demand the increased performance (excluding 4K high refresh rate) and Nvidia wanting to pretend Crypto mining isn't dead. If priced properly people would be praising the generational leap.
"I bought a Steam Deck this year, and ""the chase"" for graphical fidelity has changed for me. 40fps and in my hands, I'm happy. Just finished God of War on it.

Meanwhile the desktop with the 1080ti is able to play stuff like the latest Call of Duty or whatever I need, which is usually slightly older games anyway, I'm perfectly happy."
"You have my sincerest sympathy and empathy. I was terrified for my card during the height of the price hikes, and even now I don't want to get landed with upgrading it.

At least you seem to have landed a card that I predict will have similar longevity. I think the 3000 series is the new unicorn standard."
Got the 980 when I had to have everything at max settings. Now I am happy with things working on medium settings.
Yes, and?  A computer has always been more expensive than a console. The 4090 didn't change that.
">The freakout over pricing is overblown and unwarranted, much like every other gamer freakout.

>You can get a 1660 Super right now for under $200 if you know where to look.

I'm amazed you managed to completely refute the point you were trying to make all on your own in the same comment."
PC sales were still stronger the first few years of the new consoles. Its not till like 2016 where the advantage started to fall apart.
I never implied itâ€™s the consumers fault or job. I said the opposite.
"4090 is a tiny sliver of the market, not really relevant. 

Those halo products will always sell because they're bought by people that want whatever is the best without any consideration as to whether it makes any sense, or people that use them for work and actually calculated the ROI on their purchase. 

Sales of 4080 and below is where it starts to give an indication of the general state of the market."
"Oh is that so?

Even if, I can't imagine that the 4x series is doing numbers"
I feel like in terms of covering area of games being able to play, better off getting a ps5 if you got a pc already since you can play all through pc game pass, a series S would be nice as a traveling gadget to continue playing your games tho!
"I really can't see ever recommending an x-box though. From what I can tell, and I might be wrong - they don't have nearly as speedy storage as PS5 and is slightly weaker.

And all xbox exclusives are on PC by now."
Actually you can just buy up to 3 years of gold and convert it all to ultimate with 1month payment
Pirating games, yikes...
Yeah, I was dumb enough to make a post about it a few months back and the response was basically "We have steam sales, lmao."
Yep. somewhere that new PC game will have a nice lower price.. just gotta find it.
[deleted]
"Budget DLSS? Latest tests have shown the differences are minor to the point majority wouldn't be able to tell at all (see Hardware Unboxed) 

Sure, DLSS is technically superior, but not in a way that should matter to most customers, certainly not given that its newer iterations are locked behind overpriced ""latest gen"" GPUs, which kinda defeats the purpose (aside from not feeling as bad that a way over $1000 gpu still can't handle games at max settings in 4K without upscaling, I suppose)"
Define good? I think you are referring to priced correctly
AMD zealots can't be reasoned with.
Thats mostly second hand. There were good prices on new AMD cards around christmas though. I bought one.
Yeah 960 is rough. Half as good as a 1060 with 1/3 the vram. Even a base 6600 isn't bad though and I'd even consider something worse than that in that situation.
"Well they have to stop Nvidia from continuing to take away their market share.  The $799 4070ti comparing as favorably as it does with AMD's newest offerings, while also offering some RT performance and frame generation, will be tough for AMD to fight as they are priced today.  It's not too far behind the XTX, and from what I understand that's without frame generation or RT, and close enough to the XT where Nvidia's much larger marketing and market penetration, as well as the $100-$200 off will probably skew the market heavily in their favor, again. 

I'm certainly no marketing genius, but the last generation saw AMD [lose](https://www.hardwaretimes.com/amds-gpu-market-share-drops-to-22-year-low-in-q3-22-nvidia-controls-nears-the-entire-graphics-card-market/) market share (although arguably some of that was from Intel) with a very good competing product in the 6000 series. With the poor RT performance of the 7000 series, and Nvidia's heavy marketing of RT and frame generation, AMD most likely will have another uphill battle this generation."
"It's always been the case.  I used to go along the lines of 1080 = 2070 or 3070 = 2080.  But I can't explain anything regarding the latest pricing.  It's madness to me.

I guess what I'm trying to say is that a GPU you can get a lot cheaper now such as a 1080 TI are far more viable than a 4 series and will still play the latest games.  Yes you may not get all the RTX or DLSS, etc. features but you will still get a performant graphics card.

You can get a 1080 TI for around Â£200-250 - you can get a 4090 for around Â£1500.  That's 6 times more expensive (using the higher figure).  Is the real world performance really 6 times better than a 1080 TI?

I'll likely get a 3 or 4 series graphics card at some point but that will be likely when the 6 series is out and they've gone down to the same price point that a 1080 TI is at now.  The thing is, years ago when we were moving through new architecture there seemed to always be a valid reason to change was 3DFX, AGP, PCI Express, DX 12, etc.  At the moment, the main thing seems to be RTX and DLSS which are, in my eyes, nice to haves they're not essential to haves."
"Aww.. you deleted it. Now it's not funny.

Yeah but was it a really nice tricked out case for 40? Because you can still get basic cases for like 60-80.

I mean I paid $800 (with inflation thats like $1400) for a 400mhz Pentium 2 in like 1998. Now you can get high end consumer CPU for 500 bucks.

Or $15,000 for 128mb of RAM in 1994 or 95. Now you can max out your board for like 600 bucks or less.

Once you take inflation into account everything I'm the computer is cheaper except the GPU."
">Yeah it's not, but eh, if I couldnt afford a GPU this year, I wouldve stuck with it for another year. I just held out until the performance metrics I wanted could be reached for the price i wanted.

I did hold out to see what the 40 series was going to look like, and then gave up when I got to see the price was being bumped. It isn't worth the money, keeping in mind I'd have to upgrade more then just the GPU here otherwise I'd just have a bottleneck. The 2nd hand market here is bad.

What you would do, and what I would do is just a matter of choice. I'm not glued to PC gaming, as most people aren't."
"Good plan, but don't forget the used market. You can get some great deals on last gen (which in my mind, might as well be current gen) stuff right now if you're willing to go used. 

I recently got a 3080 used for $500 or so, and built a 13600k system. I couldn't be happier."
Thanks I'll never buy a gpu from them haha
[deleted]
Got em both.
Thanks a lot for taking the time to explain :)
"No argument here. Nvidia has gone nuts on pricing of their new GPUs, which they can keep.

But there remains ample room in the PC gaming space for 1080p and 1440p/2K gaming on a budget. And speaking as a long-time console gamer who returned to PC gaming recently, it is, by far, a better experience than consoles.

Some people would add, ""except for 4K gaming"" here, but I won't. The Series X and PS5, only rocking as they are merely the equivalent of an RX 6650 XT, are heavily reliant on compromises to make these resolutions happen.

See new release God of War: Ragnarok, which offers players the option to lock the frame rate at 60fps, and allow the dynamic resolution to dip as low as 1440p, or to lock it at 30fps and play in 4K. Because nothing says welcome to the future like playing action games at 30fps.

So I think it's a disservice to would-be PC gamers sounding out what to expect in performance and value, to *even unintentionally* feed the perception that consoles are a better value, by conflating the 1080p and 1440p/2K tier GPUs with their massively more overpriced cousins at the top end of the market.

With these higher-end cards existing largely to, by sheer brute force, beat 4K textures into submission and fit them into a gaming space where this setting has always meant making compromises.

That's not a problem to worry about, today, unless one is similarly burdened by a fat wad of cash and a sense of urgency to be relieved of it. That's why Nvidia is getting away with it right now. If we all needed a 3090, or a 4090, we'd al be screwed. As it is? People are lining up to pay their asking price.

At least they didn't buy the 3080, and aren't buying it re-badged as the 3070 Ti. So they're not stupid. They're just early adopters. And early adopters getting the shaft is not just how Nvidia rolls. It's how tech rolls."
"Well, everything gets more expensive and Nvidia got more greedy due to the boom in mining in the past 2y (not anymore). The thing is you get the same performance for less money now.

On 2nd hand market, I still see some people that are delusional about how much their cards cost now. There are for example 3080 for 650 euros but some are asking for 900. While there are 3080ti for that price. Also a lot of people selling mining rigs that are waaaaay overpriced.  For example, some guy is asking for 3k euros for his rig that has parts for 2600 euros if everything is new. I think at the end of the year, there will be even more cards, for a cheaper price."
"Ah so chip size is a direct correlation to the amount of silicon needed and is the only factor that then translates into the consumer cost. Got it!

  Tell me more about how these chips are getting smaller yet becoming more dense! Do you actually think theyâ€™re just shrinking down chips? Of course not, they physically have  more transistors than previous generations all while becoming smaller. Do your research before your just regurgitate things your read online."
"But pay is lower in a lot of those countries. 

In my profession, Iâ€™d be paid like half of what I make if I was in Germany. Combined with higher taxes, my disposable income would be significantly lower."
"I'm in Canada. The thing is, he's used to buying 70 class cards and being in the high-midrange when he upgrades. Sure, he could stick to 1080p, but it's going to be a tough sell to tell him he has to buy what he will absolutely perceive as budget parts. 

He has a PS5 and that's pretty much where he's going to stay unless these prices correct themselves."
Real purchasing power takes into account inflation
Skipped the 2xxx series cause it was so underwhelming, kinda felt that way about the 3xxx but if i had to upgrade might as well get something I hope will last me for a bit. I think my warranty for the card i got is like, 4 years or something. So that's nice
"Hilarious, isnâ€™t it.

 *â€˜If you look really hard, youâ€™ll be able to find a low tier card from 4 years ago for around $200!â€™ What are you even complaining about?!*"
"4080s are a tiny sliver of the market, not really relevant.

According to Steam hardware surveys, most gamers use XX60 and XX50 cards.

Those cards aren't yet released, so the GPU sales lull has nothing to do with 4000 series pricing."
Always recommended for Game Pass. And actually has faster hardware than the PS5 except storage speed.
[deleted]
To the publishers, pirating or used copies are the same fucking thing, a lost sale.
"Yeah, people are still treating Steam sales like it's 2012 or something. They're good, but they're far from the events that they were a decade ago. Elden Ring is a great example - Steam took until the winter sale for it to go on sale at all and it isn't a particularly deep discount, used copies on PS5 have been moving for $35-40 on eBay since spring.

And console games also get steep discounts during sales, too - when physical stock is involved, your brick and mortar retailers have an incentive to move them. Black Friday for console games can be really good."
"not to dis your comment but have you seen the graphics comparisons on YouTube these days?
There's barely a noticeable difference between the latest gen consoles and PCs. At least not enough to merit the $$ increase over a console - unless you're a die hard videophile.
Its just not worth it atm."
"People are using FSR 2.0 on their GTX cards because Nvidia told them to go fuck themselves.

AMD isn't your friend, but Nvidia is really, really not your friend."
"Disocclusion artifacts alone can reveal FSR 2  easily. Some door openings in Calisto were quite awful and I assume this game has the latest version. In the same HUB tests DLSS2 and newcomer XsSS XMX easily outperforms FSR 2. Mind you it's not bad, especially for the reach it has, but other vendors reconstruction offering are better. Dp4a XeSS doesn't exist. In my experience DF GoW analysis is still apt.

The question is how sensitive you are to those artifacts and how much you're willing to pay for better image reconstruction. Today I would not pay the same price for Radeon I will for Geforce if all things are equal. And there's no fanboyism in it. Geforce is just more well rounded product."
No drivers problems, for a start
Ah, you're right. So about a $70 to $80 premium on top for a new card. I see what you mean about the low end, then. The folks selling 1660s new clearly haven't got the memo that there are way better cards listed on the same page, going for like $30 more.
Yeah I mean, I am CPU bottlenecked, but I got a good deal on a RX 6650 XT this year, so I went with that. Just saying there are reasonable upgrades now. Just not from nvidia.
No, you won't. Since EVGA left the GPU market.
Like what were you playing? Was it all MMO/ trend multiplayer games?
Not a problem, I know the Series S gets downed a lot but really it has a lot of potential to be worked with, a ton of fun modding and such is open to gamers and hackers.  Same with Series X, just it has a bit more power.
Of course, wafers cost more but the silicon is the material which is why a 104-die or a 102 die cost the same for 6 or so years despite a massive increase in transistors.
That low tier card runs every modern game perfectly. You're a bunch of whiners.
It kind of does. Nvidia doesn't want to lower the prices of Ampere, they think it's okay to sell two year old cards at or above MSRP. This also means they can only release high end cards, otherwise they'd have multiple products at the same price point. Them wanting to sell Ampere and Ada at the same time, and then wanting to keep prices high are all related.
"I don't see it like that, then again, I'm the biggest fucking hypocrite in this regard. I despise people who pirate games but pirate movies and series myself.

I'm probably naÃ¯ve and foolish, but to me reselling a legally bought game ""might"" encourage the seller to spend the money they saved reselling it to buy more games with the knowledge that after they are done with them they are able to recoup cost."
"Yeah... Honestly, I absolutely love PC. My machine will keep running games well for years to come. But eventually there may come a time where Console gaming begins to be so much better that it makes me fully switch.

But there are a lot of things that needs to be dealt with, IMO.

Online play HAS TO BE FREE. That's just a no-brainer, give us other incentives, but don't restrict online play, jesus christ.

Better support for other resolutions. I've just gotten an absolutely bonkers 1440p display, but it doesn't look nearly as nice when I use my PS5. (Though, as I googled it, it does seem like there is support for it...)

But most importantly - keeping physical discs alive. If the PS6 goes fully digital I just wont participate."
"Not sure what version Calisto Protocol was on, but FSR 2.2 was a pretty major upgrade wrt artefacting. The video I was referring to, but didn't link because I was replying on a phone, was [this one](https://www.youtube.com/watch?v=w85M3KxUtJk), by the way, and they came out fairly even in that testing.

In the end they're all upscaling and making up the difference, so there's bound to be artefacts. Only way to have perfect image quality is to play at native resolution, which means that for quite a lot of games 4K is right out.

As for the GeForce vs Radeon argument, I'm on my first AMD card since their graphics division was still ATI. For me nVidia's shenanigans (like their insane pricing, twice now, and the mandatory GeForce Now account) combined with the fact that AMD is getting very close to nVidia's performance the pendulum swung AMD's way this time around (well, last time, technically, since I'm on a 6000-series card atm)

Next time around, we'll see, maybe it'll be Intel, who knows? :)"
"Yeah it's ridiculous. You have the 6600 at $250 and the 6650 XT for $275 currently but below that, the market is just broken. You got 1650s, 6400s, 6500 XTs, 1660s, 1660 tis, etc. all selling for within a few dollars of each other, with varying levels of performance. 

Like...the LOW LOW end market has ALWAYS been like this. You ALWAYS have poor GPU pricing at the low low end where you vary between say a GT 710 (bad) and say a 1050 ti (good), since that is now a 6 year old card and should rightfully be low end.

But normally....the NEW 50 cards are in that price range. And you always had good options in that price range. I remember when I got into PC gaming, you could get cards as good as the HD 3850 for like $130. You could get 4770s, 4830s, and 4850s for less than $200, and those cards rocked a lot of the time. You could get the GTS 250, and later the 5750 and even the 5770 (although that was closer to $200) at the time. Those were solid cards.

And later on, stuff like the GTS 450, GTX 750 Ti, 950, and 1050 Ti were all solid buys in the sub $200 range. You never got the best at that price range, you never got something super futureproof, but it was a price tier in its own right where you could get some value cards for around $150ish.

That market is drying up. The sub $100 market is just about GONE, we're selling 1030s for like $120 still last I looked which is freaking pathetic since that card should've gone for like $80 at launch. And we're still selling stuff like 1650s, 6400s, and 6500 XTs at that price range. THose cards have been down there for FOUR YEARS now, and there has been zero improvement per dollar. If anything, it's a regression. And with the $350-400 range being the new ""mid range"" and it seems like they're trying to turn the $200-300 area into the new ""budget"" category and the sub $200 market as that ""you're ####ed"" category, the market is just broken. 

To be fair i blame nvidia more than AMD at the moment for this. AMD has had good value deals on the 6600 and similar cards in the mid range range, which is quite frankly where they should've been all along (remember they cost $330-400 at launch), but ultimately, the low end of the market is feeling squeezed out, hard."
"As a matter of fact the more i think about it this is why im getting annoyed by this situation. We had this happen before. Nvidia actually tried to push insane prices back in like 2008-2009 with their GTX 200 series. Looking at MSRP they wanted like $450 for a GTX 2**6**0. And the 280? $650. 

What did AMD do? Release the 4850 for like $200 which had GTX 260 level performance. Then followed up with the 5850 in 2009 (which I bought, my first REAL GPU) for like $250-300, and it beat the 285. 

We're not competing like that any more. Nvidia is just raising prices by insane amounts and while AMD is undercutting them, they're not doing it aggressively enough. You used to have all of these sub $200 options which were GOOD. Spending $400+ used to be like the high end market. 

Now if you buy sub $200 you get like....1650s and 6500 XTs and stuff. 1660 tis are still in the $200-250 range. 6600 currently starts at $250 when it should be closer to $200 IMO. And yeah. It's like, these companies arent competing properly any more and have abandoned the low end market. These are truly dark times for PC gaming."
I dont know why it does honestly.  Find me a computer you build that can do what the Series S can for around 250 bucks, sometimes even less depending on sales.  You can't. You can't even really get close. Youd think PC gamers would recognize the value in that buts it no different than the stupid "console wars" back in the 80s/90s.  As much as some want to pretend it isn't, its that same exact mindset.
Factually incorrect and again no data you can present supports your opinion.
"We're still too early for the mainstream cards. The 3060 wasn't released until late Feb 2021.

The dip in sales has to do with crypto's crash and the flood in the secondary market. We saw it last time crypto crashed as well."
"Yes, it can you just have no idea what you're talking about and will be surprised when these things come down.  


https://www.youtube.com/watch?v=kJTX7TRB2G4"
 Thanks for proving my point. You can only regurgitate info you hear off the internet and have no hard data. Full of opinions and no desire to be intellectually honest with even yourself by doing the research. You my friend clearly do not understand economics in general as made apparent by your other posts. I think point alone has skewed your opinion on this topic and is blinding you of any truth that come your way. I wish you well
All of them are bad value for money. If RT matters to you, pick Nvidia.
"To be honest they're all terrible value for money.  I personally find only the 7900xtx to be worth it, and still that card should be like 750 USD, not 1k USD.

I watch a YouTube video a little while back and I personally agreed with the video.  He stated that for this generation, Nvidia and AMD opted for small production but for big profit margins, versus large production for small profit margins. 

 He said it's because Nvidia and AMD knew demand would be all time low for their GPUs this generation. Whether it was people being content with their last gen GPUs, or people aren't stuck at home anymore, and inflation being at all time high, their predictions where low for sales. The video also stated that the scalpers for last gen GPUs showed Nvidia and AMD that a certain portion of the community is willing to buy the latest and greatest, no matter the price.   So for this generation that's why they opted to do small production for big profit margins, versus large production for small profit margins."
The value? not even there. I'll gladly wait a couple generations and buy an older card when the time comes. RT on a 165hz monitor when $1600 can't even get that? No thanks, I'll leave it off for now and play 1440p 165hz lol
"""The only way to win is to not play the game.""

This generation doesn't offer a value for money at all in my opinion.  MAYBE the Arc A770 16GB, but even that requires a hefty leap of faith that it will get better with driver updates over time.  AMD and nvidia are both drastically overpriced."
"Not an obvious answer. RT performance in specific games sees the RX 7900 XTX on par or over the RTX 4080. In others, it falls way back. 

[https://www.techspot.com/review/2599-radeon-7900-xtx-vs-geforce-rtx-4080/](https://www.techspot.com/review/2599-radeon-7900-xtx-vs-geforce-rtx-4080/) 

Overall, the RTX 4080 performs in a more constant and predictable way in RT scenarios, while the RX 7900 XTX oscillates from very good to very poor performance. 

Does this justify the extra $$? Not really, considering it's already way over an acceptable price point."
As someone with a 3070, I can confidently say that RT is the biggest waste of time and performance. I can't even notice it.
"Terrible. AMD is the better value, but nVidia has better RT. Both are wildly overpriced and without any excuse.

You decide who the loser is, because nobody wins when things are this bad."
the 7900xtx is a year late to party had it come out in 2021 beating 3090 on raster and matching it in RTX for $1,000 it would have been  killer card , getting close to 4090 is some titles and having decent raytracing is cool and all but ill wait till its around the 700$ mark , I got a 6700xt to hold me back  not cool with normalizing 1k gpu prices.7900xt should have been priced at 699$ m  4080 should been priced at 800$ 4070 ti $650 there is no value in this gen.
Ray tracing will only be worth it once we have true photo realism in games and mid range GPUs that can handle it. Ray tracing only works for high end GPUs which are overpriced. People who bought the rtx 3080 for example won't be playing 1440p with ray tracing enabled for long thanks to the limited 10gb vram. If you're buying a GPU for ray tracing you're good for like 2 years and then after that the games become too demanding.
"I really hate where pc gaming has gone. The prices are astronomical, my rtx 3080 suddenly lacks RAM, upscaling is being used as an excuse for lazy programming and PC ports are shit most of the time (often because of denuvo). I find myself using my PS5 over my PC.

If prices don't come down than this gaming PC is my last. I am not spending car money on a pc."
Don't sleep on Intel. Drivers have a long way to go but getting closer every day. For value of your dollars the Intel is hard to beat. If you have deep pockets than the Nvidia is the way to go.
For RT itâ€™s Nvidia all day. No question about it
If you want the best RT then 4090. AMD is at least a generation behind in ray tracing.
Weâ€™ll all of the rly good ones are too expensive but if your comparing the lower of the new than the 7900 xt is a much better deal. Also 7900 Xfx vs 4080. Weâ€™ll 4080 is a little better for 400 more $. So I think amd is way to go unless you want the top of the link 4090.
My thoughts are buy what you want. Everyone values things differently. For some having the best of the best is worth the premium. For others, if the price goes up for a similar category of product, its not worth it.
[deleted]
"I mean the RAW rtx performance increase was enough for me to justify switching over to a 4080 as well as another dual 3090 rig for the sake of better thermals and faster renders for less power. 

But iâ€™m talking about redshift, octane, UE work where the difference in unreal editor is much more vast than in a ue game typically. Also good for some AI training and what not. I still have a and gpu and open up unreal or games without much a headache. really depends on if you can get an roi. If not how much can you responsibly afford for leisure and go from there. 

Both can get the job done. feel like pricing and availability on specific models has been rather annoying. my 2 cents"
For ray tracing Iâ€™d go 4090 but I donâ€™t use RT and then the itâ€™s a clear choice the 7900xtx is a BEAST and beats the 4090 at call of duty warzone once I saw that I was sold
How will this affect game development, if the general population effectively skip a new generation of cards? Will it have zero impact, or will developers have to be mindful of lower vram etc when designing games for pc?
4080 for best gaming or a used 3060ti
"All are a horrible value. Just a couple gens ago, a 70 series card cost ~$300. Now, a neutered 70 series with insufficient vram and memory bus costs $800. AMD isn't any better and it seems they're working together to gouge the GPU market after seeing miners paying insane prices for cards. They seem to think that demand will translate into the general gaming market, which it obviously does not.

Personally, I refuse to allow myself to be gouged, regardless if I can afford it or not. If more people followed suit, we wouldn't be in this situation."
Used market? They are all so expensive
Ray tracing? Intel. A770. Kills the 7900xtx.  1/3 the cost. Or wait for next gen battlemage
Drop some change on a used last gen unit, and in a couple years get whatever new card thatâ€™s not kneecapping everyone
nVIDIA is better for RT, but there is no good value in current gen GPUs. Last gen's RX 6000 series is about the best value going right now.
4090 is somehow the best value for money this gen. Let that sink in =_=
Iâ€™m more concerned about Taiwanâ€™s hold on the chip industry, an invasion from China will collapse half the world as chips are no longer being shipped
if you want rt go nvidia, if you want realistic gpu pricing go amd
"I will never understand Ray Tracing lovers, at the moment it's hardly noticeble in any game aside from Cyberpunk.
Maybe in 10 years it will actually be worth it."
Raytracing impacts performance far too much to justify the thousand dollars it costs for any good GPUs
RT really isn't a big thing right now. No matter the RT capacity of the card, ask yourself if the performance hit you will inevitably take is worth the extra money.
"I'm not really at liberty to comment in depth, all I'll say is I'm playing through resident evil 4 right now, pretty much everything maxed out, RT normal settings, FSR off, TSAA instead of that plus FXAA(can't tell the difference tbh), 100% rendering, and hair settings at their lowest, and my 7900xtx is putting out a stable 120fps at 1440p. Just adding a little testimony.

In general I went with the 7900xtx because I decided RT wasn't at the level I was willing to factor it in, nor did I want to pay hand over fist for the 4090 (even the xtx was a bit much), but I'm pleasantly surprised by what this card can do, given the competition."
For RT Nvidia. AMD simply can't compete on that front just yet they're about a generation behind. The 4090 despite the cost utterly curb stomps everything else when it comes to RT performance to the point where even at the bloated price tag it looks "compelling" simply because everything else is priced so badly in comparison.
A770 is pretty decent value. It might be worth waiting for Battlemage to come out assuming the value is similar.
Bought a 4080 last week, and Iâ€™m very happy with it for 1440p gaming and some 4k gaming in my OLED tv. Certainly very pricy, but hopefully it will last me 3 years or so. Itâ€™s my main hobby so I donâ€™t mind spending a bit of money. Other hobbies can be very expensive as well :)
7900xtx, 4070ti are the only gpus id pay for
"For most games besides Control, metro exodus and maybe a few other titles. RT is either poorly implemented, poor optimized, or both. That said I don't mind playing at 60 fps with med-ultra settings with RT enabled and DLSS scaling on. I love how the light looks. Since most games with RT isn't an esports title I don't really care about getting 144+ fps. That said you are giving up smoother overall game play and potentially worse looking textures etc due to the up scaling. Although It's getting hardly noticeable at 1440p nowadays with up scaling on.

I've always held that if you're paying top top dollar for a GAMING gpu ($1000 usd or more). Then it's gotta have all the bells and whistles since you aren't really considering price to performance or overall value as you don't really care for that if you're dumping a grand or more on just 1 component for your PC. So I've always been team NVIDIA in that regard because they just have a better overall product, with the option for RT there. Pretty inexcusable to have far inferior drivers and terrible RT performance on a $1k+ card at this point where we are the 3rd generation in for the RT technology."
Very happy with my 4080 so far.
There id any point of buying 4x series imho, optimal is 3x series, and wait until they come with better solution, for size of the card and its consume. Buying 4x card today seems a waste of money for me.
Me personally Iâ€™d go with a RX 7000 or better a good budget RX 7600 at that.
They all suck at RT except maybe the 4090.
Bad. If youâ€™re spending upwards for $900+ for a GPU, might as well go the NVIDIA route.
RT on AMD gone.
Nvidia all the way. Fuck AMD
Wait till RDNA4
If you really want to enjoy ray tracing at higher resolutions and frame rate, then you need at least a 4080. And those are hot garbage for value. So is the 4090 but you arenâ€™t allowed to say that on Reddit.
Got the 4080, its just eat whatever I throw at it. Idk much about value, but its good stuff.
Put it this way. Get a 4080 or 4090 and use it for 5 years. Then it's only a few hundred a year. Most people spend more on drinking or gambling. Who cares about the price if you care about gaming.  The whole pricing thing isn't a thing anymore . Time to move on and face the facts.  Wringing about a graphics card price isn't going to fix anything
I mean, if you're looking for RT I think you have to go for Nvidia.  I don't think AMD has RT yet.
If you got it, spend it. You will just want it anyway. Mooreâ€™s law is dead thinks the 5090 series will be double the performance of the 4080. So maybe wait if youâ€™re wanting better value.  But you got to pay the tax or wait for some leveling.
I think theyll be more reasonably priced when the next generation comes out.
"I feel like I lucked out getting a 2060 below market value and then selling just as Covid pricing peaked. I over paid for 3080, but the amount of money that I sold my 2060 for made it not that big of a hit. 

But thatâ€™s also the reason I wonâ€™t be upgrading for probably three cycles"
Why would you buy either of those
Not great, depends on what price you pick them up for. I managed to pick up a rx 7900 XT for roughly $800 US, so while not terrible value, itâ€™s still not amazing. Closer to $750 would have been great. $700 would be a no brainer.
Like everyone else says, forget RT. It only looks good when youâ€™re looking for it.  When youâ€™re actually *playing* a game it does nothing except lower your framerate.
"I have RTX 4080. Donâ€™t waste your money. All the RTX hype was a huge let down for me. You may notice it few times but then forget about it. Plus it hits your FPS pretty hard.

The only redeeming factor is dlss frame generation. If implemented right it may be a game changer in the future."
"Get a used RTX30 or RX6000.

There, I solved the value problem."
May not be popular but 4070ti and  7900xt imo. They both rival in gaming and price to performance imo.
RTX 4070 Ti is a good one if you want RT.
I experienced Frame Generation today on my 4090. This feature is amazing definitely go RTX.
I don't have a desktop rn and was looking to get the 7900 XTX for the value per buck. I can afford it but doesn't necessarily mean I'd like to pay 1000-1200 for the card. Do we think there will be a price drop soon? It'd be nice to pay 800 or 900 for it instead.
Always nvidea for ray racing but of you only care about rasterized performance go amd
Bought a 3080 this year and will wait for 40/7000 to come down to around 600 or less before i upgrade again.
Nvidia has become too big and now we have this shit
Both are very overpriced atm. Your better off upgrading to/sticking with a 30 series or RX 6000. If your upgrading from an older card and don't care about ray tracing and aren't streaming. The 6700XT-6800XT is the best deal for GPUs right now. Over on the Nvidia side Id have to go with the 3060Ti. All those cards I mentioned are beast at 1080p and 1440p with 4k 60 potential as well, and all are easily future proof and will last you around 5 years before you start having to turn some settings down.
There's no good value cards out. But as others pointed out, if you care about RT, then go for Nvidia. While AMD upped their RT performance, it's more or less on par with 3080
I run a 60hz 100 dollar ONN monitor from walmart so my RX 6800 runs fine :D
RT isn't worth it yet. Except select few titles, you won't see much.
"My 4070ti seems like it was a good choice performance wise for what we are now asked to pay. 

But if Iâ€™m being honest it and itâ€™s amd counterpart are overpriced by about 300$ in my opinion. 

If your gonna jump though. Nvidia has better rt, great performance and way better video encoders than amd at the moment. If you do vr itâ€™s pretty much nvidia or donâ€™t (unresolved driver issues on amd rx6/7000 cards)

I had both for a bit If it matters. Went with nvidia for eat, dlss, and nvenc."
40 series has frame generation so it wins just for that
Depending from wherw you are, where I live a 6950xt is 800$ so i would take that every day over the 1000$+ 4070ti and 7900xt. But that depends on you, if u wanna pick with your wallet or not
Have a 7900xtx with a 5800x3d, and itâ€™s absolutely amazing. Beautiful graphics at 1440p with high fps. Never seen temps higher than 68Â°. And itâ€™s much cheaper than the 4090/4080. If you care about ray tracing I guess it may be worth the extra money. If youâ€™re looking strictly price to performance , amd all the way.
Rtx doesnâ€™t matter like I planned to use it when I built my pc 2 yrs ago and still havenâ€™t sure dlls is good and all but nvidia is over priced rn and I currently have a 3060 ti and if I am going to upgrade which Iâ€™m not Iâ€™m would get a amd card
If you want RT and best value for money now, get a RTX 3000
[removed]
"Depends on your budget

- RX 7900 XT seems to be a better value than the RTX 4070 TI. 

- RX 7900 XTX is supposed to be 200e cheaper than RTX 4080. But it is not the case in europe, only the reference card that suffers from overheat and coil whine. Good customs RTX 4080 can be found at the same price than 7900 xtx customs. They perform almost the same in reasterisation but the 4080 is 20% faster than 7900 XTX, runs cooler and consums less. Then if you can afford it, go for the 4080. 

Prices are getting lower and lower. I would wait few weeks/months to see where it goes"
"Get 7900 xt sell it 2 years later and get whatever card you want. Doesnt make sense to go nvidia unless it is used. See all 3070 8gb etc cards are useless now due to low vram 


If you will go nvidia you have to buy 3090 4090 etc."
"All current pc hardware is a bad value right now if you ask me. 

Crazy how performance keeps going up from gen to gen yet consoles continue to stay the same price. 

A series x or ps5 has it but for some reason pc hardware is different and I donâ€™t know why. 

I rebuilt last fall with a 13900k/4090 but doubt Iâ€™ll be upgrading it again in the future unless games start getting a little more love on the pc side. 

Long story short in my opinion there are no reasonably priced pc parts anymore, there is no budget options and QC is a thing of the past."
It's not difficult, buy last gen, but don't spend more than $700
I think it goes without saying that the value per dollar from both companies is far from where most of us are comfortable. That being said, if you yourself place high value in RT as a feature, then you'd be well served purchasing any of the current Nvidia 40 series line, which at this point would be the 4070ti and up. As insane as it may sound, I think the 4090 actually technically may be a better value for the performance you get, but that's an astronomical amount of money and is in itself only good value because of how performant it is and how badly everything below it is priced. For RT, you really should get Nvidia. They've invested heavily in it, so its in their best interest to continue to iterate on its acceleration technologies.
"Imo a 7900 xtx at $999 is the best value for money card currently, that said it's still too expensive. 

It's just that it's not that far behind 4090 for some raster stuff, has 24gb of VRAM, and it keeps up with 4080 in a fair amount of RT stuff as well."
Buy an used 3070 ti for a good price
"If value/budget is a serious concern your only real option is used Nvidia.

Personally I wouldnâ€™t even consider new AMD at its price point.  When Iâ€™m buying a premium card, itâ€™s always going to be the one with the best support and most developer buy in, which is inevitably Nvidia.  

AMD is also mostly worthless for AI applications if that matters to you.

4070ti is also a decent value imo if you donâ€™t need VRAM for whatever reason"
"Honestly none are great for value as of right now. To expensive but they do pack good performance. 

The 7900xt dropped $100 so you can pick one up for $800(much better than before imo) but still over priced.

But for future proofing I would say as of now, AMD is the better choice when looking at Vram alone"
"I bought a 4070.  Same if not better performance, not insane issues with drivers being reported.  Same money.

  I'd only buy a ATI product if it was priced well below NVidia."
Their plenty of quality cards out there from the 30 series.
RTX 4090 is the best value. NVidia has propped that as the best performance/dollar so that everyone gets that instead of the poorly priced RTX 4080 and RTX 4070Ti. AMD has stupidly supported NVidia and made their cards a horrible value.
"Our friends at Hardware Unboxed made a video on this exact topic: [https://youtu.be/9kiOLC2Ca\_I?t=549](https://youtu.be/9kiOLC2Ca_I?t=549)

TLDR: cards with best value (cost per frame) are Radeon 6xxx atm."
I would just completely ignore RT at this point, even on the latest gen cards till they make it run more efficiently.
"This.

The GPU market is no longer pegged to any notion of value for money or reason; it's now effectively a monopoly controlled by NVIDIA, and you'll pay whatever is demanded of you.

Go with NVIDIA for ray tracing, their technology is more mature and better supported."
Exactly way over priced on both sides smh
Sadly this too will just stroke Nvidia s greed coz they'll see people are still willing to pay for it!
">If RT matters to you, pick Nvidia.

Or encoder capabilities, or VR, or idle power consumption on high refresh rate and/or multi monitor environments."
To be honest, at this point, I don't know why there are still so many people who believe that RT doesn't matter. Literally any RTX (and some AMD) GPUs can do RT, some may just need some upscaling.
dont be so harsh, jesus.
A 7900xtx is best value, but I had a 6800xt for a brief while and had poor performance with drivers on ue4 games, if its mainly triple a games u shoukd he good with amd, but indie titles have less optimisation for them.
"Which hopefully means the RTX 50-series will have decent cards at decent prices once more. 

Ironically the RTX 4090 is the best deal of the 40-series if you were already dead set on getting a Titan-class card, considering the fairly massive performance uplift over the 3090/Ti at the current price point of $1600."
The next year will decide if GPUs pricing will have any hope of coming back to reality. A lot will hinge on Intelâ€™s lessons learned from ARC and how they price Battlemage, which supposedly has hardware and performance between the 4070ti and the 4080. If they decide to go for market share again, and price it below or around the 4070ti, there will be hope. It will be up to us to upgrade at that moment
"Here in Costa Rica a Red Devil 7900XTX is $1,600.

A 4070ti is 1,300 for an Aourus version there are some cheaper ones like base MSI and Zotac.

Terrible prices. One of the biggest stores bought SO MANY cards and now they can't sell them :)"
The 4070ti isnâ€™t a terrible value imo. Considerably cheaper than the rest and still puts out great performance
The only value is AMDs previous cards. A 6700XT for $330 is solid buy.
Have you heard of Battlemage? It is Intelâ€™s next card. It is supposed to be a lot better than the other two. I am glad intel is in there too. Maybe with more competition we can get these cards back down in price.
I'm talking about games with good (and more serious) RT implementations, such as Control, Cyberpunk, Minecraft or Fortnite (UE5 Lumen). Not like Shadow of the Tomb Raider with only RT shadows lol. Of course 7900 XTX can perform better there.
In games with raytracing, I'll enable the option and I really have to look hard for the differences, aside from the fps drops. I haven't seen any games where raytracing has made such a big difference that I decided to keep it on.
"Ray traced GI and reflections are both amazing in any games that support them. I think the biggest problem though is most games with ray tracing just have a single effect or 2, full path tracing is far more impressive. 

You will start to see this more as game engines transfer over to a ray traced rendering pipeline, UE5 already has with Lumen which supports hardware and software ray tracing. Games just have long development times now so we haven't gotten a chance to see many games built from the ground up to support the technology."
3080, and same. I don't even bother turning it on.
tbf, GI can do wonders for indoor areas, as well as reflections, but yeah, nothing is worth dropping below 100 fps for that.
"Itâ€™s most noticeable in older titles like Minecraft or Half-Life. Never realized just how much lighting can make a difference in games until I saw Half-Life with RTX.

I played Spider-Man with RTX on for most of my playthrough and I never really noticed a difference unless I was really looking for one."
You might not be waiting long, Iâ€™m on the 6750xt and looking to grab an xtx and watching prices. If your in the us the xfx card was available for $980 last week. Doesnâ€™t sound like much but thatâ€™s how the 7900xt started before falling to $799 as of now. Iâ€™d give it a few months and itâ€™ll probably be available around $900. $700 is ambitious but the 6900xt hit that price before rdna3 announcements and also had a $1000 msrp so not impossible if your willing/able to hold out like I am.
"I used to think that ray tracing was a gimmick, but now I have to say that it's superb at breathing life into older titles.

I'm only on a 2070 super (in an eGPU setup), and it's more than enough for raytracing in games like Half Life and Portal. On Crysis 2 remastered I got 60 fps with DLSS enabled, which is less than ideal but more than playable."
And true photorealism is inaccesible without ray tracing. Also any RTX or RX GPU can handle RT at the appropriate resolution and upscaling settings. Just don't expect an RTX 3050 to do RT at 4K native.
">  rtx 3080 

Most people knew 10GBs wouldn't last long at 1440 and above. I hit 8GBs @1080p when RE7 came out in 2017."
You are comparing a machine that can only play games to a full fleshed PC. You can do a million more things on a PC itâ€™s not even comparable.
Did you see the 70,000 render machine that LTT made a video on? It has 4 1200 watt psus, it is all liquid cooled has 5 rtx 3080 or 3090s cant remember which and two AMD EPYC processors. The thing renders what took a regular PC 3 minutes and rendered in 10 seconds. The funny thing is they showed the wattage draw and it was over 2200 watts when rendering.
4090 isn't needed if you're not gaming at 4K. Personally I'm leaning towards 4070 Ti territory for RT value.
No A770 doesn't kill 7900 XTX, what are you talking about lol. It kills the 6700 XT.
tbf if u compare to prev. flagships in QHD Gaming..  it seems like you basically get \~20% more performance for \~15% more $ with the 4070Ti and \~25% more perf. for \~20% more $ with the 7900 xt
people can have their subjective taste
It's noticeable in Minecraft, Portal, Control, Dying Light 2, and many others. That statement is incorrect.
There are surveys online that show most people turn RT within 10 minutes of playing a game.
Any game with RT reflections except FC6 = noticeable and good
Do you like frame generation?
4070 Ti is also $800
I am looking for price to performance, just in RT.
Well as a matter of fact I already have it. It's just my dad wants to buy a PC for himself to play warzone 2 with me and I thought maybe I can persuade him to buy me a 4070 Ti (or 7900 XT, whichever the comments suggest) so then I could give him my 3080. Then we both will have an enjoyable experience
As of right now, the 4070 Ti is cheaper than the 7900 XT and beats it in most RT scenarios. So you're not really paying a premium.
I think they fixed the overheating and coil whine. And I'm not interested in rasterization performance.
Maybe the budget options will be released later this year. The RTX 4060 and 4060 Ti are rumored to come out in May.
And go used for bargains
But there are numerous reports of 6000 cards having low FPS in RT, and I specifically asked for RT cost per frame, not rasterized. I know full well about the value in rasterization. Does that claim still hold true in RT gaming? Please tell me I want to know.
But their company is predatory in their pricing and then on top of that the 4080 came out and Iâ€™m pretty sure tons of people ran the numbers and it should have been labeled as a 4070 or 4060ti or something similar. Screw them. I went from 1080 to 6950 last month. Was initially planning on 4080. But nvidia can suck it now
"TL;DR Neither, both companies suck. Go Nvidia if you enjoy spending money.

It's collusion by Nvidia and AMD. AMD is more than happy being little brother to Nvidia in the graphics card space. They could be fighting (and winning) for more market share but announced they cut their chip supply to keep prices high.

I think gamers might be hosed. AI compute power can keep the demand up for GPU's. Nvidia will divert a pittance of high end cards our way if you've got the cash.

If you just want to game, I think I'm going to start recommending consoles. There is no chip shortage, GPU crypto mining is currently dead. There is no reason for the current lack of bargains other than a giant middle finger from AMD and Nvidia. They have us by the balls, and all we can do is walk away and bitch about it on social media.

There is one glimmer of hope. The used market is finally coming down. I saw 3060ti's on eBay for around $230. That's a legit bargain, IMO. I've been waiting 3 years for one. There was also literally a few minute window (almost as quick as the darkest times, where inventory sold out quicker than my browser could refresh) where Best Buy in the US had 3080 FE's for $415. That specific model is almost certainly gone, but keep an eye out for similar fire sales. Warehouse space must be cheap because there is a glut of last gen cards that haven't sold and they're happy trickling them out as to not get our hopes up.

If you can't tell I'm pretty jaded, but they haven't kicked all the hope out of me yet. Things cost what they cost, and I barely have time to play anymore. They've priced me out of the market, and anyway I'm too busy working to afford all the other crap that has shit (that was a typo for 'shot', but fuck it, I'm going to leave it) up in price. Half the fun for me has always been bargain hunting and building or upgrading to the best bang for the buck PC. I couldn't name you a value champ right now."
How is it a monopoly of Nvidia. AMD is still there. Most people on the internet love to shit on Nvidia and suck up to AMD, but when it comes to it most people just seem to be upset  they can't afford flagship Nvidia cards. Go buy an AMD card if you have a problem with Nvidia and stop whining about them. Or better yet, just keep using your present GPU, because I'm sure it still lasts.
All of the things you listed are also non-issues on AMD GPUs.
Hardware unboxed just did a video on how AMD is better for multi monitors than Nvidia today.
Nvidia doesn't pay you to spout lies
"> I don't know why there are still so many people who believe that RT doesn't matter.

Probably because they've tried RT.

I'm firmly of the opinion that we're still 2 generations away, at least, from RT being a necessity. You give up too much performance, and don't get enough extra visual fidelity in return."
"Honestly? RT doesn't do enough. Like if I had the option to enable it and still have 120+ fps then sure, but it's not really that big of a difference from baked lighting right now. Reflections are a big visual update, and they're gorgeous, but other than that everything is just eh. 

&#x200B;

Eventually tho I hope that RT becomes the norm."
"RT isn't used in any game changing ways outside of one or two titles. 

Until there is broad support for game changing RT it will be an also ran feature that doesn't matter."
It depends on what you mean by "doesn't matter". RT is amazing and some games look really good with it right now, but for some people the performance hit still isn't worth it, even with the 40 series and DLSS. Ultimately RT doesn't matter in the same way that maxed out graphics settings don't matter, most people would rather just play games without spending $1000+ on a GPU.
"1. Not every game looks noticeably better with ray tracing enabled. I can think of 2 games that implement raytracing effectively, every other game either does not have raytracing or does not implement it well.

2. Not every game can run ray tracing without crashing on modern hardware. (Watch Daniel Owens recent video on RE4. He could not get the game to run with RT on @ 1440p with a 4070ti. A 1440p card according to Nvidia? lol)

3. You get a loss in frames and/or resolution for enabling it on modern hardware. A tradeoff which is going to be a hard sell for most people. 

4. RT is the only video fidelity feature that also utilizes the CPU. For anyone that is currently CPU bottlenecked in games, RT could cause significant performance drops as their CPU is the limiting factor.


I would equate RT to a 5% increase in fidelity for a -30% decrease in perormance. That is the average scenario with modern hardware right now.
The real question is why have you bought in so much to Nvidia's marketing that you see that tradeoff as beneficial right now?


It is clear to everyone else in this thread that the technology is 4 years away at a minimum from being viable due to the numerous setbacks and limitations of current hardware and software I have mentioned above."
RT looks mediocre at best. And it takes my fps from 100+ down to 20.
">  there are still so many people who believe that RT doesn't matter.

Cause I dont care about it enough to pay >$2000 when my 3060ti can play every game on high settings without it just fine"
Looks like there's a contract for a million dollars to downvote me for an opinion related question
r/literally
"The performance hit still isn't worth the visual improvement. If it's RT and 60FPS or no RT and 100+ FPS then I'm doing no RT everytime. 

Raster lighting tricks aren't photoreal, but they've gotten absurdly good and I'm generally not paying much attention in most high speed games. 

IMO RT needs a killer app game. A new ""Doom 3"" that really showcases lighting tech top to bottom elevating the game before I think a lot of people will make the switch."
I don't think he's being harsh
Oh no, did I hurt the evil multi-billion dollar corporations little feewings? I'm soooo sowwyyyy.
"Nah, the reality is that the prices that these companies are charging for what are primarily marketed as gaming products are just way too damn high. 

All the new-gen cards are subjectively bad value. Like, you could spend far less and still have a great gaming experience on a rig with a 5600 and an RX 6600 XT. Yeah, you'd want to be running at 1080p and without ray tracing but games would still run nicely and be fun. Or you could build with a 13700K and a 4090 and spend three (or more) times as much (for the whole rig) and games would not, subjectively, be three times as fun.

Of course, to a degree this has always been true. But it was easier to justify buying the higher-end GPUs when the price gulf between the mid-range and the high-end was $300 vs. $700 as opposed to $300 vs. $1600."
Damn son! That's as hard as I've seen anyone get smacked down in this forum. Was he talking crap before?
Dont count on it. It might take them losing billions in sales for them to change.
Depends where you live I guess, you can get a 4080 for $1700 in Australia, 4090 starts at $2900. Big difference in both price and performance.
Moore's Law is dead speculated next gen Nvidia cards to be more expensive to produce so Nvidia is teaching consumers to expect $1000 mid tier cards.
$1600? Please youâ€™re looking at at least $1800 with taxes and any other fees included
"It won't be *as* cheap, but it will be cheaper. 2000 series was a rip off in comparison to 1000 series.

Nvidia has had low sales that have impacted investor confidence; they'll need to come out with a banger that everyone wants to buy and can.... but still, the actual GPU's are more expensive to produce."
It is probably the worst choice out of the 4 new cards that are available right now unless you are gaming at 1440p. 12GB of VRAM is barely enough for 4K and some games it isn't enough. So in the future you will be in trouble if gaming at 4K.
Yeah, I am cautiously optimistic that Battlemage will be better hardware and their software/driver support will be more mature by the time it releases.  I think a 3rd player in the game can only benefit consumers.  Right now nvidia's lead has lead to ridiculous pricing.  AMD's pricing isn't much better unless you are willing to buy older generation hardware.  I miss the days of being able to but a current, competent GPU for $300ish.
I'm just happy there's someone stirring up competition. I wish them luck and indirectly I wish all of us a better future.
Java Minecraft doesn't support raytracing, and bedrock Minecraft isn't worth playing due to feature disparity :p
And I'm also taking into account other features such as DLSS 3.
Cyberpunk with RT shadows was a HUGE quality boost, but it's still personal preference.
There is a difference if you stop and look at the building windows
It will , remember the  6950xt was released last summer at $1,100 and in less than 6 months went down  to $700.
Bruh that's a fact why are you downvoting me lol
"They already have a pc with a 3080 in it.

It is actually criminal that it is over Â£1000 cheaper to outright buy a Ps5 than it is to upgrade 1 PC component"
He also has a 3080...Which is still great. I believe the PS5 is the better buy in a lot of situations.
Yeh, imagine buying a graphics card to play games. Absurd !
"yeah i was low key drooling over it! Back when the 20 series launched 4 2060s would be cheaper and more powerful than a 2 2080ti system for octane. Though ngl Iâ€™ve been getting away with doing a lot of things at half res and then using ai to upscale or get the job done faster in other ways. 

I canâ€™t justify something like that yet but itâ€™s would be a power move to have enough people working with me to use on as a god box. 

I like what EK is doing with the workstations they sell with quick disconnects. Cheaper than the mac pros i see people some picking up."
I game on a 3k 21:9 (so pretty much 4k as far as Pixel count goes)
"It isnâ€™t noticeable enough for the performance loss.

That is the point.

RT makes reflections look gorgeous, but losing 50-100fps depending on the game, not even remotely worth it."
There are surveys online of people not knowing they have a 144hz monitor after years of using it on 60hz, some pc gamers donâ€™t even know what theyâ€™re doing half the time
I hear Doom Eternal has good RT ðŸ˜€
Thats why I said depending where u live, u know the prices aren't the same all around the world...
"Hope he tells you to save up and buy your own hardware if you want the new gen every year.

Warzone 2 recommended specs is a 1060. You got a 3080 that gets fantastic performance at 1080p. Boohoo it doesn't compete with a 4090 at ray tracing, that happens literally every year with new tech.

 Maybe instead be excited your dad wants to play warzone with you instead of trying to trick him into buying something *for yourself* that you don't need just because some YouTubers told you ""rt is required""."
Maybe when Nvidia and AMD release their lower tier GPUs that statement will be true. Personally I look at it like this: AMD offers good value and perf/$ in non-RT gaming, and Nvidia has better RT perf/$ than AMD. I just created this post to see if anyone can prove me wrong and also because there are some scenarios where AMD offers better value overall (both in RT and non-RT). I want someone to point out those examples to make me start considering AMD as a viable option.
[removed]
"Speculation is $499, thatâ€™s not budget pricing on a budget card. 

This was my point exactly, you use to be able to purchase budget cards for $150-200, you can hardly buy a new motherboard for that now."
Oh my bad, I misunderstood the question. Yes NVIDIA cards are better with ray tracing at the moment.
"But you're not mad that AMD made a whole presentation about their generational gains and released a GPU barely better than their last gen that gets 110c because they poorly designed their GPU with shoddy QC? Per Der8auer. Also overpriced although Nvidia is far worse pricing wise. 

This is just a bad gen for bargain hunters and GPU's that make any sense....just get what you need and makes you happy. 

And stay mad at both, they both screwed us.... Fuck picking a side..."
I went with a 3080, no way i am paying 1k+ for a gpu that should be 400-600
But entry level cards follow the price trend of flagships, this is why people are mad
It's collusion. Both companies should be criminally investigated if you ask me. Unfortunately, no one did.
VR performance is terrible on 7 series cards, look at reviews. Encoder is better on nvidia cards, that's a fact (AMD is usable too). Idle power consumption on a multi monitor high refresh rate systems is a problem for a lot now, but that's not a bug itself, it's by design.
I have a 4090 and I still rarely use RT, itâ€™s just not that noticeable and when it is it costs too much to matter
Why would I play cyberpunk in 1440p with rt on and get maybe 25-40 fps with my 3080 when I can turn it off and get above 100fps. Who wants to play anything under 60 fps.
That all depends on the game. In Fortnite for example, when you turn on Lumen, it looks so pretty I'm happy to accept any performance loss. Plus, if you're so adamant about performance, you can always gain it back using... drum roll please... upscaling. Especially with frame generation and whatnot.
The GI is also good, if you look at Fortnite, Minecraft, or Portal RTX
There isn't really a tradeoff in resolution. DLSS looks good enough nowadays and since we're talking about 40 series here, you can enable FG and get EZ 100+ FPS. And to those who are CPU bottlenecked, that's a PC building skill issue, not an RT issue.
There are plenty of games with good RT implementations that I have mentioned in various replies here. And the performance hit isn't that drastic. You're overblowing it.
yeah I can't fathom paying $1100+ for sub 60 FPS.
3060 Ti can handle RT just fine with some DLSS.
You can downvote all you want, just don't call me a troll afterwards.
I think those killer apps exist, in the form of Minecraft/Portal RTX and the upcoming Cyberpunk RT overdrive mode. IMO, the people that claim that RT isn't worth it either play the wrong games or own the wrong GPUs.
Lot of guys here simp for huge companies and for executives paid in stock. I'll never understand it.
surely not, but you might have gotten some ppl who didn't think twice about spending $1k on a GPU a little hot and bothered.
Things are so bad my 2080 purchase back in 2019 makes sense
Also I predict another shortage. I think people will buy them in droves again next generation
Also I think AI is going to fill the crypto void.  Consumer card sales may not be as high of a priority.  So they can sit on their hill for longer without dying.
And Nvidia is doing everything but losing billions. So I wouldnâ€™t expect anything to change. I plan on just waiting and holding current hardware for ~5-6 years instead of just getting a new GPU every couple years.
1700 on sale right?
4K I can understand but 1080/1440 itâ€™s still a pretty great card. If youâ€™re going 4K though everything is going to be expensive no matter what it just is what it is, although I still donâ€™t think the current 4080+ cards shouldnâ€™t be as expensive as they are. I benched my 4070ti and I get minimum 100fps and a max of 155fps, 1080 max settings it shreds and 1440 high settings it still does very well for me at least
But 4070 Ti isn't really targeted at 4K. Nvidia recommends it for 1440p.
Anything other than the 4090 isn't up to snuff for 4k.
We need someone to sell a card for $200 that runs faster than the rtx 4090. Lol. Can you imagine that?
That last part is really debatable tbh. Java is better for like redstone stuff but bedrock runs better and has crossplay. Also bedrock has controller support.
">with good (and more serious) RT implementations, such as Control, Cyberpunk, Minecraft or Fortnite (UE5 Lumen)

I think there's a bit of confusion here. Those are all not exemplary implementations as they're all based on the RX platform that Nvidia developed.  Control and Minecraft in particular are poorly optimized when it comes to the ""public"" RT builds (and not the technology previews at launch). An acceptable benchmark would be a DX12U implementation that is not really too much unbalanced towards one manufacturer. Fortnite is the only one that is sufficiently neutral. The 7900 XTXC excels in that game.

DLSS3 shouldn't be taken into consideration when comparing RT performance, as it can be used outside of games too."
Dlss is only Nvidia. Jensen-like typing detected.
Cyberpunk is one of maybe two games where it really stood out to me. I can see it in others, but itâ€™s a lot more muted.
Yeah, but youâ€™re also literally playing Spider-Man thatâ€™s swinging past buildings all the time. You never really focus on them for more than a second during real gameplay unless you actively went out of your way to look at buildings or puddles on the floor.
[deleted]
Mostly because you are incorrect
Do end up noticing them while playing? Plus, many games use RT in limited ways like shinier puddles. Reflections on bodies of water are still mostly just screen space.
Oh right
But then he gets the 3080 which will give him max FPS
Oh I meant US
Maybe $399 for the 4060? I am hopeful because really there is no other option. If the lower tier cards are as expensive as the higher tier ones, then the PC market truly is fucked.
Youâ€™re not wrong. I guess nvidia made me more mad. And I was open in trying something new. It felt good not to give them my money though Iâ€™ll tell you that.
"I agree, be mad at both or neither.

I am on the fence to be honest. Nvidia does have qualities like actually innovating despite being comfortably at the top. Most companies in such position just hoard cash - i.e. would have produced slightly faster 1080tis for higher MSRP."
110c. Damn. mine only runs 55c under full load. both the 7900xtx and xt that i have running in rage mode. plus run better under 4k than the 4080 and 4070Ti that i tried
I presume the 4070ti was too expensive or not out yet?
I feel you. I got a good deal on my asrock oc 6950xt. 670 bucks new.
"> VR performance is terrible on 7 series cards

Terrible is a stretch. 7000 series has some driver issues in a few specific VR titles. 6000 series seems to be completely fine.

> Encoder is better on nvidia cards, that's a fact (AMD is usable too).

Splitting hairs unless you're a professional streamer, in which case you should probably be using a dedicated streaming PC with CPU encoding anyway. AMF encoder is more than satisfactory for most people. I agree NVENC is slightly better, and has more widespread software adoption.

> Idle power consumption on a multi monitor high refresh rate systems is a problem for a lot now, but that's not a bug itself, it's by design.

I'm not seeing this anywhere. I'm also seeing reports of Nvidia having issues with multi-monitor setups, not waking from sleep, having to disconnect/reconnect, etc. This is hardly an AMD-specific issue. It also doesn't seem to be very widespread for either AMD or Nvidia, so YMMV.

I hate to always have to be an AMD shill, but the AMD haters and misinformation-ers are just so incredibly annoying and wrong a vast majority of the time."
My 6800XT idles at 7 watts with triple screens, main one being 1440P 144Hz.
">I have a 4090

Proof?"
If you have a 4090, why are you concerned about performance loss? You probably already get over 200 FPS in everything lol. How much frames do you need?
Cyberpunk is probably one of the only games where turning on RT + DLSS is a no brainer. The game looks WAY better with RT. It's probably even worth sticking to high settings instead of ultra/psycho for everything else
Turn on DLSS balanced and get 75 FPS at least.
My mans just went on a campaign tour of downvotes lmao
"Upscaling comes at the cost of overall visual fidelity decreasing and in some cases artifacting unless youâ€™re at 4k. If thatâ€™s a trade you donâ€™t mind making then sure itâ€™s a decent option.

Many prefer native resolution (especially if shadows/lighting are done well by default) at higher frame rate than needing to upscale with RT as itâ€™s basically trading RT for resolution"
"I would think Fortnite would be the *last* game you'd want to turn on Lumen, considering the fast pace and competitive multiplayer nature of the game. It looks nice, sure, but you give up 60-100 FPS to turn it on. Not worth at all.

Upscaling introduces its own set of problems, especially in fast-paced multiplayer games. It's a good option, but it's not just free performance. You are sacrificing to use it."
I have a 4070 Ti and at 1440p, ray tracing only costs like 10fps. It looks amazing and is totally worth it.
What you're saying may be true for Shadow of the Tomb Raider for example (which only has RT shadows) and some AMD-optimized titles with seriously dialed back RT (like Far Cry 6). But then again, I still turn it on just for fun.
Not sure why OP is being downvoted he's totally right, I play fortnite at a competitive level but even I cant resist setting it to DX12 with hardware RT and lumen, the game looks amazing, and with DLSS still well over 120fps
"Fortnites ray tracing is noticeable because of the previously cartoons graphics and how the game wasn't built to have reflections on stuff already. A significant portion of AAA games already had reflections off of water, windows, stuff like that. 

Minecrafts completely replacing the current lighting engine, ofc it would look good. If you want a more fair comparison it would have to be shader packs vs RTX, since shaders usually use the same lighting techniques as most other games for shadows and stuff like that

Portal rtx is a 2007 game, ofc it would be significantly different

The main advantage of RTX is mainly game development side. You don't have to rely on any sort of fancy tricks to get better looking lighting, and it's pretty much as simple as placing a light and tweaking the colors and everything 

It's a practical combination and upgrade of things that have been built up over the years inside of various game engines. They all rely on similar techniques, it's just RTX takes it much further in order to be able to handle everything without needing additional coding"
"Frame Generation is not the complete fix you claim it to be. FG delays showing you a rendered frame to interpolate a fake frame in between 2 rendered frames. Yes this does increase FPS, but no it does not always equate to a massive boost.

Now, since Frame Generation delays showing you a frame this increases the games latency. Also since the frames are AI generated they are not created by the games engine so they are not responsive to your inputs and you will see things accurately later due to this delay. In other words for anyone playing competitive multiplayer games, it is a completely unusable feature.

Furthermore, since you were talking about games being ""photorealistic."" Some of the frames generated through FG look almost as good or even almost indistinguishable but sometimes the image quality can be noticeably worse due to the AI not having even enough information on certain parts of the image or the AI just guesses wrong and the end result is a frame that do not quite look exactly right. Probably not a feature I would want if I was interested in ""Photorealism""."
I didn't downvote any of your comments, but I'm going to call you a troll. Is that acceptable?
[deleted]
"i think it's cuz they bought in to the brand's feature list, so it's harmful if those features become irrelevant.  imagine if dlss went the way of physx.

apple has done this brilliantly, but nvidia's proprietary blah that's nominally better than any alternative at the moment is the same sort of thing.  

  shrug."
Yeah, buying a 2070 Super felt dumb at the time back then but now I'm very glad I did
"> I predict another shortage

You're not thinking it through. The  *O N L Y*  reason there was ever a shortage was from fucking mining. Mining is DEAD. There is a gigantic surplus of the current cards, AND Nvidia is manipulating supply by not producing cards, AND Nvidia is artificially keeping prices much, much higher than the complete lack of demand dictates. Nvidia needs to go bankrupt. 

There won't be another shortage unless Nvidia decides to artificially manipulate the market even MORE than they're currently doing. 

I build computers as a hobby, I build a LOT of computers. Anecdotally, people are *leaving* computer gaming in droves. I rarely get inquiries from people starting gaming, compared to for the past 10 years, more than half of my systems went to people getting into computer gaming. Now, no one.

EDIT: PS: I haven't had a single inquiry asking about a build with even a 30 series card, let alone a 40 series. They're just too expensive for just about everyone."
Yep, but no sales on 4090's
Wanna get your mind blown? Boot up Cyberpunk, set every setting to psycho (RT included), turn DLSS to balanced, turn frame generation on and enjoy the stunning visuals with 100+ FPS.
It's VRAM already isn't enough for some RT 1440p performance. It will have frustrating performance before the 5000 gen
alot of people who game at 2k can't even afford more, used 6700xt/3070 does the job for most people, why spend 3-4x on a 1440p card, lol.
"I am not asking for the moon and the sun here.

The 1070 and the 970 were both sub $400 MSRP.  I owned both (still own the 1070 because I have a 1080p monitor and it still handles what I throw at it).  

A 4070 for $399 shouldn't be a big ask."
"Once youâ€™ve played Java, you can never go down/back to bedrock IMO. Itâ€™s just so stripped of so many features that Java has. Bedrock is only worth it for the crossplay imo, if you have friends who donâ€™t play on PC.

Controller support is really only useful if youâ€™re not playing on a solid flat service like in the car. Other than that, most people will be playing with MnK if theyâ€™re on PC because Minecraft is a better experience that way."
"Yes true but I play for survival farms and redstone builds, both of which suck to do on bedrock :p

Also we're talking about raytracing here, I don't think we're concerned about performance differences XD

It's just my opinion. Bedrock has its place; I won't play it."
Do any Minecraft creators play on bedrock? They literally had to give the hermitcraft people new computers and sponsor them to get it shown off. And they all play on java still. The feature disparity is big and as I haven't played with a controller, I can't comment on it.
Outside of games? How?
DLSS is a shorter word than upscaling and easier to type.
"The poster child for RT 'Control' also looks pretty beautiful.

Other than that and cyberpunk the difference in other games isnt all that much"
You can (in theory at least) notice them while running up buildings.
I mean OP's technically not wrong on this one if they meant any modern GPU. A 2060 would crush RT at 480p
To achieve photorealism RT is needed. That's not false lol. How are you gonna render a realistic scene without even simulating how light behaves in real life?
No no no. In Fortnite for example reflections without Lumen look like shit. The reflections on rivers, mirrors and ice massively improve. In Spider-Man (both) the buildings go from reflecting fake things that are not even there to accurately reflecting other buildings.
And I never said I wanted a 4090
"At this point I can't tell if your trolling or not. You're really not going to respond to the rest of the comment?

Mods really should lock / remove the post."
"I do not like having this negative outlook on this subject but I believe you are correct, market is in a bad place for gamers. 

For the corporations producing this hardware itâ€™s a different story."
Fair enough, I wouldn't blame you for not giving either of them money.....this is just a bad gen despite some of the gains the 4090 made.... Hoping that this trend of GPU pricing and BS ends soon ðŸ¤™ðŸ¼
"I'm more mad at AMD. I expect ""screw you, pay me"" behavior from Nvidia. They usually have the quality to back up their arrogance. I'm a car guy (PC's are the toys I can actually afford). Ferrari is the same way. I've heard their dealers treat customers like a piece of crap (Jay Leno refuses to buy one), but they keep their product desirable such that people put up with it. 

That's a little too much hubris from Nvidia IMO. Panties stay firmly in place no matter how many slots a big girthy graphics card can fill."
The crappy Gpu sales might iron pricing out next generation but everyone is afraid of recession so we are getting milked in advance.
Imo its not worth voting with my money when im sacrificing my gaming experience for it. There is no alternative for rtx4xxx series if you want RT atm.
That's not much better than a 4070Ti/7900 XT for $800-850...
">Terrible is a stretch. 7000 series has some driver issues in a few specific VR titles.

It's generally slower, in a lot of cases it's slower than 6000 series.

6000 series is fine. Again, look at reviews.

&#x200B;

>I'm not seeing this anywhere.

lol, the net is full of the idle power consumption and it is by design, look it up (multi-monitor setups and max vram clock).

On the 7 series there is (or was) an issue with some single monitor high refresh rate monitors, there was even a known issue (or there is, I haven't followed driver changes recently).

But the max vram frequency with high refresh rate multi-monitor setups is by design since RDNA1.

And I have direct experience with this.

I never said there are no driver issues on nvidia cards.

I'm not an AMD hater, I've used multiple AMD cards in the past. I also have AMD CPU currently (and the previous one was AMD as well)."
"Welll, I have a 7900xt and yes, I get high idle power usage with my dual monitors (90-100W just on desktop with nothing open)

Depending on driver versions I can sometimes get it down to a more sensible 30-40W by fiddling with timing settings, refresh rate, etc, but which one works  and how well seems to be entirely random so far.

This is listed as a known issue on AMD's own download page."
Nah not at 4k & dlss quality w/ all settings maxed. I like to keep a stable 144 with amazing graphics. I get that with most games most times, but with RT off I always have perfect frames. Donâ€™t really notice it tbh outside of CP2077
Just turn it on and forget it lol
It does look amazing with it on but I didnâ€™t build a pc to get console fps. Iâ€™m probably just spoiled but I canâ€™t play anything under 60 fps anymore.
I agree. Control got me interested in RT. Played it 1080p and could notice the changes. Made me want to play it in 4k that way. Itâ€™s the only game other the. CP that you would wanna have RT in sadly. For that reason, Iâ€™m going with the XTX which is fine for 1440p RT like the 6k series was better for 1080p RT unless you MAYBE have a 6900xt.
Nearly every game I turn off dlss, it typically gives me a little fps boost but at the expense of quality picture.
More devoted to RT than 40K Imperium of Man is to their emperor
Well, can't live without downvotes on Reddit. If this keeps up, I might be banned from this sub LMAO. I already can't post or comment in r/pcgaming and this pretty much ensures it will stay that way.
Exactly bro !
Also the only game I know that has really good reflections without RT is Stray, which I think uses planar reflections.
DLSS is really good in Fortnite, I play at 1080p with DLSS quality and I still don't notice it
Yes but I still get around 100 FPS with DLSS quality (which is very well implemented in Fortnite and looks indistinguishable from native res), and it's not like I'm an eSports pro or something who needs every last frame they can get. I play pretty casually.
Re4 remake and Dead Space remake use mostly RT reflections and RTAO, now, when a game uses global ilumination it hits HARD
Really depends on the game.
Also Nanite eliminates LOD pop-in (which is especially noticeable in performance mode) and as a result increases immersion.
But the latency increase is very small, due to Reflex being forced on in the process. Also, it can boost FPS in CPU bound scenarios, as shown in MSFS. And the artifacting because of less (or inaccurate) information usually occurs when the camera is moving weirdly, like when Spider-Man jumps out of the window in the opening cutscene of Spider-Man Remastered. It's usually not noticeable or only noticeable for a split second and can be fixed by not generating frames in those scenarios. But of course, that's up to Nvidia to fix.
You can't buy that kind of brand loyalty. That's like magic almost.
Then I must be a fusion of Strix and EVGA lol
"exactly. First iteration of RT games suck and DLSS sucked too. We were overpaying for a 8gb 1080ti. Now? DLSS doesnt suck and some games implemented RT properly and its 60fps playable on native resolutions.

Hindsight is 20/20 but goddamn, turing ended up to being a great generation. 30 series was unobtanium at MSRP and the 40 series is just overpriced."
"Well over 20% of people on the Steam Hardware Survey have a 3000 series GPU so a few people might have purchased a computer outside of you.

Think it's probably more people are building their own PC or going to bigger pre-built companies to get their gaming PC's these days."
"Even a short term memory of the past ~5 years would let anyone see a pattern of rising and sinking cryptocurrency values across a variety of coins. As value goes up, so does the incentive to mine, which means more GPUs being bought by miners, so on and so forth. You're also completely ignoring scalping which played a major role in the shortage of moderately priced cards. 

Also, building computers is easier than ever. Someone could flip on a YouTube video and do it themselves in a day. Less than that even. So maybe the reason you're not getting inquiries is because people are just throwing on a guide from Linus Tech Tips or something and doing it themselves. Its a brave new world out there, time to pivot."
Youâ€™re aware that there are a load of different cryptos one could mine. One just needs to pick up value again. Like how Bitcoin did and then ETH. Not saying it will, but I wouldnâ€™t rule it out entirely.
"""Mining is DEAD""
Lmao"
We need GDDR7 fast
Yeah but those are in unoptimized games like Hogwarts Legacy. Man if they aren't optimizing performance (relying on DLSS to save them) at least optimize VRAM usage lol
"Apparently according to this site, inflation is 25% since 2016 when the 1070 came out: https://www.usinflationcalculator.com/

So we should expect around 500, which is still way lower than what people are expecting the 4070s to sell at"
"You did not account for inflation. 

$400 in May 2016 (when the 1070 was released) is worth $500 today...

Yes it is overpriced, but you should be realistic."
That would be a perfect price for that card
I have both and still play bedrock, I only play Java when I want Hypixel. Redstone isn't really a concern to me because I don't know shit about it.
"Most rendering engines support GPU accelerated RT (via Tensors, not rasterization conversion) nowadays. I use it a lot in my Autodesk suite to make the designs I create more realistic/appealing. It's one of the first thing Nvidia implemented 4 years ago.

Blender Cycles supports it too. With OptiX denoiser too. 
https://youtu.be/itNWdv2Fu-0"
Also the RTX platform doesn't mean anything, it's just branding. They all use DXR for the actual ray tracing which isn't biased towards any brand of GPU, it just depends on how good a cards RT cores (or ray accelerators in AMD's case) are.
...you said dlss 3...
Yeah that's about right. It is what DLSS is doing after all.
Light simulation has existed a long time before ray tracing. Itâ€™s called path tracing, and runs on regular gpu cores
And that's just it. Yet, when you're playing, you don't notice them that much. RT isn't worth it. There are more important things right now than a technology in its infancy. Maybe 6000 Nvidia and 9000 AMD cards will be strong enough. I traded my 3070 ti for an RX 6800, and all I can say is that I'm happy I did. 3070 ti may do better RT, however meaningless and insignificant, 6800 is better in raster. Plus, due to higher VRAM, it does RT better in some games where 3070 ti can run out of VRAM.
Ok ok I'm excited. I honestly think this PC buying thing won't happen anytime soon, because currently other things are on top of our priority list, such as going on a holiday in Norway for a few days.
Yeah man, I'm looking back and seeing what a fool I was for telling people to wait for 40 series. I expected a max $100 price increase across the board, this is just insanity
Wont happen i think. only prices up to the sky from here on
"The 4090 is a tempting buy. It pisses me off that the industry is clearly manipulating people to see the halo tier cards as good value. They actually are because the value sucks everywhere else.

The Titan class has historically been a poor value proposition; 2-3x higher cost for 5% higher performance. The 4090 actually makes sense if you've got the rest of the hardware to utilize it. Hell, I might bite if they make a high refresh super UW monitor (7680x2160)."
When you're making claims, it's on you to provide sources, not tell people to look it up.
[deleted]
Frame generation exists
Crazy that you still need DLSS with a 4090
"Hellblade Senua's Sacrifice is another game where RT looks really good.  
I've also heard good things about Metro exodus"
DLSS 2 is very advanced, I don't notice it most of the time and sometimes it looks better than TAA.
Well, looks like my parents made a mistake when buying me a PC with an RTX 3080 that came with a 1080p monitor, because I now have the most unpopular opinion known to mankind
Gotta love living in negative comment karma hell with no way to raise it.
"1080p is where youâ€™d definitely notice it, if you donâ€™t thatâ€™s great but in general you donâ€™t want to have to rely on upscaling unless youâ€™re playing 4k and upscaling from high resolutions

For reference, at 1080p youâ€™re upscaling from 720p, at 4k youâ€™re upscaling from 1440p"
You are pushing hard for RT, meaning you prioritize visuals. Yet, you are playing 1080p with dlss, meaning you don't actually know what stunning visuals look like. Give me 4k OLED running native resolution over 1080, regardless of which visual enhancements you stack on top of it.
"So you're playing 1080p and want to upgrade your 3080?  
Stop being an idiot and upgrade your monitor first."
You can still see some pop-in on objects with no collision (the leaf piles for example) or dynamic objects such as the big signs on buildings in Mega City. But that is much harder to notice than when everything has pop-in.
100%
[deleted]
The 1070 wasnâ€™t $400, it was $379, which was up from the $329 of the 970.   Either way, even accounting for inflation GPU prices have been skyrocketing.
I mainly play Bedrock because of the RT.
Itâ€™s more than just red stone in Java that Iâ€™m referring too. But to each their own so thatâ€™s valid.
"I asked about this:

""**DLSS 3** shouldn't be taken into consideration when comparing RT performance, as it can be used outside of games too."""
"RTX was also the proprietary technology that Control and Minecraft where built around. Jumping from RTX to DXR left a lot to be desired. 

DXR can still be biased towards a certain platform as the API itself is designed to run on traditional hardware as well as on hardware optimization engines. It largely depends on who is behind the development team, and DXR can be adapted to work on a certain platform more efficiently than another. In reality, shader cores optimization has impact that is far bigger than ray intersection calculations, that are faster due to the vector acceleration. This is what went poorly with Cyberpunk."
One letter more? And AMD hasn't currently released their alternative.
That's just a more advanced version of RT which is also accelerated by RT cores.
"Well 4080s don't seem to be selling @ 50% more $ than the 3080..

I guess maybe the 7900xtx is good value compared to the original price of a 6950xt, but AMD is taking their sweet time releasing a 3rd new Card.

I still can't help but think that a 4070 was renamed twice and priced $200 higher than I was hoping for.

Kinda glad I decided to stop waiting to get a new GPU right after watching both launch announcements."
The only good thing about this gen is frame generation technologies, such as DLSS/FSR 3. But honestly I'm not expecting much from AMD in that regard considering how crappy FSR 1.0 was.
"7900xtx performs the same or worse than 6900xt in many VR games and also has visual issues.  None of which have been addressed by AMD since launch nearly 4 months ago.  The big issue with VR results is there are few reviewers who even test it.  While they will hopefully fix it eventually, I wouldn't buy a card for VR until it is.  There is no way to know how long they will take.

>Although synthetic VR benches (except for OpenVR benchmark) predicted good VR performance, we were disappointed with our 7900 XTX VR experience, unlike with pancake games. In at least two games, we experienced distracting visual artifacting and texture shimmering. The 7900 series may benefit from some attention to VR from the Radeon driver team as in many cases it even falls behind the RX 6900 XT.

1. https://babeltechreviews.com/hellhound-rx-7900-xtx-vs-rtx-4080-50-games-vr/

2. https://www.youtube.com/watch?v=FSqYkuKjXwA"
"I don't think I should look up, while it's a by design thing for years and with the 7000 series cards there is another bug in single monitor in some cases which is even in the known issues section of the current driver.

[https://www.amd.com/en/support/kb/release-notes/rn-rad-win-23-2-1](https://www.amd.com/en/support/kb/release-notes/rn-rad-win-23-2-1)

  
Known Issues
  
High idle power has situationally been observed when using select high-resolution and high refresh rate displays on Radeonâ„¢ RX 7000 series GPUs.

This will be fixed, probably.

TPU review:  
[https://www.techpowerup.com/review/amd-radeon-rx-7900-xtx/37.html](https://www.techpowerup.com/review/amd-radeon-rx-7900-xtx/37.html)

&#x200B;

&#x200B;

[https://www.reddit.com/r/AMDHelp/comments/ldvyh5/vram\_clock\_speed\_with\_dual\_monitors\_rx5700xt\_2121/](https://www.reddit.com/r/AMDHelp/comments/ldvyh5/vram_clock_speed_with_dual_monitors_rx5700xt_2121/)

[https://www.reddit.com/r/AMDHelp/comments/z4a7j2/does\_amd\_rx\_6xxx\_series\_still\_idle\_at\_max\_memory/](https://www.reddit.com/r/AMDHelp/comments/z4a7j2/does_amd_rx_6xxx_series_still_idle_at_max_memory/)

[https://www.reddit.com/r/radeon/comments/vnimki/fix\_high\_vram\_clocks\_and\_lower\_gpu\_power\_draw/](https://www.reddit.com/r/radeon/comments/vnimki/fix_high_vram_clocks_and_lower_gpu_power_draw/)

[https://www.reddit.com/r/Amd/comments/zp2hla/7900\_xtx\_vram\_not\_downclocking/](https://www.reddit.com/r/Amd/comments/zp2hla/7900_xtx_vram_not_downclocking/)

[https://www.reddit.com/r/Amd/comments/1126uqk/has\_amd\_fixed\_the\_high\_power\_usage\_in\_rx\_7900\_xtx/](https://www.reddit.com/r/Amd/comments/1126uqk/has_amd_fixed_the_high_power_usage_in_rx_7900_xtx/)

[https://www.reddit.com/r/AMDHelp/comments/syupta/rx\_5700xt\_vram\_clock\_at\_max\_frequency/](https://www.reddit.com/r/AMDHelp/comments/syupta/rx_5700xt_vram_clock_at_max_frequency/)

[https://www.reddit.com/r/Amd/comments/aof5pc/anyone\_using\_a\_dual\_monitor\_setup\_with\_the\_gpu/](https://www.reddit.com/r/Amd/comments/aof5pc/anyone_using_a_dual_monitor_setup_with_the_gpu/)

&#x200B;

VR performance in this review:

[https://babeltechreviews.com/hellhound-rx-7900-xtx-vs-rtx-4080-50-games-vr/](https://babeltechreviews.com/hellhound-rx-7900-xtx-vs-rtx-4080-50-games-vr/)"
Guy defending and should aswell
Cyberpunk looks like a completely different game with RT on
Yeah but itâ€™s kind of noticeable.. tbh I switch back and forth but usually RT is off. Ray tracing is just a mini path tracing and thatâ€™s definitely worth the frame generation
Why are you so stiff about using RT? Are you wearing a leather jacket right now?
Don't you notice it in Fortnite for example? God I love Lumen.
Didnâ€™t think about it but do you have an older gpu? I remember using it on my 1660 super but after I got the 3080 I never use it.
At this point I sometimes feel that playing at 1080p is a luxury lol
Just find something else to do lil bro
720p is 66% of 1080p right?
I'd advise leaving Nanite on even when turning off Lumen because the performance impact is very small I think.
These dudes aren't paid to simp. They do it for free.
RT is available unofficially on Java through resource packs.
"It was refferd to RT.

But yes, super sampling is not a game exclusive either."
I know that RT cores can be used in Blender
I know little about this but you seem knowledgeable and Iâ€™d love to hear more specifics around what happened with Cyberpunk. Thanks in advance.
Amd FSR and Intel XeSS. It's on the way, but belies the point - your writing favors nVidia. I don't have a house in the race, I'm just looking from the side lines
"FSR 1 isn't crappy by itself. It's just a limited technology but by that very versatile because you can use it with any games. It's just a spatial upscaler. DLSS 1 was crappy itself as well and not versatilely useful like FSR 1.

FSR 2 is war better than both first versions. It's not as good as DLSS 2 for sure but the difference isn't that big and I'm more impressed it works well enough on many GPU architectures.

But I'm very curious about FSR 3. DLSS 3 works (kinda) but still needs development and I'm not sure if I'll be a fan but very interested how both techniques will develop over time."
The angry replies suddenly stopped LOL
FG is noticable??? wat
"> God I love Lumen.

doesn't require ray tracing hardware, just a whack of gpu memory."
Or Control, Minecraft, Portal RTX
You can't use DLSS on a 1660. The card has to have "RTX" in its name.
1080p is far from luxury anymore. 1080p is considered the mainstream with 1440p creeping to take its place soon (if it hasnâ€™t already) since most systems that can handle high refresh 1080p are capable of 1440p 60+ fps. 4K is a luxury.
I feel kinda bad for you man. I mean, your hard backing of RT is a little much, but I get it. I'd rather visual quality over excessive frames in most titles, excluding FPS.
Well I like Reddit generally. I learn a lot from it. I just hate being called a troll and not being able to post in some subs due to negative comment karma caused by people downvoting me because of having their settings misconfigured or setting their expectations too high
"It's 66% as many horizontal (or vertical) pixels, but only about 44% as many total pixels.

66.7% of the horizontal pixels and 66.7% as many vertical pixels = 44.4% as many total pixels."
I can notice it in some games and in side by sides, but usually only at extreme settings (performance or ultra performance). I think part of the reason I don't notice it (in Fortnite at least) is because of the game's art style.
Now with the utmost humility, I ask you all to remove the downvotes. My comment karma was gone to shit earlier, at least give me a chance to make it positive
And those unofficial "RT" implementations (which you can't actually validate if it's actually RT or not) perform worse than bedrock when bedrock looks way better. Bedrock's RT just feels more in-place.
Is it? I was under the impression java the language itself doesn't support raytracing, so it couldn't yet be added to java MC. I'm not sure shaders count? But I could be wrong, I'm not well-versed on those technicalities.
You mean VSR? (Or any other video upscaler)?
"As far as I know, we do not have the exact specifics of what caused the shading bottlenecks. Probably the rush&crunch. 

From another developer:

To explain; the most important harware feature for DXR performance outside of having ray intersection accelerators is shader core performance. Rays need intersection calculations but they also need to be shaded via radiance calculations like BRDF, phong, lambert, etx. Ray intersection calculations are much faster relative to rayshading because it utilizes hardware acceleration the later is the slowest and has the biggest performance hit as it uses shader cores. Amphere has single precision FP32 shader performance that is nearly double that of RDNA2 which more than explains the performance difference in DXR games.  This advantage is only because game developers use single precision FP32 calculations to do rayshading. RDNA2 actually has a much bigger advantage in shading performance than Amphere in that RDNA2 has more that double the shader performance of amphere whe using half precision FP16. So if game developers would just switch from FP32 to FP16 in the HLSL shaders to do rayshading the performance of the RX6000 series GPUs would sharply increase by over 100% in extreme cases."
Hard to argue with facts. The people handwaving bad VR performance is especially sad because that's the reason I ended up going for nvidia this generation. Latest gen flagships should never be beaten by last gen. Blaming the drivers doesn't make it any better.
Yeah itâ€™s not that big of a deal but I prefer it off if I already can hit 144 without rt and frame generation.
Massive latency penalty
FG is super noticeable lol.
But I watched so many videos they all said it was not noticable :(((
It doesn't require it, but it can use it to get better quality reflections. For example, in the software RT version players are not reflected, but with hardware they are. You can see this for yourself if you use unreal engine.
Seems Iâ€™m misremembering
I know, I consider it a luxury because I don't need to worry about low FPS with RT. But don't get me wrong, I would really like to have a 1440p HDR monitor. I also don't need to worry about setting my expectations too high like some of my downvoters.
Yeah, you don't really need FPS above a 100, even in competitive titles unless you're Shroud or Bugha or something. RT is perfectly viable, as evidenced by The Finals forcing you to play with it on.
I wouldn't even be interested in PC hardware nor build a PC myself if it wasn't for Reddit. And I'd still be an idiot playstation fanboy.
"That's not a super sampling algorithm, that's an upscaling tool with intelligent denoising capabilities. It's a completely different tool. 

I'm talking mostly about the professional lineup of products based on the Nvidia Drive platform."
So RDNA 2 performs better in FP16 calculations?
Thank you!
But RT is so pretty ![gif](emote|free_emotes_pack|heart_eyes_rainbow)
It's not massive. Max 10ms in some edge cases. Besides it's still less latency than native with reflex on.
Horray people being different.
I think you really overestimate how 'powerful' the current generation is, if you're using 4k, or ultrawide, you will not be able to cap out high refresh rate monitors on AAA games on high settings. The only games you reliably have super high refresh rates on are esports titles, unless you have like a 1080p monitor in which case it doesn't really matter
Well, the thing about people with the 40 series cards is that theyâ€™re typically people who are trying to take advantage of high refresh rate 1440 and/or 4K, both of which take significantly more processing power alone just to render a native resolution image. For 1080p the 40 series cards are absolutely insane levels of overkill. Anything stronger than a 1080 is, really (so most 20 series, all of 30 series and obviously all of 40 series).
"dude what are you talking about? Looked it up, the finals is still in Alpha, not publicly available except invite for the playtest. It's a vertical slice of the game, you have no idea how RT is implemented on this unreleased game or in what capacity.   


I'd just be grateful that your parents bought you a PC with a 3080, instead of coming to the comments to argue with anyone about products they own just because you watched some youtube videos."
I know all too well what being a PS fanboy feels like because I have a friend that literally worships it like a god.
Nvidia Drive? For cars? wat
But wouldn't that impact rendering quality or something?
We're comparing Ampere and RDNA2 here, but yes, it does perform better.
Especially if FG isn't noticable ![gif](emote|free_emotes_pack|shrug)
And that penalty is measured against DLSS 2 with reflex.
240hz is becoming more and more common on 1440p and 4k monitors
Well that was based on the playtest videos, of course if they change that I will change my opinion or fact or whatever you want to call it
Yep. It's the most popular OEM autonomous driving solution on the market.
I read somewhere (on Ars Technica I think) that RDNA 2 performs worse in RT because the ray accelerators only handle BVH traversal, the ray-triangle intersections have to be done by the normal shaders.
ðŸ˜‚ everything looks pretty on a 4090 if you know how to tweak it. I played a few GameCube games that could pass for new releases on this thing. I hope you get one soon my boy!
Also there were a lot of benchmarks on The Finals and personally it looks to me that the game is in a pretty complete state, they just need to add more content like more maps and whatnot
Hmm. So GPU is used to train the model or what? I don't get it.
Which I personally think is a terrible idea and if I were AMD I'd change it immediately.
A lot has to be done by shader cores, including RayShading and mesh intersections. The lack of RT performance in Cyberpunk is not exclusively given by this factor alone.
Not a 4090 but I may have a chance at convincing my dad to buy me a 4070 Ti
"the games in alpha, it's not in a complete state. ""maps and whatnot"" is the whole rest of the game. They made a vertical slice that they can pay someone like shroud to sponsor the game for them to try and get people interested in. We have no idea what the state of the rest of the game is in, how it will perform at launch, anything like that.  


That all being said, I took a look through you comment history. you seem young, and new to the hobby. You've posted some either misinformed or outright false information to buildapc before. Try keeping to good sources of information, (Gamersnexus, Hardwareunboxed, LTT) for hardware reviews, comparisons, etc. 

&#x200B;

Second I don't think you've built before. So suggesting to people that they need custom loops or that 7600x is ""destroyed"" by a 13600k, while seemingly using bad information as a reference, is disingenuous at best. If you want to get into the hobby - Great! glad to have you. Just try to keep an open mind, use good sources of information, and always watch out for shady marketing tactics that just want to sell you shit you don't need."
"It's a bit more complex than this, and does not revolve around the RTX lineup to function, although it shares al lot of the hardware (Jetson platform) and stills benefits from cuda-optimized code. 

You should looyit up, it's an awesome tool. Source: I'm a certified Nvidia Developer and I've worked on the Jetson Xavier."
Jesus christ kid you're so insanely spoilt and you don't even know it.
(I'm 16 yrs old so I don't have $800 lol)
I mean a 4070ti is a lot better than a 3080 but I still have mine if you want it
i got a job at 16 and i paid for my 4080 myself, maybe get a job?
That's old enough to get a job
Thanks but I already have it :)
YoU sHoUlD saVe tHaT mOnEy
Nice!
In 2020-2021, mining and isolation caused a massive spike in GPU demand.  Prices went up.  GPU manufacturers (particularly Nvidia) really liked that.  They don't want to let that go.
"The 3060 got 12gb because of it's 192-bit memory interface. Options were 6gb or 12gb.

The original intent through design was to sell it with 6GB of VRAM, but in practice it was a bit slimmer in performance than expected, and so it was switched to 12gb in production.

Do note that nvidia has released a 128-bit version of the 3060 version with 8gb, and that this is what they'd likely have originally released if it wouldn't have required a massive change in production to switch."
"corporate greed.

enjoy!"
"> Why is the 3060 so cheap still?

It isn't. It's an almost $400 lower-mid-range GPU.

> I want to know why the 3060 is so much cheaper than other 30XX and 40XX cards to this day

The 3060 is likely still being produced. Higher-end 30 series GPUs are not being produced anymore, because they've been replaced in the product stack with more profitable 40 series GPUs."
OP is just mistaking VRAM for clock speeds and overall performance. The 3060 has 12 GB of VRAM, but doesnâ€™t actually have the same computing power that other cards with that amount of memory have. Realistically, it will never max out itâ€™s VRAM usage because the card just isnâ€™t fast enough.
I could write a dissertation on the subject, but lets be honest, its just basic corporate greed. And the 3060 isnt cheap either, its a wrong assumption on your part.
3060 has a bus limitation, which only allowed them to pair it with either 6 or 12GB of VRAM, thus they decided to go with the latter.
"Cause it's slow. 12GB of VRAM is a gift for it. 3060 just does not need that much.

.. but yes, it is very nice to have for things like SD and NLPs

I use 3060 and A4000. Let me know if you have questions. I will try to answer."
"Multi billion dollar companies like money, end of story.

Might be a tinfoil hat theory so take it with a grain of salt, but I'd say the only reason the 3060 got 12GB of GDDR6 (non X) was to be more appealing to crypto miners. The 8GB on everything else up to and including the 3070Ti is planned obsolescence and even the 3080 10GB is hard to defend."
"Duopoly problem: if the larger company arbitrarily raises their price for the sake of profits, the other one can, too... As long as their product ""seems like"" a good value when compared to the other product.

I hope intel will shatter the duopoly and bring prices down, but we are all still waiting with bated breath for a decent intel release.

Edit: to show that it is just recent greed, look at the stock prices. (Click max timeline on the graphs)

AMD: https://finance.yahoo.com/quote/AMD/
NVidia: https://finance.yahoo.com/quote/NVDA/"
"> And then there's the 3060 with 12 GB

GPUs heavily rely on multiple memory channels to handle the amount of pixel data that needs to be fed into them. You'll often see the memory interface width advertised (ie. 256-bit, 384-bit) in connection with performance. 

If you take that number and divide it by 32 you'll get the number of memory channels on the GPU (each one being a controller element and a block of cache), and for the card to perform well each channel should be attached to an equal sized ram chip.

If you look at the 12GB 3060 you'll notice that it has a 192-bit interface which means 6 channels. If you used 1GB ram chips then you'd only have 6GB. Apparently NVidia decided that 6GB wasn't enough to compete with AMD in the same segment (maybe they thought it would be enough during development and only realized when AMD released a competitor first). However the next available ram chip is 2GB so they had to go to 12GB in total. This is far from the first time someone's gotten weird memory sizes due to the number of channels.

There are two other variants. The 3060 Ti is 256-bit which means it has 8 channels and makes it easy to hit a target of 8GB exactly. There is also an 8GB varient of the 3060 however you'll notice it's only 128-bit - 2 channels are being left empty so they can use less RAM. This decreases the cost but having only 4 channels bottlenecks the card's performance compared to the 6 channel 12GB and especially the 8 channel 8GB versions."
"Big corpo be greedy. They really loved the corona/cryptofever times when they could charge whatever and make nonsensical Ti variants that were barely faster than the OG version but cost significantly more and there were people more than willing to pay. Now they want to make that the new normal.

If the second half of your question talks about the 3060 then no, not really. It was mostly just a marketing move to make the card more appealing to certain groups. It has a big VRAM but the GPU core itself is fairly unimpressive and the memory also isn't very fast."
"Basically this. When the 3080 launched at $699, it was hailed as an insane value. But pandemic + mining basically showed what people were willing to pay for graphics cards, and Nvidia was like, ""Shit - people are willing to pay $1200 for a 3080?? We thought we were stretching it by doing $1600 for a 3090. Why should scalpers be getting that extra $500?? We should be getting that shit!""

And so the next generation launched, and Nvidia was like, ""Alrighty - let's see how much people want this"", and unfortunately, the fact that the 4080's and 4090's are commonly above MSRP or sold out has proven them right."
"Then why haven't old stock 3060s risen in price to reach the new market level?

What is locking them down at $350?"
"so it essentially got supercharged by accident, and they just didn't bother changing the price?

what's stopping the secondary market from driving up its price to match the ball park of other cards?

do you think a 12GB 3060 is a good buy? or do you think it would be smarter to go for something like a 3080 TI, or a 40XX with 10+ GB considering the ""other improvements"" to the card beyond the VRAM?

FWIW: the general wisdom in my lab is that the 40XX cards aren't worth it for ML because you're still bottlenecked by having roughly the same amount of VRAM. Our lab computers mostly use 3080 Tis or various TPUs."
Interesting about the 128-bit 3060!  Didn't know that existed.
"Then why haven't old stock 3060s risen in price to reach the new market level?

What is locking them down at $350?"
"FWIW VRAM is massively important in ML, regardless of clock speed, because the datasets we work with are potentially ungodly in size.

This is why I'm kinda surprised by the 3060 in particular. It seems like such an outlier that might be unusually good for ML."
You're living in the past. Its by far the cheapest cuda capable card with 12GB VRAM.
Should I just suck it up and get an A4000? You're the 3rd person to mention it in this thread.
"Then why haven't old stock 3060s risen in price to reach the new market level?

What is locking them down at $350?"
So in your humble opinion, the savings from going with a 12GB 3060 are probably not worth dealing with the inferior number of channels and/or inferior cores?
3060 was always an odd duck.  Not quite enough power to justify the 12gb.  Nvidia felt like they had no choice (apparently it was originally supposed to be 6gb).  They had to match AMD (6700 XT).  That's my take, at least.
"Because they are at the same price as an AMD card that's 35% faster. If they got any more expensive, people would just ignore them. They are already a pretty bad deal as is. Nvidia has basically abandoned the low end at this point. Hopefully the 40 series will have some decent offerings. 

The 3060 also has an unusually high amount of memory, more then the 3060ti, 3070, or 3080. That's because it could either have 6gb or 12 gb, with how the card is designed. And 6 is just not useable"
Personally I'd recommend getting a used RTX A4000 for ~$500.  It has 16GB of VRAM and outperforms the GTX 1080ti. Although the memory bus is smaller. It does have a bigger bus width than the RTX 3060 12GB. It uses the same die as the RTX 3070ti while using half the power.
Most people buy geforce cards for gaming, and in gaming tasks the 3060 with 12gb is usually slower than a 3060ti with 8gb because vram isnt that important in games. That is changing recently, but overall a 3060 wouldnt be worth it if it cost more because you can just get something faster.
Itâ€™s a really weak card by todayâ€™s standards that from a gaming perspective has no chance of ever utilising 12GB VRAM, that was bolted on purely for marketing.
I'd say the competition. It was never really all that powerful to begin with and there were both same-gen and older-gen cards that could do pretty much the same thing so it had a lot of competitors. You could kinda justify price hikes in the high-end but good luck price gouging something that doesn't even really beat a 2060 Super.
because that is not optimal marketing strategy
"3060 is great for professional use cases that need VRAM.  I've heard it's popular for creative pros in particular (Adobe suite, etc.). 

Search around r/MachineLearning, see what people recommend.

[https://www.reddit.com/r/MachineLearning/comments/tbsldk/d\_does\_the\_rtx\_3060\_work\_reasonably\_well\_for\_deep/](https://www.reddit.com/r/MachineLearning/comments/tbsldk/d_does_the_rtx_3060_work_reasonably_well_for_deep/)"
If you are only interested in the vram aspect, sure I guess. But when you look at the number of CUDA cores that are actually on the die, you wont get good performance for the 12GB that the card has. And if the rumours are true and the 4070 will actually launch at 550$ msrp with 12GB vram... The 3060 will still be cheaper, but it will get destroyed in compute heavy tasks and games by the 4070 for "only" 150$ more
Depends on the price you can ger it at. Regular price is close to 4080 so this might be a better option.
Not sure what you mean. At 350$ the 3060 is already terrible value considering the offers from AMD and Intel at a comparable price point. Add the fact the 3060Ti is residing at just a bit over 400$ whilst being a vastly more capable GPU compared to its non-Ti sibling and you end up in a situation where it simply wouldn't sell at all at an even higher price point.
"Actually for your use case quite the opposite. The fact that NVidia had to go with 12GB is a problem for *them* since they hadn't anticipated AMD would increase VRAM in that segment so quickly.

Now if you were buying the card strictly for gaming it might not be be a great value - you're paying for an extra 4GB you don't need in that performance class and there's something of an NVidia tax on top of that too. In that price range AMD is simply a better buy for gamers.

But for AI training it's a fantastic deal if you don't have $1000+ to throw around. For AI that extra VRAM is crucial and normally NVidia and AMD would make you pay a premium to get it. NVidia's misstep lets you get the workstation class VRAM you need (oh and install the studio drivers too) while only paying for the speed you can actually afford."
I just got an A4000 and highly recommend it. Benchmarks are on par with my RTX 3070, with twice as much memory. The smaller size and lower power requirements are just extra bonuses.
Is there anything unusual I have to be aware of when building the rig?
"Oh I hadn't considered that the lower end 30XXs would be directly competing with cards from older generations.

That said, I vaguely remember the high end 10XX cards going for like  $600? With that comparison, the 3060 looks ""half price."""
"> Search around r/MachineLearning, see what people recommend.

I get you, but that subreddit sucks. 2.6 million subscribers. I would be surprised if there are 2.6 million people working on machine learning on the entire earth."
"> if the rumours are true and the 4070 will actually launch at 550$ msrp with 12GB vram

honestly that's great news"
"So you're saying the 12GB 3060 is ""price locked"" by competition from AMD?

But then when it comes to higher tier cards, both companies just agree to run prices way up into the thousands? Or has Nvidia just found a justification for that on their own?"
"Nope. It's just like any other GPU. Only It's a single slot width. It uses a standard PCIe 4.0Ã—16 interface. 

-[EBAY listings <$500](https://www.ebay.com/sch/i.html?_nkw=rtx+a4000&_sacat=27386&LH_PrefLoc=2&_sop=15&_udlo=420&_udhi=520&LH_ItemCondition=3000%7C1500&Memory%2520Size=16%2520GB&Brand=NVIDIA%7CPNY&_dcat=27386)"
Why they wouldn't? That has been a pattern for years and NVIDIA used Samsung's "8 nm" to save a buck in the 3000 series instead of using TSMC's 7 nm so they didn't had enough headroom to increase performance drastically without increasing power consumption considerably (They still needed to do that with the RTX 3080, 3090 and specially 3090 Ti), that may be one of the reasons the 3060 wasn't a big upgrade from last gen mid range GPUs (RTX 2060 Super and 2070) besides more VRAM and a tad better RT.
Hah. Fair enough.
I wouldnt consider it great, but its a step in the right direction. However I wouldnt hold my breath, need to actually see the official pricing first and not from some leaks. Also some videocardz just published an article that says the MSRP will be 599$
"Yes and yes.

This is why people are so hopeful that Intel's GPUs pan out.  We need a third option in the midrange to help temper the other two."
"You don't even have to take the competitors into consideration, the simple fact the 3060Ti can be had for 410$ is enough.

None of these companies are anyone's friend. They simply want to extract as much money as they can from us. There are of course limits to how much people are willing and able to spend, but the crypto disaster has shown customers are willing to pay significant amounts of money to get the GPU they want, even if they are bad value compared to what was sold at an equal tier a few years ago.

I don't expect this to change anytime soon, unless we either get serious competition to Nvidia's offers across the whole product stack (including their superior featureset and RT performance) or AMD/Intel decide they are willing to cut prices significantly in an aggressive move to gain market share."
"> None of these companies are anyone's friend. They simply want to extract as much money as they can from us.

No fucking shit. Go teach third grade if you want to sound profound. I do not care about your moral crusade.

I just want an explanation about why the 3060 is such an outlier, and what that means for my build strategy, which is essentially locked to Nvidia's machine learning suport."
Dang dude. You are asking for an explanation and this how you respond to someone answering your question.  Itâ€™s okay not agree and/or like a response but at least should some basic decency toward your fellow human; there was no hostility it their response.
"I honestly am just so fed up with the dozens of irrelevant replies I've gotten in this thread where the poster completely ignored my questions because they really just wanted an excuse to complain about crypto inflation.

I simply do not care.

I am here for utilitarian advice for how best to operate within the confines of reality.

Thank you have a nice day!"
Thatâ€™s cool. As we know itâ€™s Reddit and human nature lends itself to venting/complain on frustrations. I would skip all the gpu bs and reasons behind it. If you were/been rocking with a 1080Ti since launch, just get a 4080/4090. This gives you the best current in gaming with the ML capabilities. However, some will quip at the price point of both, etc, etc, which may or may not be valid for youâ€¦.forgot to add. Yes you would be shooting in the foot getting a 3060 12gb, evening if you were not doing ML.
"Sales may be down from previous quarters, but it sort of works like New York real estate.

raise prices when times are good and people have money. When times get bad, you still keep the same raised prices for as long as you absolutely can. use other revenue streams to service your debt, and wait for another economic upswing. Yeah, you'll have a few quarters / years of low sales / high inventory, but when the economic situation changes you'll be ripe to not only keep your (already inflated) pricing, but you can make the 'excuse' that you need to raise prices to keep up with -arbitrary reason here-."
[deleted]
LoL. Wait for Feb 22th earnings call from Nvidia.
"They are selling, probably not as hot as the 3000/6000 series at launch but even if they are ""bad value"" there's still a relatively large market to sell expensive high-end products to.

Since the GPU cycle is about 2 years, if sales are down this year they will still have plenty of silicon left for next year. I expect big GPU price drops by black friday this year and definitely in 2024. Although Nvidia are experts at reducing supply just before they introduce the Ti/Super refresh models so, we'll see."
"Look more closely at what happened to GPU pricing as NV has launched new cards last year. When the 4090 launched, the 3090TI and 3090 went on sale, clearing out inventory at $900-1100, well below MSRP. 

When the 4070Ti launch got close, we finally saw 3080s go for $700 after being $800+ for much of this year. I donâ€™t see a bunch of 3080 inventory floating around any more, at least here in the US. 

What do you think is going to happen to 3070Ti and 3070 prices once the 4070 is announced? Iâ€™m betting we see $400-450 3070s as they move to clear inventory."
[deleted]
They're selling just fine is why.
"Because Nvidia has mindshare to command whatever price they wish. AMD doesn't depend on graphics cards to affect their bottom line. They will at some point make a play for more marketshare, but I think this generation is just about getting the technology out the door to prepare for the future. Look at zen history for their template. 

I'd think that CPU and especially server CPU drives their profitability."
People are buying them, thatâ€™s why.
Reddit is not real life. Everything makes sense once you understand that.
"It's because NV and AMD can wait to squeeze some more money. Some people slowly acquiesce to scrap extra $ and buy them. I see it on some forum. They complain, promise to wait and two weeks later they buy anyway.

And then when they lower the prices to (still high) values, people will jump to buy. It's an old technique."
Industry is gobbling them up. Consumers might not be but boi does my co buy a lot of em. AI is running real fast on those nvidia cards, and cloud is chock full of em
"Who says it's not selling? They're obviously selling well enough to maintain their price.

The OEMs just want to continue their massive pandemic profits."
Expensive? They are cheap as hell now. Arc 750 for $230.
I'm pretty much done with PC gaming unfortunately. It feels like it's been 5 years of ridiculous prices. I've since bought an Xbox and am making do with my GTX 760.
Sales are low compared to the RX 6000/RTX 3000 series but are likely doing alright compared to RX 5000/RTX 2000.
" Economic downturn. So cards not selling isn't that abnormal.

Prices are high because really high inflation in the last 2 years that has probably been around 3-5x what we are used to at minimum. 

They are also keeping the top end really high to incentivise selling old stock. They don't care much that they aren't selling many RTX 4080 and 7900xtx. Especially since for the ones they are selling, they are making pretty nice margins.

If you were to account for inflation the prices we see on the Rx 6000 series is actually pretty normal, if not below the usual price drop we see. But to be honest, the 6600xt and 6600 should never have launched at the insane prices they did. Those should have really been $320 and $280 respectively. So their price drops only look artificially high.

 Nvidia prices seem kind of high, even accounting for inflation."
Probably trying to clear out 30 series and 6000 series inventory. if you check Newegg or other online retailers, they're still in stock
The cards are not selling for a couple of months, they may lower them any second now, but they might try to hold for a few more months. Hope they sell some at current prices to the people that are too impatient. No way they keep them for the entire year let alone for the entire gen.
"Retail companies pay a lot of money for GPU's and they have VERY low margins. 

They also must stick with the MAP/MSRP given to them by Manufacturer. 

Even if they don't sell, retail companies won't take a loss on a product unless there is no way they think anyone will ever buy it. 

If NewEgg spends 2 million on new GPU's, they are not going to sell it off for 1.5. 

Retail companies are not selling these things on consignment. They are purchased with booking orders from manufacturers and suppliers."
We have miners to thank for these ridiculous prices. Nvidia saw their cards being scalped and tripled in price so they said fk it and just raised the prices beyond normal year to year increases. I hope etherium miners lost their ass this year and I was glad when FTX scammed everyone. If you do NFTs or mining, you deserve to get scammed.
Because there are basically 2-3 companies making them and they all have a ridiculous shitload of money, so they can afford to risk pulling such strategies. It's better to sell 1000 graphic cards for $1000 each than 10000 for $100 each, isn't it?
[deleted]
"There are three aspects to this that the gaming/enthusiast community constantly overlooks (or doesn't understand).

1. just because you think a video card is selling poorly, doesn't mean it actually is. news outlets and people operating on ""rumors"" (most of them outright fabrications) are wrong all the time about this. yeah maybe their buddy who works at microcenter isn't selling many video cards. But this is so far removed from the whole market that it doesn't matter. These guys get paid by the click, and if they post ""GPUs selling normally"" . . . who the fuck is gonna click that? They sensationalize everything and look for anything and everything to get you to click on their shit and give them attention. It literally means the difference between having a job and not having one for them . . . So they do it.
2. We are in a weird GPU supply issue right now. There was a flood of GPUs and inventory that got piled up as Ethereum crashed last year (and eventually went POS). And it looks like Nvidia/AMD/AIBs learned their lesson not to have a fire sale on GPU inventory, and instead to reasonably manage prices and not do a race to the bottom to try and clear inventory. New GPUs are selling (4000 series and 7000 series). But old GPus are just about now clearing inventory, and these are the GPUs you hear about that have huge inventory problems.
3. It cost A TON more (3-4x) to make new video cards (7000 and 4000 series) than the older generation video cards (14/16nm video cards). The two factors that largely determine the price of a Video card are the GPU and Memory. Everyone always cites die size area when comparing teirs of cards and how much they should cost . . . Which is fine . . . If wafer prices stay the same . . . But they haven't. In addition memory prices have not really come down much since GDDR4 . . .But the amount of memory we are puting on flagship cards has jumped from 4-8gb to 12-24gb. Wafer prices have increased from 2-4k per wafer, to 10-15k per wafer. So we have the two most critical/expensive components (GPU and Memory) that have went up 3-4x for GPUs . . . So surprise surprise . . . We see a 3-4x increase in video card prices. Why everyone ignores this simple fact is beyond me; but i think it is pretty obvious that the first point in this post is probably the answer . . . It gets clicks to make you pissed off at Amd and Nvidia."
They are selling well, there are just lots of bitter people who wish they werenâ€™t (as there have always been) claiming that they arenâ€™t selling. This is despite the fact that any MC Iâ€™ve went to has had any 4090s arriving sold within a couple of hours even though itâ€™s 4 months past launch.
"NVidia can freeze the dGPU market (and it did). He said it (like amd admit it also). 'Freeze' means that althougth the next gen maybe can be shipped at a certain date Y they push that date to Y+X. Why they did that ? Again they said it. To clear their inventories.  Or to put it into a customer and not corporate spin .. until you buy them .

So you the customer in a frozen market you will keep bumping on the same products either on shops or on youtube videos and you simple have two choices. Either to buy them or not. No third choice. If you had a 3rd or many more choice is more than propable that another company would have not the inventory issue and wouldnt freeze it's next gen dgpu thus pushing more good products on the market.  *(keep in mind that even if a company dont change its dgpu architecture just by riding on tsmc+asml new nodes would get out a better product !)*

Critical question: How long that will take ?  Now since there is no competition and NVidia has the 90% of the dGPU market theoretically that time limit can  be as long as it takes for nvidia to achieve it's goals."
[deleted]
Here is my issue with the new GPUs: if a 3080 (or really even a 3070) is perfectly competent for 1440p, and 4k monitors are still prohibitively expensive, "upgrading" to a 40xx card only makes sense for the richest of rich kids.
"The thing to keep an eye on is the inventory of chips - if that starts going up then expect upcoming price reductions - keeping a devaluing asset (as chips get outdated) in a warehouse gathering dust is of no benefit to AMD or Nvidia unless they expect it to be a pretty short dip before the market recovers and they can offload them. I believe the inventory of public companies is part of their earnings, though it may be assigned a pretty arbitrary dollar value and spread over large business units.

If inventory isn't really going up, then they're selling what they're building already, so have no need to reduce the price.

The high costs and corresponding relative low sales may be due to lower supply - if that is the case then don't expect things to change in the short term. No point putting things on discount if you can't sell much more anyway :p"
One of the unfortunate, or fortunate, issues is frame rates. People perceive even the low end as valuable because it's still enough for a great experience, especially in e-sports, and that was rarely the case before Maxwell and Pascal. The 8th gen consoles kept graphics from really requiring anything more than a GTX 970 or 1060 for many years. This value perception allows Nvidia to charge a lot more for a 3050.
It's because they are actually selling ok and the media likes to post sensationalist headlines for clicks, earnings will come in better than expected.
Iâ€™ve just started evaluating the latest â€˜quadroâ€™ cards and the 6000 model (the RTX 6000 ADA) to  be exact which in past revisions was a Â£4500 card is now list at Â£10k. What the actual fuck!?! Thatâ€™s just plain greedy !!!
Why are Bentleys still $500,000 if they only sell 20 of them a year?
4080's aren't selling well, but they're selling. And 4090s are selling just fine, they still go out of stock quickly despite being the most expensive GPU of all.
I hope GPU manufacturers eat crow for the pricing of the last two generations and take a hit...  pricing is absolutely ridiculous
"Because AMD graphic cards suck and can't threaten NVIDIA cards at all.

So NVIDIA can sell with higher price easily.

It's a shame even Intel Arc does better than AMD GPU."
No one will buy them cheap, so why not maximize by selling them expensive to those who will buy?
"CEO of Nvidia told investors(?) just prior to the current gens release that he had no intention of dropping price points for either gens' cards. Dispsite the end of crypto mining, he believed history showed that gamers would continue to pay... 


We'll have to see what Qtr earnings shows, but so far I'm glad I didn't wait and bought my 3080 12g when I did during a low point just before the new gen came out. No regrets about not waiting, that thing is smoking."
AMD and Nvidia are holding stock back to keep prices high.
"There's a couple of reasons for it, first off it takes time and competition to chip away at the prices to squeeze the margins..

But secondly, the price of everything went through the roof with the pandemic, and the chip shortage is only just over. Even cardboard was stupid expensive and shipping turned into a 'sold to the highest bidder' war.

It'll take time for those prices to come down and also the inventory to flush clean of stock that cost a lot to produce, so it'll be a gradual thing.

There is of course, greed. But that's not the whole story."
Because they're playing chicken with consumers. They're trying to reduce the supply of GPUs to continue to command high prices, but at the end of the day, I think nvidia is gonna have to do a market correction eventually. Hold the line. Or buy AMD/intel.
"This generation for AMD and NVidia has mostly been the culmination of a massive amount of time and effort overcoming the obstacles of the various node shrinks. It used to be just shrinking the node and cramming more of the same cores in would bring major performance benefits, it still does, but it seems like it requires more effort and smarter designs as well to get there now.

My personal opinion is this generation is really just a stopgap for both companies to bring new designs and processes online, AMD seems to have got to chiplet based GPUs first but nVidia MUST have a chiplet design in the works. Their just squeezing everything they can out of a monolithic design first.

With everything that happened during the pandemic and chip shortage they will be keen to recoup every penny they can this generation. It doesnâ€™t help that with the mindshare that exists around nVidias top tier cards, AMD procing their cards too low would make them â€˜lookâ€™ cheap and inferior even if their not too far behind.

NVidia is currently the leader, so they get to set the prices, and itâ€™s pretty clear they will always fuck over consumers in the name of profit. Until AMD or some other challenger takes back the crown, prices will never drop if nVidia have their way.

Thatâ€™s not to say AMD might not have done the same if the roles were reversed, but I donâ€™t think they are that cutthroat. Companies like nVidia and Intel have greed and shareholder profit as a core value of their company culture. AMD has always seemed to value engineering more (hence why they always seem to be poor at marketing)"
People would riot if you actually knew how cheap these cards are to make.  The price is inflated by 3 to 4X because of the crypto surge!
A billion dollar companyâ€™s not gonna go into any kind of panic mode when their $1000+ products arenâ€™t selling for a few quarters or years. People are still gonna buy eventually. You ride it out and live to fuck the consumer another day.
Because they are selling, just not to you?
3080 is $420 at bestbuy
Problem is they are expensive to make. Also, they waste a lot of electricity. It's because of nVidia that raised too much the entry level, that should be 2 TF, and especially the high end (four hundred watts?! ðŸ¤¨). It's just unsustainable.
Just like the housing market, it is seller's greed and denial of adjusting back to normalcy.
[deleted]
GPUs are getting huge and newer process nodes are more expensive per transistor than previous ones. An example of this is an RTX 3090 has 28 billion transistors and an RTX 4090 has 76 billion.
they are still hard to make
"Prices of mid end hardware has dropped though, that's also the first thing that drops because it's the main market.

The high and low end gpu's aren't as much affected because those aren't the top sellers anyway.

Hence you get these situations where a 3060 , even 3060ti are quite cheap at the moment, but the higher end cards keep their pricing longer.

There's also inflation, that warps the view of the situation a lot. 400$ today is not the same as it was last year. It's less worth, so the prices have gone down more than it looks."
Idk, but I think the higher powered gpus arenâ€™t really necessary if you donâ€™t have a use for them. Anything higher than a 3080 and you probably need to just settle for 4k at 80fps lol
the silicon allocation would be selling for more in a professional card
Greed is a helluva drug
You only need to sell half as many if you're charging twice as much
Less demand, less supply, price remains constant.
"Basing it on my limited knowledge of economics...

* Something happens to cause the price to go up
* People get used to said item being expensive
* Whatever happened before subsides
* People are now used to the new inflated price
* Inflated price becomes new normal"
I almost bought a 4070 ti but I chose to get a used 3090fe Iâ€™m going to skip the 40 series
"Because gamers only account for a fraction of GPU sales. Industry professionals are still buying them and many other companies who need the best of the best. So unfortunately Nvidia is getting enough revenue elsewhere and when you consider that their board partners like MSI and Asus have already paid Nvidia so they are the ones who will get left with most of the stock. 

In the end it comes down to whether Gamers will hold out long enough so that Nvidia will make at least somewhat less profit than they projected and sadly most gamers don't have that willpower or foresight."
"Hi,   
I have recently bought an i5-12600k CPU and since my GPU is 1660 Ti, I decided to upgrade. I was considering getting something like RTX 3070 or RTX 3070 Ti, or RTX 3080. But I wanted to ask for recommendations, the main thing I want to avoid is the bottleneck. So which GPU model would be as low as I can get without bottleneck?  
Thank you."
The 4090 sells well and is often out of stock. The 4080 isn't selling as well because it's overpriced, which makes buying the 4090 the better value proposition.
"I don't understand either.

It's stupid, it's like they're purposefully trying to kill off PC gaming."
They are doing what the diamond manufacturers (and now every industry is doing) helping to pad their bottom lines by greedflation. This means jacking up prices on everything and keeping them there, even if they don't sell. The old outdated stuff won't depreciate as it used to, and the high-end stuff will end up keep getting more expensive. Artificially keep supply constrained to cause scarcity & boost profits even more. If you want a computer, you'll have to pay $500-1000 for 'a' graphics card, and $2000 or more for 'a good' graphics card. HAHAHA. Once consumers get used to paying those prices and think that's what they cost, ratchet up the prices more by producing even less. Mark your average $5 bottle of wine up to $50 and sell it as premium. All the other companies also see how much profits are soaring and do likewise. You might think I'm just joking, but this is actually what is happening in real life: companies are in a race to the top regarding prices, using the pandemic and supply chain woes as cover.
NVIDIA WILL REMAIN EXPENSIVE TILL PEOPLE STOP BUYING THEIR GPUS AND FOR A LONG PERIOD OF TIME SO ID WAIT A COUPLE YEARS AND DONT INVEST IN NVIDIA GPUS THEN WE WILL SEE IF PRICES REALLY ARTE 1K FOR A 4070 BUT CALLED A 4080 16 GIG.  thats a card that even with a generous rise in price shouldnt be anymore that 600 uk pounds.
"and imagine the PS5 has this for 399.00 and its faster than all the systems out spec and benchmark wise

 CPU  
x86-64-AMD Ryzen Zen 2  
8 Cores/16 Threads  
Variable frequency, up to 3.5GHz  
GPU  
AMD Radeon RDNA 2-based graphics engine  
10.28 TFLOPs, 36 CUs at 2.23GHz  
Ray Tracing Acceleration  
RAM  
16GB GDDR6/256-bit  
448GB/s Bandwidth (14,000mhz)  
Internal Storage  
Custom 825GB SSD  
5.5GB/s Read Bandwidth (Raw)  
Typical 8-9GB/s (Compressed)  
Expandable Storage  
NVMe SSD slot  
External Storage  
USB HDD support  
Optical Drive  
4K UHD Blu-Ray Drive  
Up to 100GB/disc  
Video Out  
Support of 4K 120Hz TVs, 8K TVs  
VVR (specified by HDMI version 2.1)  
Audio  
""Tempest"" 3D AudioTech"
"It's just doing the same thing used cars are doing. May 2023 you can find pretty good deals. Look at RTX 3080TI on amazon new it's like reduced price, plus $250 off, plus $100 checkout coupon. $899 for brand new 3080TI I feel is pretty great.

&#x200B;

But some cards pricing is crazy and also places like Best Buy just left the old prices on so it's like $1450 for a 3-series card. Same with used cars you also see 25 year old used Toyota Camry's selling for $6,000 when it's worth close to zero zero zero. Too many people spent too much money buying when things were crazy high priced and now don't want to take a huge loss. Trust me they ARE going to take that loss but they are just keeping the price high until it never sells and they can do a tax dump thing.

&#x200B;

TLDR there are good deals now. And there are still a bunch of bad deals."
It's not a fun time for PC gaming when GPU prices are easily comparable to how New York real estate works, haha.
"The companies offering â€œdeep discountsâ€ (whatever market segment) right now are generally the ones struggling with Cash Flow (I.e. money is tied up in product - stored in warehouses or wherever) as they need to invest in future product, for many consumer-product companies this is also tied to cost-reduction for future products that can sell with a better margin at the same retail price.

What does it all mean in this context? Basically, GPUâ€™s would only drop in price if the manufacturer desperately needed to clear some inventory to bring some cash back into the business. Pure electronics are notoriously â€˜cheapâ€™ to manufacture, most of the cost comes from amortised R&D so they will most definitely sit and wait for all this to blow over."
"The difference is that New York real estate doesn't become obsolete (or at least it takes multiple decades for a building to be), meanwhile noone wants to buy a last gen GPU at MSRP.

Imo Nvidia is betting that the AI boom (which largely relies on CUDA and so Nvidia hardware to train the models if I understand correctly) will carry its sales like crypto mining did. 

I believe Nvidia increasingly see gaming as its comfort zone where they can dump whatever is not bought by the professional market which is bad news for the PC platform unless Intel becomes somewhat competitive (imo AMD isn't trying in the GPU dpt now Ryzen sells so well)."
"> Sales may be down from previous quarters, but it sort of works like New York real estate.

There are several key differences: 

* Unsold GPUs become outdated and lose value within a year or two. 

 * The real estate market is artificially propped up such that if you keep a property empty, it'll still be worth the same or even more. 

* The same AD102 die can be sold as a $10k L40 professional card or a $2k 4090 gaming card. So if Nvidia has no incentive to flood the gaming market with supply if they have enough professional demand. 

 * Real estate can technically be reconfigured for more profitable use cases (e.g. office vs residential vs retail), but it's an expensive/litigious process so you're often ""stuck"" with whatever use case it's already configured for, so you're more likely to need to keep it unoccupied."
I'd say GPU prices at least for lower models are already breaking. 4070Ti released month ago was already few times on sale below MSRP, in once case it was already $250 below local MSRP.
 If they really think prices will return to mid mining boom state, they are insane. I doubt that's it, more likely they are waiting for the last enthusiast to buy their new gpus and they will lower them soon.
"Prices are significantly down over the last 8 months though.    Though the last 2 months have been fairly flat. 

Sales as measured by total $ volume are down, but by unit volume it is not down as much.

The launch of mainstream next-gen GPUs will be where we see what really happens.   

The high-end stuff being high right now is not the same as the overall GPU market -- where RDNA2 GPUs and 3060tis are way down from their highs.  I do suspect NVidia will hold prices as high as they can, but I suspect this will be like the 2000 series and we'll see significant decreases in the mainstream stuff about a year after the flagship launch."
The gpu market is a bit different though. If gpu value for dollar isnâ€™t rapidly improving - its becoming more like the car market - maybe the new CPUâ€™s should be all high-end - low- and mid-tier can be filled with second hand gpuâ€™s. Which is a horror story - but a horror story that is close to be real.
"Times won't get good ever again for GPU sales. 

GPU mining is over, only a few will pay absurd prices from now on."
Are you Louis Rossmann, the NY real estate guy?
Capitalism sucks
It's worth noting that 'not selling out' is different from 'selling badly'. If prices are static, it probably means that cards are selling as well as is tolerable for Nvidia and partners.
Sorry there's been a manufacturing shortage of lawnmowers, it's now 1 lawn for $200.
"Adding to this - you'll also have the chance to mow some of those 15 lawns you didn't get to LATER...   


nVidia and AMD are basically pushing out demand and harvesting more profit right now."
AMD and NVidia both love the high margins and both are afraid to rock the gravy train. Thankfully Intel is coming along and trying their best to take advantage of the situation. Really looking forward to Battlemage, especially since Intel isn't stingy with memory either.
While I feel you are right, that's gonna be the death of pc gaming if this trend continues: More and more devs stop bothering with a market segment that doesn't keep up in terms of volume,  more people switch to consoles, and that's just a vicious circle...
"Math doesn't check out.

They're charging triple the price, but these high end cards are only for the top 10% of the market.

Say their profit margin was only 50% at normal price (the prices they changed for the last 2 decades). So now, at triple normal prices, their per-card profit is actually a whopping 5 times what it was.

That's still not nearly enough to ignore 90% of their customers.

They're waiting until the suckers stop buying these overpriced cards (why wouldn't they?) but they have to release something for the bottom 90% eventually (and they know it)."
Well....if lawns are cuda cores, it would probably be better to sell 5 $300 GPUs with 3000 cuda cores, than to sell 1 $1600 GPU with 16000 cuda cores.
Easy... if 4 out of 20 lawn/house owners tells you they don't need you anymore you still got enough customers to make a living but if your only 4 give you the finger because you are too expensive then you are out of business. Some costs come back whether you earn income or not so its more important that money keeps moving.
"Yeah it's retailers getting fucked here. NVIDIA is fine.

NVIDIA won't say it but, to paraphrase Don Mattrick: NVIDIA has affordable midrange cards, they're called the 3000 series."
What do you think will happen?
4070 is probably gonna be nearly 700 dollars. 400-450 for a 500 dollar 3070 is pretty lame when AMD has 6700 XT's for 350 and will probably drop them lower should Nvidia cut prices.
">it will fly off the shelf regardless of how shitty the value is.

It pains me how accurate this is."
"That point intel comes in and brings in better priced gpus in order to gain market share, thatâ€™s what they need to do to gain market share anyway.

I disagree with general sentiment about NVIDIAâ€™s success. I think they are most vulnerable to a disruption by intel. Intel going into gpu hurts nvidia more than amd."
except it's not "the internet", it's Steve from Gamer's nexus talking to retailers. The 4080s were a complete bust.
anyone with 400-500 probably already bought a used 3080 or 6900xt
"Accurate. 

And they are selling in many places around the world.  Best buy, Amazon, and Newegg all have most models in and out of stock with only a few models in stock at any given time. That's not normal conditions."
If you are in the market for budget GPUs because of you know, budget, it makes no more sense to go for PC, go consoles.
I mean, microcenters pretty much always have dozens of rdna3 and Lovelace cards on hand at any given time. Models of the 7900xt are pretty typically below MSRP at this point. Hardly sounds like these cards are flying of the shelves.
They are selling, but also selling less than usual. You don't need to sell as many when you're making several times the profit on each card.
yap, people are detached from reality.
People have reached out to retailers and sales have been lower than expected, especially on the 4080.
If you sell one card at a 500 dollar profit instead of four cards at a 100 dollar profit each, you are making more money and selling record lows of cards.
There are lots of headlines saying GPU sales are at â€œrecordâ€ lows lately. The part many people are ignoring is that similar headlines are popping up for; home PCs, laptops, smart phones, TVs, kitchen appliances, cars, etc.
I think there were a few stories that seemed to actually have some numbers, but 90% of posts I've seen were just people losing their minds over video cards actually being in stock, like they're supposed to.
230 is too much in our country
[removed]
6000 series is a saviour of these dark times. I'm getting a 6700XT to tide me over for a couple years but you can be set with a 6600 for $250
"This is true, but retailers can tell manufacturers that they're full up on stock they're already making a loss on, so they won't be ordering more, even if they cut the price of new stock.

The manufacturer could go to another retailer with their wares, but most likely that retailer is *also* full up on stock.

This usually results in the manufacturers giving them a rebate for what they have so they can drop prices."
1000 times 1000 is equal to 10000 times 100. I get your point though.
Sure, but what about 5000 for $250 each?
"4) I dont consider $250 to be budget. We used to be able to buy decent cards for as low as $100-150. Heck my first budget GPU back in the day cost like $80. 

But yeah, both AMD and intel are offering decent $250ish offerings right now. Only nvidia is truly insane with prices. And let me just say, with all that brand perception crap, nvidia just lost a customer in me with that crap. They need to be brought back to earth. i dont care about what a premium brand they are if their prices are crap for the value. They can go bankrupt for all i care with their attitude. 

5) ""Just start your own GPU business bruh!""

What nonsense."
"The 4090 costs a bomb to make. That is a huge piece of silicon on recent (but not bleeding edge) node.
The 4070ti manufacture cost are likely within 10% of the 3060ti. The 4070ti is shocking value in terms of cost to manufacture vs. cost to consumer. Amazing engineering to wring so much performance out of so little.

Source: I design consumer electronics for a living and have had to keep several lines in manufacture over the last 2 years. I have a decent understanding of costs and markets.

EDIT: Clarified what I meant by ""value"""
"People who buy 4090s don't really care about price either because they use it for their job OR because they have enough disposable income to get the best component whether it costs 1500/2000/3000 (btw I would argue this category was much smaller 10-12 years ago).

I'm not sure this logic applies to the vast majority of the market which is probably looking for value in the 200-600 range. Imo if your budget today is less than 400 for a GPU (so approx a $900 PC) you're effectively priced out of the hobby since everything under threshold this is just poor value to play recently released games."
"Eh the top end always is priced at a relative premium when there's little competition. The 2080 TI was ~30% faster than the 2080 but was 60% more expensive (MSRP was fake, even Nvidia called it a 1200 dollar card in the Ampere reveal). The price difference between tiers is almost always greater the higher up you go (3050 and 3060 are closer in MSRP than 3070 and 3080 and definitely 3080 and 3090).

In this case the top end looks a lot better because the 4090 price to performance is roughly the same as that of a 4080. If this was like before where the 4080 would be 85% of 4090 (33% more money, ~15% more performance gives the same value ratio as 2080 TI vs 2080), then many people who had a budget of 1200-1700 would just get a 4080. Now those people are either buying 4090's or waiting.

Which is why the 4080 is selling horribly. The 4070 TI has a slightly better value ratio and its price is more mainstream, so it's doing better. Not many actually got the 3000 series for MSRP so it appeals mainly to Turing and Pascal people."
"The thing is that market volume vs price is a bell curve. The vast majority of people are buying GPU's for between 300 and 600 dollars.

Selling only 800+ dollar GPU's gets them margin but not volume. They might upsell a few 4080 buyers into 4090's, but most are just going to hold off since the 4080 is both mediocre compared to the 4090 *and* vs prior gen GPU's.

Plus if they have both 800+ dollar GPU's and cheaper ones, why not release both for even greater sales? The cheaper ones existing may prevent a few people from being upsold from a *still* at MSRP 3070 into a 4070 TI, but they'd also sell a lot more.

The reason why they *aren't* doing this is because they need to get rid of 3000 series cards in the channel."
Fair, but from what I've seen they're producing record numbers of cards.
Yep. I had to wait a month to find a 4090 FE in the UK and every time they drop they sell out quickly.
"4090 supply is beginning to catch up to demand and prices are dropping. It's now fairly easy to get a 1600-1750 dollar model.

4090's average selling price decreasing and availability increasing means the 4080's are going to need to as well. Which may trickle down to the 4070 TI."
">he believed history showed that gamers would continue to pay...

I think at one point the 'addiction' arc must be discussed.  And keep in mind that teenagers maybe a big percentage of that consumer base.

Promoting overpriced 500 euro cards to teenagers and even kids to play games  for me draws a dark picture for nvidia and amd .

There was a time in some countries where you could not promote game adds in television networks in prime time. Now i guess we throught that policies to the waste basket. **Now everything goes ....**  We keep discussing the nvidia pr spin and noboby holds them accountable for maybe being a game company that sells products mainly to unprotected youth.  And a product that is bundled with addictive games . So yes. Kids wants to play.. so parents will pay.."
You make some good points, but the value of putting the memory controllers and cache on chiplets, is debatable.
">Thatâ€™s not to say AMD might not have done the same if the roles were reversed, but I donâ€™t think they are that cutthroat.

AMD priced the 7900 series to the absolute maximum it could get away with. The XTX is already priced against the 4080 which offers higher performance and better featuresets. The XT itself is literally priced so high that it's actually a worse price/perf value than the more expensive XTX. And all of this came at the same time AMD talked big at the 7900 launch event about how its MCM design choice was saving it money and lowering fab costs. 

AMD had a choice here to regain market share or to eke out a wider profit margin, and AMD smashed that profit margin button with vigor. So it doesn't matter if AMD did regain the perf crown, we know they would price what they think the market can comfortably bear, and the last two years has changed both companies views on what the market will bear."
"That was clearance pricing, they had like no supply and it will likely never be restocked.

It was a bit baffling and sudden tbh, AIB partners are probably pissed. Not sure why they decided to cut 40% off the price suddenly instead of just organically discounting over the past few months."
Ah yes, the anti capitalist far left members of this sub, who want demand to drop so that prices drop. Sounds like capitalism to me. Who is calling for nationalizing the GPU industry here?
[deleted]
Actually its crypto miners who are the biggest buyers, gamers were a large percentage prior
They are. Console games spend way more money on games that require internet and game passes season passes and dlc that if you dont get you want have players to play with. This is what they wanted since steam came out and saw was possible with cod mw2 dlc sales. Downhill ever since. Hell the new xbox doesnt have a cdrom. Do you really own anything if its all digital and (rented)
And CUDA target consumer is shifting to people running machine learning models. Which is more commercial than gaming = higher prices.
"> New York

m8, it's like this across the country"
PC gaming is great if you just holds off on those AAA games that go all in on graphics.
Thank goodness for console gaming. Quick and easy and just meant for gaming. You can  still get a mid tier gaming PC for casual games and indies though.
The used market is great though. You could probably get a 3070 for 300$ or less.
[deleted]
Remember that the current "crypto winter" is not the first and is unlikely to be the last. This shit comes and goes in waves. It's entirely possible that some other coin becomes ultra-profitable within the year and all of a sudden $1600 for a 4090 sounds like an absolute steal (that the scalpers will capitalize on if Nvidia doesn't).
AMD also makes the GPUs for Playstation and Xbox.
The gaming industry is basically a money printer instead of entertainment nowadays. Don't get me wrong some games are great, but a lot of the good ones don't even need anything amazing.
AMD is going to try and make APU's that replace the low end of GPUs. Intel is sort of fighting a two front war and seemingly understands that; Arc cores/tiles are for use in discreet cards as well as for tiling on processors.
"It doesn't matter if prices are significantly down, they are still overpriced, starting with jacked up prices on the 2000 series, long before the US and EU started the money printer (read: inflation narrative).

Mid range (like the 1060 6GB I've bought for â‚¬200 a pop inc. VAT) used to be $200-300 for as long as I can remember since last Intel's agp dGPU being sold 2 decades ago. Now, you can buy for â‚¬300 a 3050, just to proof my case here what kind of value you get in return (a low end card vs mid range) for 1.5x the price it used to cost several years ago.

The entire gpu market is still a ripoff, while people think it's a good deal to buy a 2 year dated gpu near a jacked up MSRP on the 2nd hand market is throwing 3x salt in an open wound (as used prices for 2 year old usually is 50% or more depending on the usage and condition). Hell, I even heard and read stories from miners selling their old equipment for the same price they've bought them for, so they ran zero losses on investment + profits from mining. It's pretty self evident here who the winners and losers are.

If you can't control your emotions to buy right now to game AAA titles, then you're even better of buying a PS5 in terms of value.

I hope Intel will make a good impact on the gpu market to increase competition and better prices. I'm not too impressed with the gpu market for several years now, the perf/Watt is not as good as the relatively much cheaper cpu market's perf/Watt. Cranking up wattage to get more performance != innovation. I'll wait until '24 to see what Intel's Battlemage is going to bring on the table (so far I'm impressed by the A770, pretty good value (the hardware and features like hevc + av1 encoder) on paper. The only thing what's preventing me to buy one yet is the high power consumption in idle and perf/Watt, otherwise I'd have bought it if the price would be around $/â‚¬300-350. I've already read the workaround messing with aspm's powerstates, but that option is not available on my motherboard, plus, I'm running multi monitor setup where that fix does not count and 2 nvme ssd's that rely on pci-e, so messing with the aspm powerstates is not always a smart thing to do.

TL;DR; The gpu market is still messed up and overpriced as of now. When affordable mid-mid high end become available, price/perf and perf/Watt improves above small margins year-on-year, I'll reconsider. If the jacked up prices are the new norm, then APU's/iGPU's + consoles are possible the way forward and the biggest revenue coming from the consumer masses is then not flowing to nVidia, maybe they don't care anymore now that they are in the data center and ai business that most likely is way more profitable than this very tiny gaming enthousiast niche and bulk combined, as they still will be profitable is all what matters to the corps to please their shareholders."
AI will need absurd amounts of GPUs and NVidia is leading there.
$300 
The ol' "manufacturing shortage"
Well some geniuses banned gas lawnmowers, so 500$ now
"And they also get to spend that time doing some other hustles (use some of that TSMC wafer allocation to build products for markets beside consumer graphics.)

And then later my mow some of those lawns as you mention.

My theory is we may see some price drops if the overall market turns the corner such that demand for those other products slows or other companies cut back their orders and TSMC redistributes some cheaper allocation to NVIDIA and AMD. 

If that happens we might see a volume play by one or the other while they work through the pent up demand."
[deleted]
AMD's GPU margins were 16% last quarter. NVidia's were 60%
intel is not coming with anything.
[deleted]
[deleted]
They actually did say it. In their 3Q conference call, Nvidia was explicit about their intent to overprice and undersupply 40 series, so as to sell off their stock of 30 series.
That right there was the final straw that pushed me into upgrading my 1060 now and buy a 6800xt ($519). He cemented the fact that Nvidia wouldn't have decently priced mid-range offerings on 4xxx, and the recent rumors/leaks of those cards have supported my purchase. AMD hasn't shown they will be competitively pricing their cards either (looking at you 7900xt).
Does their 3000 series run 4K / 60 FPS fine?
strong datacenter revenue and probably decent gaming revenue due to it cratering somewhat in the past.
I suspect the shareholders will applaud Nvidia for keeping their profit margins.
"Strong datacenter sales but bad gaming revenue. 

Rise of chatGTP and interested in AI in general has been surging Nvidia stock prices for the past couple weeks. 

Having said that, Nvidia really likes fudging their numbers so it's anyone's guess really."
Webistics miners will continue to jack up prices
"So the premise of the OP is that GPUs are simultaneously not selling well and also not seeing price cuts. I just wanted to point out that while that may be the narrative on this sub, Nvidia and retailers are definitely doing price cuts and successfully clearing old inventory. 

As to the 6700XT pricing, I agree that it's the better value card. That said, it says something about buyer preferences (right or wrong) that the 3070 is still at $500 and NV hasn't felt the need to drop prices yet."
Ehh I donâ€™t know, people are buying PS5 these days.
"Which retailers?

Because Amazon, Best Buy, Newegg and all the other major large outlets do not normally disclose internal sales figures to random people on the internet. 

You might get the occasional specialty or â€œenthusiastâ€ retailer who reveals sales information, but they are usually a very small fraction of the total volume of these products."
Sure, but Nvidia margins have pretty much always been in the ~60% range.
Sure but Nvidia's profit continues to decline, so it's more like selling one card at $500 instead three at $250.
"a lot of headlines are fake news  

Remember how all the teletubers shitted on the 4xxx series?  
You are following fake news stories that are detached from reality."
Yes and it's also bad news for the manufacturers of those products.
Comparing 4090 to rdna 2 is silly, just about any nvidia will outsell any Radeon.
People say that, but the 6k series still looks super expensive compared to the market I bought an RX480 in, not to mention earlier eras.
[deleted]
I got a 6650 xt for 250.  The heat sink is only 2 heatpipes I think but so far I'm pretty happy.
"Yep. 6000 series is truly the new ""everyman's"" GPU. 

Anyone who wants to upgrade their old 1060s/480s/580s should look into the 6600 series and maybe the 6700s if you want a step up."
Apparently you don't ;)
Still not even worth considering. You have to keep in mind that GPUs do cost a substantial amount of money to make.
"AMD's margins last quarter for their ""Gaming"" category (just GPU's, CPU's and Mobos are in the ""Client"" category) were just 16%. That's probably pulled *up* a tiny bit by 3 weeks of 7900 XT(X) sales which probably have the highest margins out of all their current products.

Cheapest cards also tend to have the worst margins. They *cannot* sell a 6650 XT for 150 and profit, nor the 6600 at 100. Mathematically it doesn't work at all.

NVidia's GPU margins are tougher to find in their reports, but their company wide margins are 60% and GPU's made up ~40% of their revenue last quarter. They were able to sell 2060 12GB (originally destined for miners) for 240 on Amazon in Q4 last year and presumably profit, yet the cheaper to make and worse in *every* way (except a small gain in efficiency) 3050 is *still* chilling at 300+."
Everything is more expensive: rent, electricity, restaurants.  "Back in my day" is useless.  $250 is a budget dGPU.  The iGPU in a budget CPU is even cheaper than what you originally paid for a GPU and is way faster.  That's the cheap solution.
TSMC 4nm was the most modern node in production when the 4090 launched, and we probably wonâ€™t see any products on a newer node until September 2023, a year after the cardâ€™s launch. It doesnâ€™t get much more bleeding edge.
You mean shocking value for Nvidiaâ€™s bottom line, right?
"> The 4070ti manufacture cost are likely within 10% of the 3060ti.

3060 Ti costs ballpark $200 to manufacture. Even if we pretend the GPU die costs the same, there's more than $20 more VRAM on the 4070 ti alone. Then you have to worry about the additional cost to make PCBs and fit components in 2023 versus 2021."
"I estimate the GPU silicon alone for the 4070TI vs to 3070ticost \~$100 more for AIBS. and VRAM cost $20 more. throw on some additional costs for cooling and pass through some margins, and you have a card that cost \~50-60% more to make.

It uses more memory, and 4nm is almost triple the cost of SS8nm. Very unsure how you arrived at 10% more to make . . . Are you referring to the cost excluding GPU and memory?>"
I totally disagree. Sure some people may be willing to pay even more, but itâ€™s these kinds of bad generalizations that make any discussion pointless. Sure, see how many people buy it if the price was $30,000 by your logic.
Debatable *now*
AMD literally launched their flagship at the same MSRP as their previous flagship. Despite this they are still being painted as evil and greedy even though there has been record inflation and prices have gone up for components.
[deleted]
"It's weird. 

I moved from an area where a house can be had for $150,000 easily, and not a small house either. 

There are well priced homes, they just exist in places you don't want to move to unless you already have a stack of cash from living somewhere that you can actually make money."
">m8, it's like this across the *world*

Fify"
I doubt some place in Alabama is experiencing outrageous real estate prices. From that I can see, it's the coasts
Even in this market you can build a pc that will perform as good if not better than a console for about the same price. A lot better than a console if you donâ€™t mind used parts.
In Europe the prices are miserable right now. You'd have no hope of getting a 3070 less than $450-500 equivalent
Youâ€™re completely right, I was oversimplifying for shortness & the masses by saying R&D: I include tooling, NRE, set-up, down time, cp/cpk etc in my own head.
Remember Luna? No? Exactly. Well they relaunched it even though everyone knew it was flawed at best, a grift at best and the prices still go up with 2.0. People will NEVER learn and are quick to forget. We are in around cycle 4 for crypto and right now AI has taken over until crypto comes again or a new hot tech comes around.
I totally disagree, GPU mining got so big because of Ethereum network effect, the opposite isn't true. Crypto users don't care about the consensus method as long as it's secure and distributed, they care about the dapps and more importantly where the liquidity is.
"Bitcoin is really the only proof of work chain left, and that is dominated by specially built devices. Pretty useless to mine with a GPU.

Not sure GPU mining is ever coming back."
That's great, but Ethereum changed to PoS forever, not just for fun
We really don't know what's going to happen because crypto has had its crazy cycles in low interest rate environments. Interest rates are high, so big and institutional investors aren't likely to gamble on crypto atm.
We are basically paying the price for the people with more money than sense. At the end of the day just cause you *can* buy a GPU at stupid scalper or inflated prices doesnâ€™t mean you *should*. If more people would stop buying from scalpers and just wait the market would stop slowly shifting during these waves.
While crypto summer is likely to return, I would be much less confident that the next crypto boom will use GPUs. GPUs aren't actually very good at solving crypto math. The only reason ETH used GPUs is that they intentionally designed their algorithm to be GPU-friendly and ASIC-hostile.
">they don't care

If nvidia and amd can stay afloat since they don't need to rely on gpus, does that mean they don't need to budge their increasing prices which at this rate it will never end? I assume they won't lower any 4000 gen card price til next gen late 2024 despite not many people buying the 4070ti in 2023 so far as shown by the availability since they can afford to keep inventory.

Budget gamers who expect 1000 series value won't be getting that anymore in the future. Seems like we're at a point in time and circumstance where sub $200 budget or even mid to high end tier gpus in general aren't worth the company effort, time, money, resources, opportunity cost that could be used somewhere else more profitable.

Once the budget 1600 and 6600 cards are gone they would just keep selling at msrp $300+ gpus in the future. Nvidia and amd would start increasing entry level gpus by a hundred each gen with a $300 4050, $400 5050 and so on until no more gamers feel its worth the money.

I guess if there was value, it was the 1000s, 3080, and amd similar counterparts. If I were to get into gaming, I might just call it a day with 66/700xt or 4070ti since no scalping unfortunately seems like a deal. I get how people say 4070ti is a bad deal $100 more than a 3080 with small performance gains when tech should be getting cheaper for the consumer. But it's there without the scalper hassle, markup, and messing up warranty ownership, coverage."
"Somebody up the road is willing to pay 400 because they heard there was a mower shortage, the asking price is now 450.

*2 years later*   
people are used to paying $450, no need to go back to the old price. Drop it 10% after launch if the only other lawnmower company in town gets its act into gear."
$300 + (10% sales tax) + (15% tip) + ($50 donation to starving kids) + ($30 misc fees)
"If it keeps going up, at some point it would be cheaper to strap a weedwhacker to an old Roomba that is modified to go off-terrain and have it shred the entire lawn.

/s"
"Intel is only the savior because they have no market share and a product that's not as stable as the competition. Until both of these issues are solved, it will be forced to keep prices down.

 But once they're in a comfortable spot, the situation changes. We all know how Intel can be complacent with their products. They might not cut into R&D like they did with their cpus pre-Ryzen became a thing, but they would absolutely love to join this high margin gpu market.

TLDR: Intel will be cheap until the gpu remains a little bit shit"
They're not because the Arc sucks right now, so it's not a threat to nvidia. If it performed as expected, things could've been different.
If itâ€™s not their long term goal then I donâ€™t know why theyâ€™d even attempt it given the cost involved.
With Nvidia's margins, they don't have to worry about a product selling well, they just have to make sure they don't produce more than the market can absorb.  The only downside is that it can result in shortages, but that's a problem for consumers, not for Nvidia.
Sounds like AMD's problem is them making an inferior product.
Is that gaming only or does it include productivity cards? (I assume data center etc is excluded)
Battlemage is supposed to trade blows with the 4000 series and releases in Q1 2024.
"But that's the thing, why would I buy a a $300 gpu on top of the rest of the hw to play at 1080, when I can spend $500 on a console that will have at least similar specs but way better better performances at 4k because games are actually optimized for consoles.

And let's be real, all ports that have released so far this year are opmized like shit. And I am saying that as a 3080 owner, I still love my pc for MP and other comeptive games (that are actually properly optimized), but for the rest, SP games in particular, I only see benefits in buying a console and not bothering updating my gpu for a very long tong time."
"> Despite Nvidia and AMD being able to build the same GPU on 1/3rd the die size, they will charge more or less the same price as previous gen. 

At the low end of the market, the GPU die itself costs very little. Whether the die is 50 mm2 or 100 mm2 is almost irrelevant -- maybe Nvidia is saving $5 each. That's less than it costs them to fit 1GB of VRAM. Meanwhile, the component costs are increasing significantly for everything else, from PCB to MOSFETs to power ICs to ports.

The whole low end market will be swallowed by iGPUs within the next couple generations. Look for dGPUs to start at $300+, maybe even $400+. The economics simply don't work for sub-$200 parts anymore, even when you make a part as awful as the 6500XT."
How has the 6800xt been, I hear any AMD gpu has weekly bullshitting to get games to work/monitors to stop flickering/blacking out
With DLSS they do alright
Note that cards getting sold for crypto were counted in the 'gaming revenue' section of their revenue reports, the Q3 cratering is not going to suddenly bounce back up to the same level.
"Decent compared to AMD sales maybe, not up to Nvidia shareholder expectations. Some may have bought the stock based on last year's numbers, and are in for a nasty surprise.

I expect a drop in share price (after it was ""mysteriously"" pumped in the last month leading to this report). If we're lucky Nvidia will drop prices. Looking at the layoffs in other companies, they might just cut expenses (meaning: employees) instead and keep prices high."
A next gen gaming console for the price of a mid-tier GPU
Actually according to sales numbers, they're buying Nintendo Switches.
People usually quote Mindfactory's numbers as though they were representative, but a fairly boutique German e-tailer doesn't feel like it represents a whole lot. They had some very weird sales patterns in the past too if I remember correctly.
"Mostly smaller retailers that HWU and GN reached out to. But if they're seeing way less demand, then the larger ones are as well.

We can also see on NowInStock that 4080's almost immediately get restocked after they sell out (which pretty much means that they have way more supply than demand). Across all retailers."
What has to do anything with the other? Also, do you only read headlines, that are designed to grab attention?
$200 RX480 in 2016 is equal cost to a $250 6600 XT today, considering inflation. I think the 6600 XT is a better value today than the 480 was back then, actually.
Yeah people in my country think 6600/xt are great because they can be found for ~280/~320 euros and I'm just thinking these are basically the rx 570/580 of this gen why do people think this is good?
I wonder how viable an Intel GPU would be as an Al accelerator
If both strategies earn the same amount of money then why is one better than the other?
Missing the point. It's like lawns. One lawn that's 16000 square feet or 4 that are 5000 each.
"Oh noes a 16% margin, the horror.

https://www.brex.com/journal/what-is-a-good-profit-margin

Sounds like they're in good shape to me."
"First of all, the prices rose significantly higher than inflation. Two, most of the current inflation wave are corporate greed. Three, the prices started rising before 2021 and ""inflation"", and four, this sub is full of rich people so out of touch with the rest of reality it's like they dont care people are being priced out of the market.

EDIT: Also ignoring that until 2016 price/performance steadily rose at a pretty consistent rate, then from 2018 onward we just saw higher prices and relative stagnation, especially on the nvidia side. But hey, YOU can afford a GPU so its everyone else's problem, right? I hate this sub sometimes.

If demand is low, and prices are high, and GPUs arent selling, it's quite clear what the problem is. Just virtue signalling ""inflation"" is the most useless thing you could post other than weirdo ""look at me and my fancy new status symbol lol poors"" 4090 posts."
"Indeed! Shockingly high.

I wish the product I work on/have worked on sold with such a healthy margin!!!"
Our NVIDIA H100 GPU racks start at $30,000 just saying hehe
"The problem is that the 7900XTX isn't that good performance wise for a flagship, it's not even remotely close to the 4090. Overall it's not the generational leap that was rumored.

The most disappointing part is that people expected lower prices with the MCM architecture and it just did not happen."
"Evil? No. Greedy? Maybe. Friends? Nope. It's a gross oversimplification anyway. The XT price didn't have to be a **worse** price/performance ratio than the XTX, but that was AMD's decision. 

I don't think some people realize how odd this situation is. It used to be the lower down the stack you went the better price/perf value models became. This generation suddenly the value gets worse on AMD's side, and it almost doesn't change on NVIDIA's side (4080-4090) because the 4080 is priced so high. At launch day the best case was a 15% performance gain for spending 2x the price on a 3090 over the 3080. That sort of pricing trend was normal behavior. Only now with VRAM capacity becoming an issue is the performance difference widening in some games. 

Frankly I'm more worried AMD will price itself right out of the market regardless of MCM advantages. AMD's market share is floating between 8-15% depending on where you look and despite the even worse value of budget NVIDIA cards AMD reportedly is still losing share. Intel is here to stay and not afraid to offer good value on the low end, so simply existing as the ""not NVIDIA"" option will no longer keep AMD in the market. I haven't even mentioned the ""value"" of the $200 6500XT yet. What little market share they have is going to get eaten alive by Intel if AMD tries another desktop/workstation Navi 24 or more 7900 XT pricing shenanigans to upsell higher models over the next couple years."
"Horrible take, because 6900 XT was pure crap value compared to 6800 XT, which was almost just as good at 650$ and 7900 XT, which is definitely the actual replacement of 6800 XT -- literally every hardware enthusiast thinks the same -- costs 250$ more.

If 7900 XT was 700$ then this argument would have made sense.

Not to mention that 7900 XTX is definitely a worse card than 4080 overall, 6900 XT was definitely better than 3080 in terms of raster and it also has enough vRAM for the future."
"> I never said anyone is calling for nationalizing. If you want to be a smartass, at least be smart enough not to use fallacy.

Anti capitalist implies they are socialist, which means they want the workers to sieze the means of production. A lot of what you call ""the left"" are still capitalists, they're just liberals and socdems and stuff. 

>Want an example of the far left idiocy? Go read the thread that was titled something like '4080 pricing is a literal war crime'. Then read through all of the delusional bullshit they say. That's what this sub has been turning into for a while. They may not necessarily identify as far left, but if it quacks like a duck...

I mean to be fair im one of those ""far left"" whackos according to you. Wont go so far to call 4080 pricing a war crime, but at this point, we should be doing a federal investigation and possible anti trust stuff into the mix due to their insane price gauging."
Or you work from home. As long as the internet is good.
[deleted]
"Yeah I would say it's even worse in Canada. Avg home sale price in Mississauga, ON, was about 630k in 2018. This went up to over 1.1m in 2022. Now we are hovering around 970k. 

These are not giant houses either. Think of a 3 bedroom duplex basically (which we call ""semi-detached""). 

It's obscene. When you consider that median household income is about 80k - 100k a year, you're looking to be buying a house that is 10x your household income. By the time you pay it off in 20 years (optimistic), especially given current interest rates (6% or so for a max of 5 years), you're probably paying off your house 2-3x over by the end of it all. 

Imagine spending $1.8m on a fucking house that's labelled $900k."
If you compare local wages to the local housing markets youâ€™ll find that it is. The average house hold income in my area is like 30-40k. But any home without wheels is 300k or more. I make 60k a year and am having trouble buying a house in the area where I make more than 2 standard deviations above the mean.
"Sorry mate but that's just not going to happen, and I really don't think the used market is fair game for PC parts if you're not looking at used consoles.

The Xbox Series S launched for $300. What PC can you build for that price which can play at the level the XSS does? 

Even if you consider the PS5 or XSX, which released for $500, you still can't build an equivalent PC for the same price. 

Consoles and PCs have their place. I currently game on a laptop because it's all I have, but ideally I'd get a console because it's so much simpler and I can game from the couch with the press of a single button. It's also simpler in terms of setting it up/buying it: I don't have to look up the system requirements, troubleshoot with driver updates, fiddle around with the settings/FPS etc. 

Some prefer PCs. Some prefer consoles. No way to say which one is objectively better."
I'm in Europe(Romania). I just saw one for 320$ a few days ago.
"Pretty sure the main point of cryptocurrency is to sell them to suckers.

Otherwise, the ""real world"" price wouldn't matter as much, and downturns would be more permanent as the price levels off."
good riddance
I keep seeing youtube videos from chia miners...
"No crypto that wants to be mined by lots of people would want ASICs to take over, though. No one buys Bitcoin ASICs, for example, because the difficulty spike associated with lots of people buying a new ASIC quickly makes them obsolete and non-profitable. The only people that benefit from ASIC use are their developers/builders that can mine on them alone before the difficulty goes through the roof.

And the Ethereum algorithm wasn't so much inherently ASIC-resistant as it was easily forkable: their ""resistance"" just meant that could react to a new ASIC coming out by quickly changing the algorithm (turning ASICs into paperweights) and propagating it through the whole network without drastic consequences. While general-purpose hardware like GPUs doesn't care that much what math to perform."
"Yep, that's a very real possibility, although I don't think this will be sustainable for AMD, unless they move to other targets, such as data centers, ai, cloud services, etc., which Nvidia has much better performance in than AMD and I assume most of their 4090's are sold to them rather than gamers.  
From my own experience, I don't know any gamer in real life having bought it, except IT professionals who have the need for CUDA cores.

The difference between work requirements vs gaming needs to be taking in to consideration, as those gpu's are assets when they generate income (whether it was mining or the need for other work that relies on them) vs consuming (i.e. gaming).  
For example, if you mined with a 3090 during the last 6 months of the peak, you'd got your ROI back. Now compare that to a gamer who doesn't make any money from it and upgrades each 2 years => â‚¬2000 4090 / 24 months = â‚¬83,33 writeoff for the gpu alone and half of that for 4 years. This is very hard to justify for modal income for gamers in even the western world where buying power right now is at levels since the great depression for the first time, whereas it was a good deal for a miner.

We're now in a situation we were in 2008 as well with AMD and Nvidia, which were sued for pricefixing and settled before it went to court. Shortly after, the big crash happened and prices went below MSRP significantly until repriced MSRP for the next gen was over half of previous. The economy is in a similiar situation as in 2008 as well, ironically, with Bern Bernanke recently calling for a soft landing, which they (the banksters) had publically declared as well in dec 2006. We all know what happened.

This also happened with the DDR4 market (pricefixing, until the FSC stepped in and magically prices went down since).

Everything in life happens in cycles, so the high price schemes will not last. I expect no real price decreases (other than discounts on 2x prices is still overpriced) until 2024 (optimistically) and most probably 2025. Intel can stir the pot up heavily with the Battlemage in Q1-Q2 '24 if priced right. Until then, AMD and Nvidia are milking gamers dry.

Even if in the worst case scenario possible, which is highly unlikely, but prices will increasing each gen with $100, then we might see more powerful APU's/iGPU's coming in the low-mid range eventually, because the entire gaming industry would then be severely limited (as a trickle down effect in the medium-long term) so they have to adapt to certain predefined standards to work on, such as console market are right now. This would be bad, because we could end up in a situation where we move to SoC and the freedom to choose parts will then be severely limited (like you have with consoles, nucs or even worse, smarthpones.

Another thing to consider is that there are many people looking for an affordable upgrade. The steam survey proves it, many people are still on a 1060. The many reddit and youtube comments also give indication of this. So even if there are truly good deals, they probably will all sellout before you have a change.

If you're in need right now with budget $250-300, the A750 looks promising if you can stomach the well known issues, otherwise AMD's 6600-6700 and simply nothing from Nvidia (well, you can buy bad perf/price if you want to buy 250-300 low end that is worse than the 200 for the 1060 6gb). If you can wait, then def do, like me.

Personally, I'd not buy second hand (especially mined ones), but I can imagine many budget constraint people might look for this route (be careful what you buy), although prices are still too high even there. I can clearly the see controlled selling there not to dump prices by miners (selling several cards at once with hundreds of reviews in the last months selling their inventory) below the usual 33-50% of msrp on 2 year old gpu's. Also, I ethically oppose buying from miners.  
It's best to wait for next gen and then buy 4000/7000 series if you go this second hand route, to not buy a mined dud and probably have better 2nd hand prices as well.  


TL;DR; If you're budget restraint, wait 1-2 years. If you can't and want to upgrade in 2 years, then buy the cheapest available thing that will serve you, because for 300, you can't buy a ps5 either or limit the games you're playing (instead of AAA games, maybe games like Rimworld or Hearts of Iron IV or even Company of Heroes 3, being released today, doesn't require that much). 

The good times will come back, it just will take time as always, best to take a break from checking prices each day/week and be happy what you got right now, would be what I'd do if I were in this situation."
"The problem with that is that wafer allocation is decided way in advance and GPU's are a depreciating asset.

Due to investor pressure, they also cannot (heavily) delay the next gen cards to keep current gen prices up. Because delaying tech advancements makes investors fear that the company will fall behind."
[deleted]
It is, I'm just saying this isn't really AMD's doing. Granted, margins are probably better on the 7900 XT(X) than the 6000 series right now.
They just called the category "Gaming" so I assume just GPU's. CPU's and Motherboards are in "Client"
If it has the same kind of delay as Alchemist did then it will be ready just in time to lag behind 5000 series in 2025.
2024 lol
"> But that's the thing, why would I buy a a $300 gpu on top of the rest of the hw to play at 1080, when I can spend $500 on a console that will have at least similar specs but way better better performances at 4k because games are actually optimized for consoles.

Consoles get you in with an attractive entry price, but ongoing costs are higher for the console gamer. The main advantage I see with consoles is that the level of knowledge required to get a good experience is lower, but for technically savvy people I have no hesitation in recommending the PC gaming ecosystem.

> And let's be real, all ports that have released so far this year are opmized like shit. And I am saying that as a 3080 owner, I still love my pc for MP and other comeptive games (that are actually properly optimized),

Console ports often suck, at least at launch, but you know what sucks more? PC game ports to console. They usually don't even exist. When they do exist, they are often comically terrible -- Starcraft for the N64 is a notable example."
"cause with a computer you can not buy your games ... \*hahem\* 

A 1000$ computer is less expensive on the long run, and can run most games in decent graphics. 

Had the PS4 and well, games are more expensive, less sells, and if it is still a thing you need to pay an extra charge to acces online games like 18$/month or so. 

Plus with a computer you can mod your games (dont starve together can do with UI modding) There is no comparaison beetween a PC even budget and a console. 

The only one that is worth something is the switch, cause it is big on lan multiplayer. It just offers a different service"
">But that's the thing, why would I buy a a $300 gpu on top of the rest of the hw to play at 1080, when I can spend $500 on a console that will have at least similar specs but way better better performances at 4k because games are actually optimized for consoles.

Gaming PCs **were never** cheaper than consoles. Even if you could get gaming-grade CPU and GPUs for $100 each, that leaves you $300 to buy a case, power supply, SSD, RAM, Windows, etc. At least with new gear, you can kind of min-max with a used Optiplex and ebay GPU.

I don't know why this is suddenly a surprise. It's always been like this, the overpriced 4080s hardly changes this."
"I don't disagree with you, but it's worth noting that very few games run at native 4K on console. It's all upscaling. Pretty good upscaling, yes, but still upscaling. A lot of them are rendering at 1080p or 1440p internally.

At the high end (native 4k, heavy RT, high refresh rate), PC will remain king for the foreseeable future because it's impractical to put that kind of power in the cost and form factor envelope of a console, even as a console may present far better value."
Havenâ€™t used it yet. It took over a month to get here and just arrived yesterday.
They will also probably say the 4070Ti is the fastest selling current gen card too.
Weren't they sued for combining the crypto sales with regular gaming, as they are VERY different customers and thefore pretty misleading?
"> Decent compared to AMD sales maybe

decent as in revenue will go up not down. We won't be seeing an all time high.

> after it was ""mysteriously"" pumped in the last month leading to this report

i thought most of the pumping was caused by the AI craze, but who knows what is going on~"
There's something to be said for buying a $400 box that you can plug into your tv and flawlessly run most every game that comes out for the next without lifting a finger.  The PCMR crowd seems to get really caught up around graphics and framerates, but what they don't realize is things have been 'good enough' for most console gamers for a *while* now.  The two groups seem to prioritize different things and talk past each other.
At what point do we finally stop calling it next-gen? It's just current gen now.
Yeah, a whole system with killer exclusives for the price of only the GPU, and a shitty one at that
A next gen console that is equivalent to 2 generations old midrage GPU.
"Lol next gen? It came out in 2020 and has a CPU comparable to a 2700X, which came out 5 years ago.

Next gen..."
30+ million PS5s have been sold, they're buying both
"The fact that 4080s are selling out, to get restocked, means that they are selling fairly well. And not â€œrotting on shelvesâ€ as some hyperbolic commenters would claim. 

Itâ€™s also important to remember that before this whole crypto fiasco, this was normally how the GPU market was. Cards sold, got restocked, and were readily available for purchase at MSRP."
I donâ€™t know if Intelâ€™s been around long enough in the GPU space to have much software support. I remember reading that their Stable Diffusion performance is currently worse than AMDâ€™s
1000 GPU's take a lot less warehouse space, use less components, require less packaging, has smaller personnel costs etc.
Let me rephrase this to make it way more clear: it's better to work 30 days being paid $50 each day than to work 50 days being paid $30 each day, isn't it? You still earn the same amount of money, but you invest less time and effort.
"You said AMD should sell 6600's at 100-150 because they apparently have the margins to do it. I was pointing out that they do not, you can't cut your prices in half when you're only making 16% margin.

If you know how they can profit selling the card for 100, you should ring them."
I'm not arguing any of that.  I'm saying that "budget GPU = $250".  Show me a $100 GPU and I'll eat my hat.  Conflating reality with desires is worse than saying nothing at all, which is why I made a comment.
I don't think anyone expected the performance of the 4090 tbf. Even still it's a good upgrade over a 6900xt and was able to stay at the same price despite inflation and other factors in part due to the MCM design. It's a very good card at it's price point.
"That argument is entirely dependent on how you categorize a ""flagship"". If you consider it to be a performance ""tier"" and the 4090 is the benchmark, then sure the 7900XTX is lackluster. 

If you consider it to be the highest performing part in a companies lineup, (which imho is how almost everyone does, performance tiers are dictated by performance)

The 7900xtx is in the top 3 performing consumer GPUs in the world, regardless of what way you look at it. And it's priced to compete with the 4080, not the 4090."
"The 4090 is priced to sell to as many people as possible in the halo bracket. It has no competition, but Nvidia wants to get as many out there as it can.

The 4080 and 4070 ti are close enough to the 3080/90 variants still floating around that people will be in the market for any of them."
[deleted]
"Lol Comcast was my only option, $120 a month for 20mb/s.


Awful internet. Dropped all the time no matter where I lived lmao"
The first one lmao
"Here in Christchurch, New Zealand we went from $400k to 8-900k in the same span for 2 bedroom shitbox apartment like condo things

We're allergic to buildings that go too far above the ground here in quakevile which doesn't help"
That's 1 year of wages as a down payment. I can't see why you aren't able to afford it
"Iâ€™m not saying pc is better, just saying itâ€™s a misconception to say that pcs cost more. Even if you discount the fact that most people have a pc and thus are likely a few upgrades away from a console or better than console experience a pc can be built with new pieces for 500-600 bucks that will perform just as well as a ps 5. 


The 3060 will compete with the ps5 and can be had for 359: https://www.newegg.com/p/1FT-00EY-00009?Item=9SIBF3CJ4W3816&Source=socialshare&cm_mmc=snc-social-_-sr-_-9SIBF3CJ4W3816-_-02172023

They go on special occasionally as well. That leaves 150-250 for the rest of the machine. 

So 89 for a motherboard:

Check this out on @Newegg: https://www.newegg.com/asrock-b550m-phantom-gaming-4/p/N82E16813157966?Item=N82E16813157966&Source=socialshare&cm_mmc=snc-social-_-sr-_-13-157-966-_-02172023

Iâ€™ve used a version of this in the past, itâ€™s a good reliable if not the most feature rich board.

Ryzen 5500: https://www.newegg.com/amd-ryzen-5-5500-ryzen-5-5000-series/p/N82E16819113737?Item=N82E16819113737&Source=socialshare&cm_mmc=snc-social-_-sr-_-19-113-737-_-02172023

94 bucks.

Ram: https://www.newegg.com/corsair-8gb-288-pin-ddr4-sdram/p/N82E16820236674?Item=9SIB8PYJEA6043&Source=socialshare&cm_mmc=snc-social-_-sr-_-9SIB8PYJEA6043-_-02172023

X2, 60 bucks, ddr4 3600 16 gigs.

And thatâ€™s off Newegg. Iâ€™m sure if I searched for better deals I could put together the whole system case and all for under the budget. It you have a computer already then this will with the parts you already have match the ps5, and if it doesnâ€™t the 3060ti is only a few bucks more:

https://www.newegg.com/p/1FT-00EY-00011?Item=9SIBF3CJ558974&Source=socialshare&cm_mmc=snc-social-_-sr-_-9SIBF3CJ558974-_-02172023

Building a machine with used parts gets even better here is a refurbished card for 295:

https://www.newegg.com/gigabyte-geforce-rtx-3060-ti-gv-n306teagle-oc-8gd/p/1FT-000A-00591?Item=9SIAYB7JRH3685&Source=socialshare&cm_mmc=snc-social-_-sr-_-9SIAYB7JRH3685-_-02172023

If all I put in the system thatâ€™s used is a refurbished graphics card that brings my space up enough to get an ssd or a case and power supply (not a great case or power supply mind you but still..) we arenâ€™t talking thousands of dollars to meet or beat the performance of the ps5. 

Again Iâ€™m not saying pcs are better or worse, just killing the myth that you need several grand to beat a console."
May just be a UK market thing
Chia is mined with hard drive space, not GPUs though
Chia's also kind of dead. At least its peak has passed. In my country people have been selling used HDDs for way below retail price for close to a year.
"GPUs aren't all that general purpose. They are only efficient for performing a fairly small subset of calculations. If you just construct some random mathematical function to calculate, odds are good that GPUs aren't very good at solving it.

This means that a future crypto summer involving GPUs very likely requires a coin designer intentionally targeting GPUs. I don't see a strong argument for why they would want to do that, so I'm bearish on GPUs being involved in the next wave."
"I have an irrational all or nothing mentality of playing on either 720p30 for non eye candy games while I am willing to subscribe to 4080 gfn ray tracing eye candy so I can go either way. 

I'm fine with my laptop and hope it'll last me forever, but in case it doesn't pcpartpicker is showing me a new entry level build around $500-700. I'll pay the premium (kind of like pc insurance) for new parts covered by warranty items since I have bad luck getting ripped off buying stuff in general due to my ignorance.

I know ownership is a big deal and companies moving towards subscription revenue for everything can easily turn bad for consumers but the $10 cloud gaming services seem to be the cheapest option for people like me with a regular laptop. It completely depends on low latency internet so not everyone can try it though."
"> I'm just going to spend an extra $100-$200 on an Nvidia card that I know will work well.

Same here. I hate it, but it's not worth the grief. 10 years with 9800GT to 560ti to 970 had way fewer crashes than the first year with a 5700xt. It has gotten a lot better though. Or less bad I guess. Just hoping that I can get a good deal on a 4070 in about a year or AMD picks up its game or the 7700xt is stable. I'm not paying more for a card than a PS5 or XboxX."
Been rocking AMD since basically the Radeon 9700. Almost continuously. I literally have not had one driver problem that was on the mainline code.
Same here. AMD is the Dodge of electronics - best horsepower per dollar on paper, but you canâ€™t enjoy it because itâ€™s been built like shit
"yap, this is why I picked Nvidia when AMD was faster years ago.  
Reliability >>> performance  

I made the mistake to own AMD only once."
I was more thinking about Nvidia because of their high margin pro cards, but I think you've answered my question.
"The person you are replying to is talking about the absolute stranglehold that Nvidia has on workstation/datacenter GPUs. Which typically have a higher margin than consumer products.

""Gaming"" implies that it's not actually including those business-focused cards."
"It's the entire department responsible for gaming GPUs. So it means, AMD's GPU sales were able to finance the entire division and its investments during the quarter.

It doesn't really say anything about the actual margins they get for each GPU, but iirc they typically aim for something like 40%+ of MSRP as profit. Since AMD has higher fixed cost in relation to revenue, it's expected that their gaming division is much less profitable than NVIdia's."
"tomorrow, tomorrow... it's only a year away ;)

... until it's delayed repeatedly."
"I am sorry dude, but I feel you are stuck in n the past. Denuvo is doing numbers on piracy with only one person  able to to crack it, and even then you're not getting patches that can actually make the game run better.

And $18 the monthly cost for MP is also a thing of the past, don't you have access to like game pass with it? And AFAIK, free only games don't require this sub anymore.

Also, if today I wanted to buy a system that run most of my games as good as a console, $500 is the bare minimum I would need to spend."
It can be true and the selling numbers can still be bad.
Which is super funny too. "Really, the 800 dollar card sold more than the 1200 and 1600 dollar ones? Wow, how shocking."
they were sued for not explaining that was reason the  cards sold so well. Even if they are different customers they are buying the same thing "gaming cards"
My issue is not the graphics and framerates, it's the controller. I've had a PS3 for close to a decade at this point and could never adapt to a gamepad paradigm, it's like a mental block. Until keyboard/mouse compatibility becomes universal, I'm not buying a console for any kind of serious gaming.
And 60fps is either standard or a settings option this gen. 30fps used to be the primary gripe with console gaming. Full disclosure I have a PC but will definitely look at picking up a PS5 at some point.
">The PCMR crowd

For a lot of us consoles just aren't an option due to the type of games we play. I haven't played a game that isn't PC exclusive in many years. All of my 20+ friends that I regularly play with all play on PC. Even if consoles were free it wouldn't be an option for many of us."
Until every new game we get is next-gen only
Killer exclusives against PC is a bit odd, PC has way, way more (also good) exclusives than any console.
[removed]
1 gen old GPU, it's RDNA 2
Most people don't own one yet and still has like 6 years before replacement. Plus it has cutting edge storage and controller haptics, now a promising VR solution
3700x. PS5 CPU is Zen 2
"Too bad for the downvotes, but you are actually fairly close. The [4700S Desktop Kit](https://www.tomshardware.com/news/ps5-cpu-inside-amd-says-eighty-4700s-desktops-coming#xenforo-comments-3712658) is purportedly the PS5 SoC itself, as the article shows in the die shots. The L3 cache and frequency are a close match too.

And here's 4700S's multi- and single- thread [comparison](https://www.cpubenchmark.net/compare/3481vs3238vs4309/AMD-Ryzen-5-3600-vs-AMD-Ryzen-7-2700X-vs-AMD-4700S) with the 3600 and 2700X. As we can see all three CPUs are very close in performance. Digital Foundry often cites 3600 as the closest mainstream CPU to PS5, so I've included it here for good measure. The 2700X beats 4700S slightly in ST in this synthetic measurement, but loses slightly in MT. On the whole 2700X appears to be very close to PS5, even closer than 3600. Ever so slightly behind the PS5 CPU imho, considering the MT performance.

For a desktop to run games primarily designed for PS5 and then ported over to PC, I'd pick a slightly more powerful CPU than an exact equivalent to allow for losing the PS5 optimization, and for a performance penalty of porting. Now what *that* CPU should be is another exercise, but something like a 3600X is the minimum I would consider. In the current generation the Ryzen [5500 (link includes 3600x)](https://www.cpubenchmark.net/compare/3481vs3238vs4309vs3494vs4807/AMD-Ryzen-5-3600-vs-AMD-Ryzen-7-2700X-vs-AMD-4700S-vs-AMD-Ryzen-5-3600X-vs-AMD-Ryzen-5-5500) is at a good price and should handle any PS5 ports comfortably. The more expensive 5600/X and 12400/F would be significantly faster still."
They would be restocked so quickly after "selling out" that they don't actually appear to be selling out.
I see. Thanks for clarifying.
"Youre putting words in my mouth. I think $220ish is a fair price for a 6600. 

My big issue is that the 6400 and 6500 XT SUCK yet are still $150-200. 

The 6400 should be like $100, and a card like the 6500 XT should be like $130. And then there should be another card between a 6500 XT and a 6600 (say, 3050 level) that costs around $170ish. 

Also, a 16% margin is A LOT as I just demonstrated, the article said that 5% is a low margin, 10% is a ""healthy"" margin, and 20 is ""high.""

They're making good money and can afford to lower prices at 16%. 

Also, I'll tell you why they dont offer a $100 product any more, it's because there's no incentive. 

My first GPU ever was a HD 3650. I'd classify that as roughly a modern equivalent of like....a modern 6500 XT give or take, given they had the 3450 which was even lower tier.

The 3650 was a $75 card. I spent $80 on it. Plugging in the MSRP into an inflation calculator, I get $104. 

Yeah, they can probably make $100 cards, they just choose not to. Because the market is broken.

Stop licking boot."
"In 2016 the GTX 1050 was $110. Close enough. Heck the 1030 had an MSRP of $80. You can still find them for $90.

https://shop.asus.com/us/90yv0at1-mvaa00-gt1030-2g-csm.html

You can get its successor, the 1630, for $125.

https://www.newegg.com/msi-geforce-gtx-1630-gtx-1630-aero-itx-4g-oc/p/N82E16814137757

You can get a 6400 for $130-140.

https://www.newegg.com/gigabyte-gv-r64eagle-4gd/p/N82E16814932514

You can get a 6500 XT or 1650 for $170

https://www.bhphotovideo.com/c/product/1687481-REG/xfx_rx_65xt4dbdq_radeon_rx_6500_xt.html

https://pcpartpicker.com/product/HMK2FT/gigabyte-geforce-gtx-1650-g6-4-gb-d6-oc-rev-20-video-card-gv-n1656oc-4gd

That, btw, is the minimum I'd recommend getting if you're serious about gaming.

5600 XT is $200

https://www.newegg.com/asrock-radeon-rx-5600-xt-rx5600xt-cld-6go/p/N82E16814930030

6600 is $235

https://www.newegg.com/asrock-radeon-rx-6600-rx6600-cld-8g/p/N82E16814930066

Arc A750 is $250

https://www.newegg.com/intel-arc-a750-21p02j00ba/p/N82E16814883002

6650 XT is $265

https://www.newegg.com/msi-rx-6650-xt-mech-2x-8g-oc/p/N82E16814137737

As I said, the minimum I'd recommend getting is the 6500 XT or 1650 for $170, and I'd personally recommend a 6600 or 6650 XT which get 3060 type performance for significantly less. 

It's only nvidia who has such insane GPU prices. The 6650 XT and Arc A750 both run circles around the 3050 for the most part. 

The thing is, this sub is horribly out of touch with what it considers budget. This is because it's a bunch of upper class white collar professionals who make $75k+ a year doing programming and web development, and graphic design, and often have tons of money to burn. 

So they buy these grossly overpriced GPUs, play at 4k, and think not playing at max settings with ray tracing is unacceptable.

And then they look at us normal 1080p gamers still getting by on mid 5 figure household income and buying $250 GPUs LIKE WE ALWAYS DID as if we are the poors. 

And looking at your attitude, you're just about as bad as the stereotype i pointed out. 

And btw, Im not saying that this level of performance is acceptable for the price either. Quite frankly given how the 6600, Arc A750, and 6650 XT perform for around $250, the prices on the lower cards are WAAAY out of whack.

If we had the same price/performance down the stack, the 1650/6500 XT would be closer to $125-150, the 6400 should be $100, the 1630 would be $75 (seriously, it's on par with the 1050 I mentioned that started at $110), and the 1030 should be like a $40 card. 

The fact that so little progress has happened in 6 years is concerning, and I fear for the TRUE budget segment (which I'd define as around $150 for ""serious"" gamers, those are your traditional ""50"" series buyers). 

But seriously, $250 is your normal middle of the road ""60"" buyers before nvidia decided ""lol, let's charge $350-400"". 

And at this point, given where the market is right now, yeah, this is an NVIDIA problem. Both Intel and AMD have solid offerings for $250. a 6600 will run any game out there at 60 FPS at 1080p on AT LEAST medium, and most on high, if not ultra. It's really the crazy new 2023 releases that make me say ""medium"" given hardware requirements are inflating at a startling rate, but yeah. 

This isn't abnormal for typical midrange buyers though. I traditionally gamed at 900p on like medium-high with your typical $250 card and held onto it for about 5 years on average (4-6 is the typical GPU lifespan). I only went up to 1080p with the 1060. 

And now for some reason rich guys on this sub think that that's minimum and budget and for the poors. No, you guys are just out of touch. 1080p is the most popular monitor size, 720p is probably more popular than 4k in practice, most people game at 60 hz. Etc. 

As I said it's you ""enthusiasts"" on this sub who dont understand how most people seem to live."
"The 6900 XT was priced ridiculously compared to the 6800 XT which was almost as fast. The only reason it's not remembered that way is the mining apocalypse.

The 4080 is faster and cheaper than the 3090 at launch, but you can't claim the price of cards hasn't gone up comparatively when the 3080 also existed."
"> Not all leftist want socialism or Marxism in particular. That's NOT the point anyhow. Also, I doubt any of them are smart enough to think that far into it anyhow.

As someone whose two major interests are politics and PC gaming, I can guarantee you that you are wrong.

Also, leftists are smart they're just stuck in theory land and dont put a lot of time into empirically coming up with solutions that actually work. 

>That's fine with me. If they're corrupt, it should be handled.

i think that given the runaway prices of the past generations, something is seriously wrong with the market."
Whatâ€™s up with these prices in the US? Here in Albania, I can get a 300mbps connection for ~$20.
Thatâ€™s crazy, where is it? In CA I have 200mbps for $50
Elon Musk entered the chat
"My dad gets like 5 mb/s for at least $60 from Frontier. It's the fastest available to him.

My friend in the same county can't even get internet at his new house.

Comcast Xfinity was good when I had it."
[deleted]
"Ah yes. All they have to do is not buy anything for an entire year.

With strict budgeting, most people can stretch to save 10% of their budget. Which means a *minimum* of 10 years of net income, just to afford the down payment."
"1. You linked a MoBo without WiFi, so throw in another $30 for it.
2. No OS either, so unless you're planning to play on Linux. 
3. The PS5 comes with a controller, the PC you linked has no peripherals. So add a keyboard and mouse to the storage.
4. Aaand you also didn't mention the storage. The PS5 comes with a 1TB PCIe Gen 4 M.2 SSD.

[This is what I made on PCPartPicker, and I seriously doubt this could match the PS5 in terms of performance](https://pcpartpicker.com/list/rr7Lmr). It costs $760 without the OS. And the PSU is not that good either.

I'm not even counting on the fact that a lot more households have a TV and a place to watch it from when compared to a desk, a chair, a table, and a monitor. 

Again, picking used parts and going ""you can build a PC for about the same price as a console and match the performance"" isn't fair because a used console would likely cost less as well. 

I agree you wouldn't have to spend multiple thousands of dollars on a PC to get the performance of a PS5 but you'll still have to spend significantly more, around twice as much at least. Not to mention that PC will likely be less portable than a PS5 unless you go mini-ITX which will again drive up the price."
">itâ€™s a misconception to say that pcs cost more.

Technically PCs do cost more. No PC part is sold at a loss while consoles are. Console profit comes from inflated game prices and online pay (which are a good reason to get a PC instead but that's not what we are talking about here). Maybe you can cheap out on some things like an HDD rather than a TB M2, get rid of wifi, weaker PSU (most of the times this translates into not reliable for more than a year if stable at all under high consumption) and still have a more powerful machine. But logistically speaking, consoles are in fact cheaper for what they bring. 

Also when you say ""You can get a much better PC if you use used parts"", do you account for used consoles prices as well? Or are you comparing apples with oranges?"
"Iâ€™m late, but on top of what has already been mentioned, youâ€™re missing some other things here. 

When using Quality/Resolution mode (whatever theyâ€™re called), Series S targets up to 1440p60 while Series X and PS5 alike aim for native 2160p30 or 2160p60 with checkerboard rendering. On current and future gen games. The Series S features 10GB of VRAM, while Series X and PS5 feature 16G. They ALL additionally support raytracing on aforementioned resolutions at aforementioned framerates. All on a small form factor and while running on a measly 500 watts during gaming sessions. 

If youâ€™re into VR, having a PS5 also makes it possible to get PSVR2, which provides some of the most immersive VR gaming in the consumer market at some of the most affordable prices. It even competes with the upcoming Quest 3, which is insane. 


You donâ€™t get this stuff on equivalently priced PCs, even when theyâ€™re homebuilt on used components. Consoles have won when it comes to bang-for-your-buck gaming, and it will likely stay that way. The fact that the entire rig is assembled in factories lowers costs. Game/engine devs and other software engineers keep your your very specific specs in mind when optimizing, which leaves you with performances that PCs with similar hardware are too software-limited to match.

PCs arenâ€™t inferior, though. Far from it. They are still THE powerhouse of multitasking, productivity, competitive gaming, and bleeding edge tech. The fact that you get to build it yourself is a wonderful feature, granted that you enjoy it which plenty donâ€™t. Its customisability allows users to personalise it in both appearance and performance, which is something that console users miss out on entirely. May be other advantages that didnâ€™t come to mind. But if youâ€™re strictly looking for a machine to game on, consoles are definitely the best value."
">Chia's also kind of dead. 

Hopefully its all the way dead soon"
[deleted]
"> Been rocking AMD since basically the Radeon 9700. Almost continuously. I literally have not had one driver problem that was on the mainline code.


haha, I swear to god, if you are lost in the woods all you have to do is yell ""I have had occasional driver issues with my AMD card"" and someone will promptly be along to tell you that the 5700XT, Vega, and Fury X series never existed.  We can throw GCN 1.0 into that bucket too with the timespan you've given.

[GN](https://www.youtube.com/watch?v=JluNkjdpxFo&t=259s) and [der8auer](https://www.youtube.com/watch?v=x03FyPQ3a3E) are right, you guys need to give it a chill, the reddit marketing squad is ridiculous.

like GN is literally [specifically referencing the *massive AMD driver issues that existed during the time period AMD marketing was flinging shit at their marketing event about how worthless DLSS was.*](https://youtu.be/JluNkjdpxFo?t=310)  Guess that didn't work out for them in the long term, but like, *even Steve thinks you're full of shit*, his exact words were ""the worst GPU drivers known to man"".

And even if you didn't own those cards, *surely you've at least heard of* some of these cards that had widely-known, severe, and widely-encountered driver issues?"
"Were they? They just said AMD and Nvidia love high margins and that they're looking forward to Battlemerge.

Edit: Oh the other person sorry"
im choosing to believe that desktop battlemage is mostly cancelled anyway. They might release a single gpu in low volume if it makes sense financially.
"Honestly, it all turns down to the question of: Do you want a gaming device with a low entry cost, pricier games s(ince Sony/Nintendo/MS barely does sales  outside of gamepass) or a pricier gaming device but frequent sales and freedom in operating the OS? 

I'd personally choose the latter because I can't stand being unable to mod shit but I do see the appeal of the former. You don't have to troubleshoot much nor worry if your game can run properly, it's just plug and play.

Also fuck Denuvo, piracy is pretty much limited these days."
"Well i got the last spider man for my nephew. So some games i would never spend a cent for are still there :)

Otherwise good for ps4 users, it was outrageous.

That being said i believe a ps5 is aroud 500$ too. Without possible upgrade and little memory no ?

Edit : i do live in the past concerning ps x and piracy"
Then there's me who has like 10 different controllers I use on PC. Mind you, for FPS games kbm is still the way to go, at least until gyro aim adoption takes off
It might be that you're suffering with Sony's controllers. Ps4 was a vast improvement to the old dual shock abominations and the ps5 controller is a pretty big improvement over the ps4. HD Rumble aside, it's nearly at comfortable as Xbox controllers now.
What games do you play?
[deleted]
The last PS2 game was made after the PS4 came out.  Was the PS3 not ever a current generation console?
[deleted]
Real world test with game like Death Stranding proved that ps5/xsx are on par with a 3060
[deleted]
"Eh...kinda.

The PS5's GPU is best thought of RDNA+ as it's kind of a hybrid of the two. It lacks hardware VRS and it doesn't support mesh shading in the way RDNA2 does."
itâ€™s closer to a 3600
PS5 and Xbox Series CPUs are based on the mobile variants, so they only have 4MB of L3 cache per 4-core cluster. The desktop Zen2 processors have 16MB per cluster. It's a 15% or so hit to performance based on the benchmarks I've seen.
"The 3700X is 4.4GHz. The PS5 is 3.5GHz.

That's 79.5% the clock speed of a 3700X. So yeah, it performs similar to a 2700X."
"The flat costs in making a card have increased, they're not making high margins on 6400/6500 XT. And flat costs are universal to any GPU, so the cheaper the product the less money can be put into the die, VRAM, etc. They'd need to be making about a 40% margin right now to get a 6400 from 140 to 100. I don't think they're making 2.5x their overall margin on their *cheapest* GPU.

Think about it, the price to performance of the 6500 XT makes little sense since the 6600 is a dramatically more powerful for not that much more money. And there's cards with twice the performance for the same price on the used market. Yet the price hasn't dropped. If AMD was artificially holding prices, retailers would refuse to carry them. Most likely the prices are what they are because they *can't* make them cheaper.

3650 was 75 for the 512MB version but the 1GB version was 95. That's the same gimping bullshit Nvidia does now so they can brag that the GPU starts cheaper (1060 6GB vs 3GB). But you're pretty much *never* better off even in the medium term getting the gimped SKU. Losing *half* the VRAM for a third the cost of a game makes no sense.

95 dollars in 2008 is 132 now. There's $140 6400."
Seek help.
[deleted]
"In many parts of the rural US there is no competition for many firms . The single provider has no incentive to slash prices or increase speed 

They lobby politicians to prevent competition"
"Prices throughout the US are all over the place. I'm in central Virginia and get 1gbps for $80.

When in Georgia I was getting pretty much the same as the OC, both areas Comcast."
"I think part of the problem is that the US internet infrastructure is kind of suffering from being an early adopter. Most of the US got widespread internet very early, which meant that internet infrastructure was built on shitty old corroding copper lines (because that was what was available and it was good enough for dial-up.) Other countries that got it a few years later didn't have the preexisting shitty infrastructure to build off of, so they built it right.

And of course, regulatory capture has led to none of that old shitty stuff being upgraded. Australia has the same problem."
Price fixing and faux (but actually real) monopolies. Essentially phone and internet services have been playing palsies for so long that a vast number of them are actually the same company and the ones that arenâ€™t are in bed with the others to make sure prices stay artificially high. I could go into better detail but thatâ€™s the tl;dr.
we have cities as big as your country
They need incentives to keep people in Albania.. jk ðŸ˜œ
Yea where i moved to, I get gigabit for $75 not so it's much much better lol. But yea some parts of the US you have exactly one option for internet and they screw you over as much as possible
"Haha, we are comparing a country smaller than most US states. 

US is more complicated than that, I have 1gig fiber at $60/ month and access to 5 gig at $150/month. The US has more diverse housing development and some rural areas, like in the villages of Albania, infrastructure is non existent. No for-profit-company would build fiber at a rate of 5 households per mile or less."
I get 5,000 Mbps for $10 in the US. I work for the company though. Normally it's $180.
"Brazil here, 300mbps, 20 usd
(but a ps5 is 1000 usd lol)"
The not so great state of Pennsylvania
Also CA and 250mbps for $96, plus I have a data cap. Fuck cox and their monopoly.
aham , 1000 mbps for 11$ - Bulgaria
">films

Don't trust Hollywood's depiction of anything. The list of things they get wrong in movies and TV shows, including the ones ""based on a true story"", is always long.

Places and people vary, and it's always good to research any place you might want to move to. Just, the TV often isn't research. ;)"
Or, you know, save a third of their income for three years. Don't act like that's impossible, since a ton of people do it. I did it to save for retirement, skipped buying a house and retired in my 30s
Oh boo hoo 15 bucks for windows, again I point out no one starts with nothing. Almost everyone has a pc in their house at this point to upgrade, and it likely has peripherals. Yeah 30 bucks for Wi-Fi is fair though frankly most gamers donâ€™t use Wi-Fi (my brother wonâ€™t even use it on his laptop for gaming some bs about latencyâ€¦sigh). An ssd is going to cost 50 bucks. Mean while the ps5 is literally only good for content consumption. You canâ€™t create anything on it not even a word document, it can barely browse the internet and so on. You give up portability for versatility. Very frankly the 3060ti will slaughter the ps5, particularly if you do put Linux on it. Plus I literally searched for one item and hit sort by lowest price then picked things I thought would be reasonably capable. I didnâ€™t cross shop, I didnâ€™t bang for the buck (you can still get ryzen 3600s new for less than the 5500 I linked for instance). Shopping smarter will bring the price down a good bit. It can be done.
Dude a little later in this thread I build a complete system with new parts nothing bargain basement no â€œcorners cutâ€ and it only costs about 100 more than a console and itâ€™ll compete with, or beat it.
"Generally I have found that I update my video drivers not very often. Maybe once a year max. I often lag behind from cutting edge releases. 

This generally works well for all vendors, for all products. Nothing is perfect of course, and yeah there's teething issues even on the stable codes. But it's often worked mostly out by then."
Not at all. I'm just saying that I've never experienced them. Doesn't mean that the drivers are perfect. I've generally had more issues with NVIDIA, but not that many comparatively.
"Maybe it's because I first picked up a controller when I was 18 (while using computers since very early childhood), but I legit cannot use it for anything other than navigating menus. It's like my brain fries itself every time I try. I have a purchased copy of GTA VCS in the PS2 Classics collection for the PS3, and yet I've never played it - only played it on PCSX2, and only because the emulator allows you to just map gamepad buttons to KB/M. It's janky - far jankier than native controls - but it's playable.

Seriously, if Microsoft just makes KB/M support mandatory for all their games to receive certification, with the fully featured Edge browser it already has, the Xbox might legit replace a PC for a lot of people."
CK3, Stellaris, HoI4, Civ5, OpenTTD, Factorio, Rimworld, For the King, Gloomhaven, Project Zomboid, Wargame Red Dragon, Starcraft 2, Arma 3, World of Warcraft, 7 Days to Die, Raft and Valheim are the ones I have installed right now.
"Itâ€™s not perfectly capable, and sports games already left last gen behind on old engines because they canâ€™t handle them.


The CPUs were absolute fucking dogshit at launch and itâ€™s massively limited the mechanics of games for years. Graphics donâ€™t matter. Those scale. CPU does. That doesnâ€™t."
Yeah that list of PS5 exclusives sure is looking more tempting than the top 100 games currently being played on steam. All... [12](https://en.wikipedia.org/wiki/Category:PlayStation_5-only_games) of them?
[removed]
[removed]
...ok? It's a down clocked 3700x. That doesn't magically change the architecture to be a Zen 1 2700x. Even a down clocked 3700x will run circles around a 2700x
"Ok buddy, go over to r/lowendgaming and ask any of them if a 6600, 6650 XT, or Arc A750 are low end.

They'll just laugh at you and be like wtf are you talking about while asking what GPU they should upgrade to with their LGA 775 systems."
">Most people who identify with a particular party don't really understand, in depth, what the underlying theories or ideologies are. And again, that's not the point. These people may not identify specifically. They just hear dumb parroted shit and regurgitate further without understanding the ideology behind what's happening. That's what I'm getting at. On this sub, those utterance are often Left like, so that's how Im calling it.

""Party"". Hint: democrats aren't ""leftist"". 

Either way this isn't a political forum, so...

>Something is pretty nebulous. Inflation is probably worse than people think. Fab space to make the cards is expensive and limited. Costs across the board have gone up. Those things can easily explain the price issues.

Most inflation is actually driven by corporate greed. 

And given nvidia's price increases in recent years, I definitely think something is SERIOUSLY wrong with what nvidia is doing to the market, this isn't normal. LTT actually had tier list of the past 20 years of nvidia GPUs and I did my own take on them, and uh...yeah, what nvidia has been doing the past 3 generations is ahistorical and bonkers. Like, it's like the very concept of price/performance has broken down. 

And given this sub has an *ahem* right wing bias of being market fundamentalists and bootlicking for multi billion dollar corporations because it's full of a bunch of upper class yuppies looking to justify their expensive purchases, which they can afford, but many people can't, and they're in their own little white collar world seemingly ignorant of how the ""average"" person actually lives, i really think that this sub is sometimes off their rocker with justifying this garbage.

I mean, you're right, this stuff actually is political, and political outlook will define how you see it. And yeah sorry to break your right wing circlejerk by suggesting nvidia is being greedy and swinging their 88% market share around trying to squeeze every cent out of consumers to see how much they can get away with. 

It's econ 101. Supply and demand, Demand is down. And rather than cut prices, they're cutting supply, to try to make GPUs more scarce, to keep the prices higher. It's scummy. Nvidia is screwing you over, dude. 

PS, the current prices existed BEFORE ""inflation"". Seriously. Nvidia started pulling this crap in 2018 with the RTX 2000 series. Pascal was the last truly affordable GPU series worth buying from these guys. Since then it's just been pure greed."
We also have to consider that Albania is the size of one of the US smaller states like Vermont. And so connecting rural parts of the countries means that companies would have to spend 10s of thousands of dollars to hook up just one property. But yes, it is bad when a company refuses to upgrade even densely packed neighborhoods making the only choice a low speed high cost package.
att fiber in GA is pretty dope
Ya I had spectrum and paid 100 for garbage.  Just got greenlight fiber. 70$ for 1g up and down. About to switch to 2g for 100$
"Nope, it's a matter or lack of regulation and serious lack of competition.

France had even a worse case, with one of the best copper phone line system in the world... and nothing else. No real coaxial, and no incentive and lots of red tape against fiber. So the early days were good, the last decade not so much.

Yet, the prices were very much lower than the US. Like 20 years ago, 30â‚¬ a month included: no internet data quotas or caps, some TV, advanced phone, the modem and router, with often no engagement or high exit fees. And minor amenities like static IP, as many email and webpages hosting as you wanted, etc.

And nothing like the service and network quality horror stories we routinely hear about North America.

And that mostly stayed the same for 20 years, just with the internet speed climbing as we moved from ADSL to ADSL2 to VDSL and for the lucky ones to fiber. With some premium offers now around 45â‚¬ I think; and we had more budget offers as low as 20â‚¬ I think.

And that was with the national state owned phone company owning all the last mile network, almost all points of presence, and charging as much as possible everyone and everything."
The entire country is in a state of regulatory capture :(
Wish Google fiber would come to more cities.
"This. The United States is so vast. Comparing a small country in Europe to the whole of the US is apples and oranges. Youâ€™d have to take all of Europe and compare to get a true comparison. And, just like in Europe vs the US, speeds and coverage vary wildly. 

Lots of factors. Telecom companies getting money from the government for broadband expansion and just pocketing it and not doing the work. 

There are a half dozen major companies in the US with a presence in almost every state or region. Compare that to a small euro country where 3 or 4 operates in that smaller country. 

Itâ€™s all about scale and how spread out all of the US is.

Lastly it also is all the US companies in cahoots with each other to keep speeds low and slooowwwwllly turn the knob."
Thatâ€™s even worse than IL with Xfinity. I just downgraded to 200/10 for $62/month and that has the crappy data cap too. Iâ€™ve been dinged with extra charges some months by large game updates.
">Or, you know, save a third of their income for three years. Don't act like that's impossible, since a ton of people do it. I did it to save for retirement, skipped buying a house and retired in my 30s

Congratulations; let me play you a fanfare on the world's tiniest trumpet."
">I point out no one starts with nothing

Plenty of people I know didn't even have a laptop before they bought one, or had one gifted/given to them.

I started with nothing before getting my first laptop as a gift. As did my aunt before I gifted her a laptop. 

Define ""gamer"". Hardcore folks don't use WiFi, but the vast majority use WiFi over wired ethernet especially in countries where walls are made of brick and not wood so you can't run cables through walls as and when you please.

Everybody who buys a PS5 knows it's only good for a few things, and they still buy it because it's convenient. They don't have to research parts. They don't have to look up system requirements. The ""but you can't do your taxes on a console"" argument is stupid IMO because nobody buys a console for that. That's like saying ""well you can't strip wires with your hammer but you can strip wires and hammer nails with a plier"", it makes no sense at all. 

It cannot be done for the same price of a console. It \*can\* be done for a significant price increase on a console, but not the same price."
"Ok but did you read my comment?

""Maybe you can cheap out on some things like an HDD rather than a TB M2, get rid of wifi, weaker PSU... and still have a more powerful machine. **But logistically speaking, consoles are in fact cheaper for what they bring**.""

Just answer me this. Does that machine you mentioned have a 825GB NVMe with 5.5GBS raw throughput and 8-9GBs due to dedicated decompression unit with Oodle Kraken data compression protocol? Or does it bring a generic SATA SSD or even a HDD? Because if so, that's not my point."
"I always buy whatever is the best bang for the buck when it comes to electronics. So for GPUs this usually means AMD.

I work IT so I'm willing to work a little more if it means I saved money.

However, I did have an RX 5700 and it was one of the most unprofessional experiences I've had. Daily black screens ranging from playing games to watching youtube.

Took at least three months before the drivers got stable.

I recently upgraded to a 6900 XT. It too is still having issues with driver bugs."
You'd think kbm support would be a no brainer given they could peddle some first party accesories
Cool. I'm doing a home office this year, and I've considered building a pc to be able to play strategy and Sim games. Kind of hard to justify the cost when I consider don't game nearly as much as I used to when I last built a pc 20 years ago. We'll see.
You can play PS4 games on PS5
Where I live it cost between 450 and 550â‚¬
Assuming you even have to option for console equivalent settings on pc.
"In this instance, the console CPUs offer performance on par with Zen 1 because they have half the L3 cache you would typically see in a Zen 2 CCD.

AMD released a handful of Zen 2 desktop and mobile parts with the same configuration and they performed within a 5ish% margin of Zen 1 desktop parts when clock speeds were normalized.

I mean, it's still a massive leap over the Clawhammer-esque IPC you saw in the 8th gen systems' Jaguar cores but it is lower than what you would expect."
Just stop putting words in other people's mouths.  That shit is entirely obnoxious.  People close to you resent it.  Just stop it.
[deleted]
It's pure extortion. They'll lie about their prices too. I signed up for a promotional $55/month for 250mbps for the first 2 years, but my bill started coming in at the standard $96. I called and they just said too bad, they're not giving me the promo and there's nothing I can do about it because there are no competitors.
Not everyone lives in a big city or provides for a family.
"Ok lets actually do this then, now that I'm not just on my phone:  


[https://www.newegg.com/intel-1tb-670p-series/p/N82E16820167474](https://www.newegg.com/intel-1tb-670p-series/p/N82E16820167474) 1tb HDD 50 bucks  


[https://sp-siliconpower.com/products/silicon-power-zenith-gaming-ddr4-3200mhz-pc4-25600-16gb8gbx2-dual-pack-1-35v-desktop-unbuffered-dimm?variant=43178411393279](https://sp-siliconpower.com/products/silicon-power-zenith-gaming-ddr4-3200mhz-pc4-25600-16gb8gbx2-dual-pack-1-35v-desktop-unbuffered-dimm?variant=43178411393279)  


16gigs ram 38 bucks  


[https://www.gamestop.com/pc-gaming/pc-components/cases/products/atrix-metal-tempered-glass-mid-tower-computer-case-with-cooling-fan/11201245.html](https://www.gamestop.com/pc-gaming/pc-components/cases/products/atrix-metal-tempered-glass-mid-tower-computer-case-with-cooling-fan/11201245.html)  


case 29 bucks  


[https://www.amazon.com/dp/B0BW3SHFQP?ref\_=cm\_sw\_r\_apin\_dp\_BSMQSN62VSRPDRVA5BTQ](https://www.amazon.com/dp/B0BW3SHFQP?ref_=cm_sw_r_apin_dp_BSMQSN62VSRPDRVA5BTQ)  


Zotac 3060 300 bucks  


https://www.amazon.com/dp/B0BM8YKQCZ?ref\_=cm\_sw\_r\_apin\_dp\_XXWC9XEK20M3FJMKJYTD&th=1

motherboard and psu combo 145 bucks  


[https://www.newegg.com/amd-ryzen-5-5500-ryzen-5-5000-series/p/N82E16819113737?Item=N82E16819113737&Source=socialshare&cm\_mmc=snc-social-](https://www.newegg.com/amd-ryzen-5-5500-ryzen-5-5000-series/p/N82E16819113737?Item=N82E16819113737&Source=socialshare&cm_mmc=snc-social-)  


Ryzen 5500 100 bucks   


so thats 662 for the desktop itself and then figure 20 bucks for keyboard and mouse and maybe 35 for the controller. I'm over budget by 120 bucks ish including peripherals. I could cut out say 20 -40 bucks for a ryzen 4100, but I'm not really sure if it has the power required. All these are name brand parts save for the case. And while the silicon power deal for 16 gigs is currently out of stock its not a rare occurrence and you could sub it with an equal corsair unit if you like its just the first one I ran across. We aren't talking a huge markup here. And at the end of the day while the idea ""you can't do your taxes on a ps5"" is silly, that doesn't mean it doesn't have a kernel of truth to it. You can't write e-mail, or a letter, or do your taxes, or balance your budget, or browse the internet on a ps5, you'll almost certainly need a computer for your life that is in addition to the ps5, and that will cost money."
"The build includes a tb m2 ssd. The psu is a known brand and of good power output, however you are right I didnâ€™t budget 30 bucks for a Wi-Fi card. 

The ssd in the build is an intel m.2 ssd. Itâ€™s nvme based, itâ€™s not the absolute cutting edge, but itâ€™s also not spinning rust or sata based. Itâ€™s pciex4 gen3. 50 bucks for that is insanely good, and frankly the ssd in the ps5 is overkill."
Yeah I'm still on a 7 year old computer due the costs of building a new one being so crazy right now. Luckily most of the games I listed work just fine with an old computer like this.
So can a PS4, what's the argument here? If I wanted to play a PS4 game, I don't need a PS5 to do that.
Check 6650 XT. 6600 XT was effectively discontinued so all supplies were diverted to 6650 XT.
"No what's obnoxious is this ""well ackshully"" crap and just using nebulous claims of ""inflation"" to justify crap prices, especially when the vast majority of that gouging is from one company. 

Apparently the HD 5850, GTX 760, GTX 1060, and now the HD 6650 XT are all ""budget"" now in their respective times."
"> 100% of inflation is by the government injecting money into the economy.

This is pure ignorance. 

https://www.epi.org/blog/corporate-profits-have-contributed-disproportionately-to-inflation-how-should-policymakers-respond/

>Here is is folks. The delusions stated outright. 

As I said, this is political, you're right wing. 

>Video cards are a first world luxury good. Cry me a river about how poor people can't afford them to play their games for which they oldy have lots of free time while scavenging for resources in their own poverty. Save the bullshit. We're not talking about cheap used cars for commuting to work. We're talking bout the video card equivalent of high end sports cars. 

Ah yes, the poors shouldnt have access to GPUs. They should just work harder. Only true hard working upper class americans deserve those GPUs. 

You know how delusional you sound when i cut through the ideological bullcrap? Jesus christ this is why i crap on this sub's demographics so much. 

>I'm not a right winger. I've not expressed my specific political views. But you would think so because by your own admission, your ideology clouds your judgment. Way to show your ass by calling me something I'm not.

Given your assumptions on things like inflation, attacking people who disagree with you as leftists, and what you said comparing GPUs to sports cars...yeah I have some bad news for you, buddy...

>No one is forced to buy. Thus one can't be screwed. This is basic logic.

Then you go without. Hooray for pricing people out of the market!

Really, market fundamentalism is a cancer here.

>And none of that mattera because you don't know what's really going on. You know. Your view being shaped by your political ideology..

So is yours. At least im honest and admit it, and wish we could have this conversation on this sub more often. Sorry you dont like having your views challenged.

EDIT: LMFAO he blocked me, my response:

> The ONLY way inflation happens is via government injecting money into the economy. Our currency is centrally managed. This is not disputable.

Yes it is, but that's an extremely simplistic way to look at it. Things also matter like unemployment rate, supply shortages, corporate greed, etc. We just shut down and reopened the entire world economy for a year. The entire WORLD is experiencing inflation. Doing that shocks the entire world economy dude. It's not like a computer. if your computer screws up, turning it off and on again often fixes it. Do that to an economy and it screws it up, because its a complex well managed machine and turning it off and on again knocks the variables out of whack. 

>I never said they should not. I'm saying your dumbass stance on the issue is as of it's like a fundamental good for life and prosperity. It's not and neither are video games. So frame me out to be anti-poor, because that's what people like you do. You delude yourself that because you're poor and can't afford a new high end GPU, that the problem is everything else but you. It's childish and shallow.

Hint, it's not just the ""high end"", it's the entire market. Historically, GPUs ranged from around $100 up to around $500-700. Heck going all the way back to the 2000s was wild as I remember GPUs down as low as like $50. 

The 1650 is freaking $170, and it's 4 years old. ""60"" cards went from costing $200-300 up to $350-400. That's the ""bread and butter"" GPU. Im not talking that enthusiast crap. The cheapest ""current gen"" card is the ""$250"" (reallty $300) 3050. And in 6 years we've only seen about a 50% increase in performance epr dollar, with 2/3 of that happening 4 years ago with the 16 series. 

The entire budget GPU market is more or less GONE. $150-180 is entry level now. That's your RX 6400s and 1650s. And if not for AMD, we old 1060 and 580 owners would LITERALLY HAVE NO UPGRADE PATH. Youre in your own little world going on about ""luxury GPUs"" and sports cars and blah blah blah.

here I'm seeing people priced out of the market entirely. Theres a reason the 1060 is still one of the top cards on steam hardware survey. And the 1650...replaced it? That's WORSE! Seriously, how do you not understand that this market is just BROKEN? Full on broken.

So yeah I am gonna crap on your views here since it really does seem to reek of the typical elitism I see on these hardware subs. About 5-10 years ago it used to be full of people looking to typically spend $500-1000 on a computer. An ENTIRE COMPUTER.

Your typical PC 10 years ago was like a i5 2500k with a 760 or something. And people were still buying FX CPUs and i3s and mixing them when 750 tis and stuff. 

Again, these people are being full on PRICED OUT of the market. That's BAD. Well, unless you're a snotty elitist who doesnt care about affordable PC gaming, which sounds like half this sub sometimes. 

>That's exactly what it is right? You're broke so you're mad and blame them for being anti-poor and gatekeeping you. What an insanely narcissistic view. Guess what. They don't give a shit about your economic status. If they can get you to buy, and your market segment profitable enough, then you'll have an option.

Ah yes, more poor shaming. And yes, by your standard i AM poor. And apparently i dont deserve to game and should work harder or some crap. 

Again, you might not like having your views challenged, but you're an economic right winger. 

>You've done literally nothing to challenge me. You were easily baited into revealing your far left bias and it's obvious you were triggered. You're triggered because you felt personally slighted. Classic narcissistic toxic left wing bullshit. You can't handle that I exist on here so you feel the need to attack me personally. It's pathetic and does your ideology no favors.

Hint, you dont need to bait me to show my bias. I wear it on my sleeve. Anyone who checks my profile can see Im fairly left wing. At least by capitalist standards. The amount of gatekeeping i get from actual anti capitalist leftists is astounding. But yeah, according to you, the mere suggestion that markets should work for the people and we shouldnt have to spend $300 on an entry level bottom of the barrel GPU apparently makes me a pinko commie. 

I really couldnt give a crap what someone of your ideological persuasion thinks of me, because i quite frankly stopped giving a crap what the ""right"" thinks about, well...anything any more. 

But yeah, you ARE right wing, whether you'll admit it or not. As is most of this sub. And that bias is insufferable on here sometimes. As i said, bunch of upper class yuppies out of touch with the rest of the country, and the world while we're at it."
Yikes! Better Business Bureau complaint?  They kept giving me complimentary speed upgrades and the upping my price. I started at about $50 and was at $92 by the time I said enough is enough and called them to downgrade.
"Exactly. Some people live in a big city, or provide for a family, or don't have a lot of income.

So your assertion to ""Or, you know, save a third of their income for three years. Don't act like that's impossible, since a ton of people do it.""

is rather tone-deaf."
"With prices now, a pc makes sense in a purely gaming use not including having the functionality of a desktop. But I'm on the fence whether buying a console at launch makes sense compared to a pc upgrade back in 2020 when people manage to get it at msrp.

I don't remember how expensive but was upgrading back in 2020-2021 still the better choice than console? iirc every gpu was double price and console fared no better.

Quotes from some people 2-3 years ago

""Consoles are always a great buy when they release, give it 2-3 years and they won't look as great in comparison to pc hardware.""

and

""I think thats true for most tech though, it loses value starting from the day it launches. Right now the PS5 is a great value for the price."""
Theyâ€™re still available arenâ€™t they? With backwards compatibility PS4 and PS5 are not separate entities. Otherwise youâ€™d need to call a PC from 5 years ago and one from now separate entities because a new game might not work on the old one. Itâ€™s PC2!
Don't expect to afford a house without saving more, then. If you can't afford it, then don't buy it, how is that different from any other thing?
[deleted]
">Don't expect to afford a house without saving more, then. If you can't afford it, then don't buy it, how is that different from any other thing?

The problem isn't whether or not to buy a house. The problem is, you seem to just casually expect people to be able to save one-third of their income each year.

When the problem is that housing prices are unjustifiably too high."
You can keep your stuttering and 2000â‚¬ gpus
"They are high because of demand. Nobody would try to sell a house for a few years without buyers. There are buyers, but they have more money than other people.

Some of the buyers are landlords, which speaks to the rent prices in the area. If the rents are high it means not enough housing is allowed to be built. Otherwise people would build large apartment buildings to satisfy the demand. But because of zoning regulations they can't satisfy all of the demand and the prices for real estate keep going up

So in the end it's the NIMBYs' fault"
">So in the end it's the NIMBYs' fault

I'm willing to accept this premise.

The problem with apartments is an implicit difference between apartments and houses that doesn't have to be true, but is regularly treated as if it must be true: apartments are rented; houses are owned."
My girlfriend owns a condo in a 15 story building, so not always
